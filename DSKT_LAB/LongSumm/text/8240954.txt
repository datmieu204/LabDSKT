id: SP:ce73c4c72231d81f09492c3a18acbe4daadd0074
title: Self-Normalizing Neural Networks
authors: [{'affiliations': [], 'name': 'Günter Klambauer'}, {'affiliations': [], 'name': 'Thomas Unterthiner'}, {'affiliations': [], 'name': 'Andreas Mayr'}, {'affiliations': [], 'name': 'Sepp Hochreiter'}]
abstractText: Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are “scaled exponential linear units” (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance — even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization schemes, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs, and other machine learning methods such as random forests and support vector machines. For FNNs we considered (i) ReLU networks without normalization, (ii) batch normalization, (iii) layer normalization, (iv) weight normalization, (v) highway networks, and (vi) residual networks. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.
references: [{'authors': ['M. Abramowitz', 'I. Stegun'], 'title': 'Handbook of Mathematical Functions, volume 55 of Applied Mathematics Series', 'venue': 'National Bureau of Standards,', 'year': 1964}, {'authors': ['Y. Bengio'], 'title': 'Deep learning of representations: Looking forward', 'venue': 'In Proceedings of the First International Conference on Statistical Language and Speech Processing,', 'year': 2013}, {'authors': ['J. Blinn'], 'title': 'Consider the lowly 2×2 matrix', 'venue': 'IEEE Computer Graphics and Applications,', 'year': 1996}, {'authors': ['R.C. Bradley'], 'title': 'Central limit theorems under weak dependence', 'venue': 'Journal of Multivariate Analysis,', 'year': 1981}, {'authors': ['D. Cireşan', 'U. Meier'], 'title': 'Multi-column deep neural networks for offline handwritten chinese character classification', 'venue': 'In 2015 International Joint Conference on Neural Networks (IJCNN),', 'year': 2015}, {'authors': ['Clevert', 'D.-A', 'T. Unterthiner', 'S. Hochreiter'], 'title': 'Fast and accurate deep network learning by exponential linear units (ELUs)', 'venue': '5th International Conference on Learning Representations,', 'year': 2015}, {'authors': ['P. Dugan', 'C. Clark', 'Y. LeCun', 'S. Van Parijs'], 'title': 'Phase 4: Dcl system using deep learning approaches for land-based or ship-based real-time recognition and localization of marine mammals-distributed processing and big data applications', 'venue': 'arXiv preprint arXiv:1605.00982', 'year': 2016}, {'authors': ['A. Esteva', 'B. Kuprel', 'R. Novoa', 'J. Ko', 'S. Swetter', 'H. Blau', 'S. Thrun'], 'title': 'Dermatologist-level classification of skin cancer', 'year': 2017}, {'authors': ['M. Fernández-Delgado', 'E. Cernadas', 'S. Barro', 'D. Amorim'], 'title': 'Do we need hundreds of classifiers to solve real world classification problems', 'venue': 'Journal of Machine Learning Research,', 'year': 2014}, {'authors': ['D. Goldberg'], 'title': 'What every computer scientist should know about floating-point arithmetic', 'venue': 'ACM Comput. Surv.,', 'year': 1991}, {'authors': ['A. Graves', 'A. Mohamed', 'G. Hinton'], 'title': 'Speech recognition with deep recurrent neural networks. In IEEE International conference on acoustics, speech and signal processing', 'year': 2013}, {'authors': ['A. Graves', 'J. Schmidhuber'], 'title': 'Offline handwriting recognition with multidimensional recurrent neural networks', 'venue': 'In Advances in neural information processing systems,', 'year': 2009}, {'authors': ['V. Gulshan', 'L. Peng', 'M. Coram', 'M.C. Stumpe', 'D. Wu', 'A. Narayanaswamy', 'S. Venugopalan', 'K. Widner', 'T. Madams', 'J Cuadros'], 'title': 'Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs', 'year': 2016}, {'authors': ['J. Harrison'], 'title': 'A machine-checked theory of floating point arithmetic', 'venue': 'editors, Theorem Proving in Higher Order Logics: 12th International Conference, TPHOLs’99,', 'year': 1999}, {'authors': ['K. He', 'X. Zhang', 'S. Ren', 'J. Sun'], 'title': 'Deep residual learning for image recognition', 'venue': 'In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)', 'year': 2015}, {'authors': ['K. He', 'X. Zhang', 'S. Ren', 'J. Sun'], 'title': 'Delving deep into rectifiers: Surpassing human-level performance on imagenet classification', 'venue': 'In Proceedings of the IEEE International Conference on Computer Vision (ICCV),', 'year': 2015}, {'authors': ['S. Hochreiter', 'J. Schmidhuber'], 'title': 'Long short-term memory', 'venue': 'Neural Computation,', 'year': 1997}, {'authors': ['B. Huval', 'T. Wang', 'S Tandon'], 'title': 'An empirical evaluation of deep learning on highway driving', 'venue': 'arXiv preprint arXiv:1504.01716', 'year': 2015}, {'authors': ['S. Ioffe', 'C. Szegedy'], 'title': 'Batch normalization: Accelerating deep network training by reducing internal covariate shift', 'venue': 'In Proceedings of The 32nd International Conference on Machine Learning,', 'year': 2015}, {'authors': ['W. Kahan'], 'title': 'A logarithm too clever by half', 'venue': 'Technical report,', 'year': 2004}, {'authors': ['V. Korolev', 'I. Shevtsova'], 'title': 'An improvement of the Berry–Esseen inequality with applications to Poisson and mixed Poisson random sums', 'venue': 'Scandinavian Actuarial Journal,', 'year': 2012}, {'authors': ['A. Krizhevsky', 'I. Sutskever', 'G. Hinton'], 'title': 'Imagenet classification with deep convolutional neural networks', 'venue': 'In Advances in Neural Information Processing Systems,', 'year': 2012}, {'authors': ['Y. LeCun', 'Y. Bengio'], 'title': 'Convolutional networks for images, speech, and time series', 'venue': 'The handbook of brain theory and neural networks,', 'year': 1995}, {'authors': ['S. Loosemore', 'R.M. Stallman', 'R. McGrath', 'A. Oram', 'U. Drepper'], 'title': 'The GNU C Library: Application Fundamentals', 'venue': 'GNU Press, Free Software Foundation,', 'year': 2016}, {'authors': ['R. Lyon', 'B. Stappers', 'S. Cooper', 'J. Brooke', 'J. Knowles'], 'title': 'Fifty years of pulsar candidate selection: From simple filters to a new principled real-time classification approach', 'venue': 'Monthly Notices of the Royal Astronomical Society,', 'year': 2016}, {'authors': ['A. Mayr', 'G. Klambauer', 'T. Unterthiner', 'S. Hochreiter'], 'title': 'DeepTox: Toxicity prediction using deep learning', 'venue': 'Frontiers in Environmental Science,', 'year': 2016}, {'authors': ['Muller', 'J.-M'], 'title': 'On the definition of ulp(x)', 'venue': 'Technical Report Research report RR2005-09, Laboratoire de l’Informatique du Parallélisme', 'year': 2005}, {'authors': ['C. Ren', 'A.R. MacKenzie'], 'title': 'Closed-form approximations to the error and complementary error functions and their applications in atmospheric science', 'venue': 'Atmos. Sci. Let.,', 'year': 2007}, {'authors': ['H. Sak', 'A. Senior', 'K. Rao', 'F. Beaufays'], 'title': 'Fast and accurate recurrent neural network acoustic models for speech recognition', 'venue': 'arXiv preprint arXiv:1507.06947', 'year': 2015}, {'authors': ['T. Salimans', 'D.P. Kingma'], 'title': 'Weight normalization: A simple reparameterization to accelerate training of deep neural networks', 'venue': 'In Advances in Neural Information Processing Systems,', 'year': 2016}, {'authors': ['J. Schmidhuber'], 'title': 'Deep learning in neural networks: An overview', 'venue': 'Neural Networks,', 'year': 2015}, {'authors': ['D. Silver', 'A. Huang', 'C Maddison'], 'title': 'Mastering the game of Go with deep neural networks and tree search', 'year': 2016}, {'authors': ['R.K. Srivastava', 'K. Greff', 'J. Schmidhuber'], 'title': 'Training very deep networks', 'venue': 'In Advances in Neural Information Processing Systems,', 'year': 2015}, {'authors': ['I. Sutskever', 'O. Vinyals', 'Q.V. Le'], 'title': 'Sequence to sequence learning with neural networks', 'venue': 'In Advances in Neural Information Processing Systems,', 'year': 2014}]
sections: [{'text': 'Introduction\nDeep Learning has set new records at different benchmarks and led to various commercial applications [25, 33]. Recurrent neural networks (RNNs) [18] achieved new levels at speech and natural language processing, for example at the TIMIT benchmark [12] or at language translation [36], and are already employed in mobile devices [31]. RNNs have won handwriting recognition challenges (Chinese and Arabic handwriting) [33, 13, 6] and Kaggle challenges, such as the “Grasp-and Lift EEG” competition. Their counterparts, convolutional neural networks (CNNs) [24] excel at vision and video tasks. CNNs are on par with human dermatologists at the visual detection of skin cancer [9]. The visual processing for self-driving cars is based on CNNs [19], as is the visual input to AlphaGo which has beaten one of the best human GO players [34]. At vision challenges, CNNs are constantly winning, for example\nar X\niv :1\n70 6.\n02 51\n5v 1\n[ cs\n.L G\n] 8\nJ un\nat the large ImageNet competition [23, 16], but also almost all Kaggle vision challenges, such as the “Diabetic Retinopathy” and the “Right Whale” challenges [8, 14].\nHowever, looking at Kaggle challenges that are not related to vision or sequential tasks, gradient boosting, random forests, or support vector machines (SVMs) are winning most of the competitions. Deep Learning is notably absent, and for the few cases where FNNs won, they are shallow. For example, the HIGGS challenge, the Merck Molecular Activity challenge, and the Tox21 Data challenge were all won by FNNs with at most four hidden layers. Surprisingly, it is hard to find success stories with FNNs that have many hidden layers, though they would allow for different levels of abstract representations of the input [3].\nTo robustly train very deep CNNs, batch normalization evolved into a standard to normalize neuron activations to zero mean and unit variance [20]. Layer normalization [2] also ensures zero mean and unit variance, while weight normalization [32] ensures zero mean and unit variance if in the previous layer the activations have zero mean and unit variance. However, training with normalization techniques is perturbed by stochastic gradient descent (SGD), stochastic regularization (like dropout), and the estimation of the normalization parameters. Both RNNs and CNNs can stabilize learning via weight sharing, therefore they are less prone to these perturbations. In contrast, FNNs trained with normalization techniques suffer from these perturbations and have high variance in the training error (see Figure 1). This high variance hinders learning and slows it down. Furthermore, strong regularization, such as dropout, is not possible as it would further increase the variance which in turn would lead to divergence of the learning process. We believe that this sensitivity to perturbations is the reason that FNNs are less successful than RNNs and CNNs.\nSelf-normalizing neural networks (SNNs) are robust to perturbations and do not have high variance in their training errors (see Figure 1). SNNs push neuron activations to zero mean and unit variance thereby leading to the same effect as batch normalization, which enables to robustly learn many layers. SNNs are based on scaled exponential linear units “SELUs” which induce self-normalizing properties like variance stabilization which in turn avoids exploding and vanishing gradients.\nSelf-normalizing Neural Networks (SNNs)\nNormalization and SNNs. For a neural network with activation function f , we consider two consecutive layers that are connected by a weight matrix W . Since the input to a neural network is a random variable, the activations x in the lower layer, the network inputs z = Wx, and the activations y = f(z) in the higher layer are random variables as well. We assume that all activations xi of the lower layer have mean µ := E(xi) and variance ν := Var(xi). An activation y in the higher layer has mean µ̃ := E(y) and variance ν̃ := Var(y). Here E(.) denotes the expectation and Var(.) the variance of a random variable. A single activation y = f(z) has net input z = wTx. For n units with activation xi, 1 6 i 6 n in the lower layer, we define n times the mean of the weight vector w ∈ Rn as ω := ∑n i=1 wi and n times the second moment as τ := ∑n i=1 w 2 i .\nWe consider the mapping g that maps mean and variance of the activations from one layer to mean and variance of the activations in the next layer\n( µ ν ) 7→ ( µ̃ ν̃ ) : ( µ̃ ν̃ ) = g ( µ ν ) . (1)\nNormalization techniques like batch, layer, or weight normalization ensure a mapping g that keeps (µ, ν) and (µ̃, ν̃) close to predefined values, typically (0, 1). Definition 1 (Self-normalizing neural net). A neural network is self-normalizing if it possesses a mapping g : Ω 7→ Ω for each activation y that maps mean and variance from one layer to the next and has a stable and attracting fixed point depending on (ω, τ) in Ω. Furthermore, the mean and the variance remain in the domain Ω, that is g(Ω) ⊆ Ω, where Ω = {(µ, ν) | µ ∈ [µmin, µmax], ν ∈ [νmin, νmax]}. When iteratively applying the mapping g, each point within Ω converges to this fixed point.\nTherefore, we consider activations of a neural network to be normalized, if both their mean and their variance across samples are within predefined intervals. If mean and variance of x are already within\nthese intervals, then also mean and variance of y remain in these intervals, i.e., the normalization is transitive across layers. Within these intervals, the mean and variance both converge to a fixed point if the mapping g is applied iteratively.\nTherefore, SNNs keep normalization of activations when propagating them through layers of the network. The normalization effect is observed across layers of a network: in each layer the activations are getting closer to the fixed point. The normalization effect can also observed be for two fixed layers across learning steps: perturbations of lower layer activations or weights are damped in the higher layer by drawing the activations towards the fixed point. If for all y in the higher layer, ω and τ of the corresponding weight vector are the same, then the fixed points are also the same. In this case we have a unique fixed point for all activations y. Otherwise, in the more general case, ω and τ differ for different y but the mean activations are drawn into [µmin, µmax] and the variances are drawn into [νmin, νmax].\nConstructing Self-Normalizing Neural Networks. We aim at constructing self-normalizing neural networks by adjusting the properties of the function g. Only two design choices are available for the function g: (1) the activation function and (2) the initialization of the weights.\nFor the activation function, we propose “scaled exponential linear units” (SELUs) to render a FNN as self-normalizing. The SELU activation function is given by\nselu(x) = λ { x if x > 0 αex − α if x 6 0 . (2)\nSELUs allow to construct a mapping g with properties that lead to SNNs. SNNs cannot be derived with (scaled) rectified linear units (ReLUs), sigmoid units, tanh units, and leaky ReLUs. The activation function is required to have (1) negative and positive values for controlling the mean, (2) saturation regions (derivatives approaching zero) to dampen the variance if it is too large in the lower layer, (3) a slope larger than one to increase the variance if it is too small in the lower layer, (4) a continuous curve. The latter ensures a fixed point, where variance damping is equalized by variance increasing. We met these properties of the activation function by multiplying the exponential linear unit (ELU) [7] with λ > 1 to ensure a slope larger than one for positive net inputs.\nFor the weight initialization, we propose ω = 0 and τ = 1 for all units in the higher layer. The next paragraphs will show the advantages of this initialization. Of course, during learning these assumptions on the weight vector will be violated. However, we can prove the self-normalizing property even for weight vectors that are not normalized, therefore, the self-normalizing property can be kept during learning and weight changes.\nDeriving the Mean and Variance Mapping Function g. We assume that the xi are independent from each other but share the same mean µ and variance ν. Of course, the independence assumptions is not fulfilled in general. We will elaborate on the independence assumption below. The network input z in the higher layer is z = wTx for which we can infer the following moments E(z) =∑n i=1 wi E(xi) = µ ω and Var(z) = Var( ∑n i=1 wi xi) = ν τ , where we used the independence of the xi. The net input z is a weighted sum of independent, but not necessarily identically distributed variables xi, for which the central limit theorem (CLT) states that z approaches a normal distribution: z ∼ N (µω, √ ντ) with density pN(z;µω, √ ντ). According to the CLT, the larger n, the closer is z to a normal distribution. For Deep Learning, broad layers with hundreds of neurons xi are common. Therefore the assumption that z is normally distributed is met well for most currently used neural networks (see Figure A8). The function g maps the mean and variance of activations in the lower layer to the mean µ̃ = E(y) and variance ν̃ = Var(y) of the activations y in the next layer:\ng : ( µ ν ) 7→ ( µ̃ ν̃ ) : µ̃(µ, ω, ν, τ) = ∫ ∞ −∞ selu(z) pN(z;µω, √ ντ) dz (3)\nν̃(µ, ω, ν, τ) = ∫ ∞ −∞ selu(z)2 pN(z;µω, √ ντ) dz − (µ̃)2 .\nThese integrals can be analytically computed and lead to following mappings of the moments:\nµ̃ = 1\n2 λ\n( (µω) erf ( µω√ 2 √ ντ ) + (4)\nα eµω+ ντ 2 erfc ( µω + ντ√\n2 √ ντ\n) − α erfc ( µω√ 2 √ ντ ) + √ 2 π √ ντe− (µω)2 2(ντ) + µω )\nν̃ = 1 2 λ2 (( (µω)2 + ντ )( 2− erfc ( µω√ 2 √ ντ )) + α2 ( −2eµω+ ντ2 erfc ( µω + ντ√ 2 √ ντ ) (5)\n+e2(µω+ντ) erfc ( µω + 2ντ√\n2 √ ντ\n) + erfc ( µω√ 2 √ ντ )) + √ 2 π (µω) √ ντe− (µω)2 2(ντ) ) − (µ̃)2\nStable and Attracting Fixed Point (0,1) for Normalized Weights. We assume a normalized weight vector w with ω = 0 and τ = 1. Given a fixed point (µ, ν), we can solve equations Eq. (4) and Eq. (5) for α and λ. We chose the fixed point (µ, ν) = (0, 1), which is typical for activation normalization. We obtain the fixed point equations µ̃ = µ = 0 and ν̃ = ν = 1 that we solve for α and λ and obtain the solutions α01 ≈ 1.6733 and λ01 ≈ 1.0507, where the subscript 01 indicates that these are the parameters for fixed point (0, 1). The analytical expressions for α01 and λ01 are given in Eq. (14). We are interested whether the fixed point (µ, ν) = (0, 1) is stable and attracting. If the Jacobian of g has a norm smaller than 1 at the fixed point, then g is a contraction mapping and the fixed point is stable. The (2x2)-Jacobian J (µ, ν) of g : (µ, ν) 7→ (µ̃, ν̃) evaluated at the fixed point (0, 1) with α01 and λ01 is\nJ (µ, ν) = ∂ µ new(µ,ν) ∂µ ∂ µnew(µ,ν) ∂ν\n∂ ν new(µ,ν) ∂µ ∂ νnew(µ,ν) ∂ν\n , J (0, 1) = (0.0 0.0888340.0 0.782648 ) . (6)\nThe spectral norm of J (0, 1) (its largest singular value) is 0.7877 < 1. That means g is a contraction mapping around the fixed point (0, 1) (the mapping is depicted in Figure 2). Therefore, (0, 1) is a stable fixed point of the mapping g.\nStable and Attracting Fixed Points for Unnormalized Weights. A normalized weight vector w cannot be ensured during learning. For SELU parameters α = α01 and λ = λ01, we show in the next theorem that if (ω, τ) is close to (0, 1), then g still has an attracting and stable fixed point that is close to (0, 1). Thus, in the general case there still exists a stable fixed point which, however, depends on (ω, τ). If we restrict (µ, ν, ω, τ) to certain intervals, then we can show that (µ, ν) is mapped to\nthe respective intervals. Next we present the central theorem of this paper, from which follows that SELU networks are self-normalizing under mild conditions on the weights.\nTheorem 1 (Stable and Attracting Fixed Points). We assume α = α01 and λ = λ01. We restrict the range of the variables to the following intervals µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.95, 1.1], that define the functions’ domain Ω. For ω = 0 and τ = 1, the mapping Eq. (3) has the stable fixed point (µ, ν) = (0, 1), whereas for other ω and τ the mapping Eq. (3) has a stable and attracting fixed point depending on (ω, τ) in the (µ, ν)-domain: µ ∈ [−0.03106, 0.06773] and ν ∈ [0.80009, 1.48617]. All points within the (µ, ν)-domain converge when iteratively applying the mapping Eq. (3) to this fixed point.\nProof. We provide a proof sketch (see detailed proof in Appendix Section A3). With the Banach fixed point theorem we show that there exists a unique attracting and stable fixed point. To this end, we have to prove that a) g is a contraction mapping and b) that the mapping stays in the domain, that is, g(Ω) ⊆ Ω. The spectral norm of the Jacobian of g can be obtained via an explicit formula for the largest singular value for a 2× 2 matrix. g is a contraction mapping if its spectral norm is smaller than 1. We perform a computer-assisted proof to evaluate the largest singular value on a fine grid and ensure the precision of the computer evaluation by an error propagation analysis of the implemented algorithms on the according hardware. Singular values between grid points are upper bounded by the mean value theorem. To this end, we bound the derivatives of the formula for the largest singular value with respect to ω, τ, µ, ν. Then we apply the mean value theorem to pairs of points, where one is on the grid and the other is off the grid. This shows that for all values of ω, τ, µ, ν in the domain Ω, the spectral norm of g is smaller than one. Therefore, g is a contraction mapping on the domain Ω. Finally, we show that the mapping g stays in the domain Ω by deriving bounds on µ̃ and ν̃. Hence, the Banach fixed-point theorem holds and there exists a unique fixed point in Ω that is attained.\nConsequently, feed-forward neural networks with many units in each layer and with the SELU activation function are self-normalizing (see definition 1), which readily follows from Theorem 1. To give an intuition, the main property of SELUs is that they damp the variance for negative net inputs and increase the variance for positive net inputs. The variance damping is stronger if net inputs are further away from zero while the variance increase is stronger if net inputs are close to zero. Thus, for large variance of the activations in the lower layer the damping effect is dominant and the variance decreases in the higher layer. Vice versa, for small variance the variance increase is dominant and the variance increases in the higher layer.\nHowever, we cannot guarantee that mean and variance remain in the domain Ω. Therefore, we next treat the case where (µ, ν) are outside Ω. It is especially crucial to consider ν because this variable has much stronger influence than µ. Mapping ν across layers to a high value corresponds to an exploding gradient, since the Jacobian of the activation of high layers with respect to activations in lower layers has large singular values. Analogously, mapping ν across layers to a low value corresponds to an vanishing gradient. Bounding the mapping of ν from above and below would avoid both exploding and vanishing gradients. Theorem 2 states that the variance of neuron activations of\nSNNs is bounded from above, and therefore ensures that SNNs learn robustly and do not suffer from exploding gradients.\nTheorem 2 (Decreasing ν). For λ = λ01, α = α01 and the domain Ω+: −1 6 µ 6 1, −0.1 6 ω 6 0.1, 3 6 ν 6 16, and 0.8 6 τ 6 1.25, we have for the mapping of the variance ν̃(µ, ω, ν, τ, λ, α) given in Eq. (5): ν̃(µ, ω, ν, τ, λ01, α01) < ν.\nThe proof can be found in the Appendix Section A3. Thus, when mapped across many layers, the variance in the interval [3, 16] is mapped to a value below 3. Consequently, all fixed points (µ, ν) of the mapping g (Eq. (3)) have ν < 3. Analogously, Theorem 3 states that the variance of neuron activations of SNNs is bounded from below, and therefore ensures that SNNs do not suffer from vanishing gradients.\nTheorem 3 (Increasing ν). We consider λ = λ01, α = α01 and the domain Ω−: −0.1 6 µ 6 0.1, and −0.1 6 ω 6 0.1. For the domain 0.02 6 ν 6 0.16 and 0.8 6 τ 6 1.25 as well as for the domain 0.02 6 ν 6 0.24 and 0.9 6 τ 6 1.25, the mapping of the variance ν̃(µ, ω, ν, τ, λ, α) given in Eq. (5) increases: ν̃(µ, ω, ν, τ, λ01, α01) > ν.\nThe proof can be found in the Appendix Section A3. All fixed points (µ, ν) of the mapping g (Eq. (3)) ensure for 0.8 6 τ that ν̃ > 0.16 and for 0.9 6 τ that ν̃ > 0.24. Consequently, the variance mapping Eq. (5) ensures a lower bound on the variance ν. Therefore SELU networks control the variance of the activations and push it into an interval, whereafter the mean and variance move toward the fixed point. Thus, SELU networks are steadily normalizing the variance and subsequently normalizing the mean, too. In all experiments, we observed that self-normalizing neural networks push the mean and variance of activations into the domain Ω .\nInitialization. Since SNNs have a fixed point at zero mean and unit variance for normalized weights ω = ∑n i=1 wi = 0 and τ = ∑n i=1 w 2 i = 1 (see above), we initialize SNNs such that these constraints are fulfilled in expectation. We draw the weights from a Gaussian distribution with E(wi) = 0 and variance Var(wi) = 1/n. Uniform and truncated Gaussian distributions with these moments led to networks with similar behavior. The “MSRA initialization” is similar since it uses zero mean and variance 2/n to initialize the weights [17]. The additional factor 2 counters the effect of rectified linear units.\nNew Dropout Technique. Standard dropout randomly sets an activation x to zero with probability 1 − q for 0 < q 6 1. In order to preserve the mean, the activations are scaled by 1/q during training. If x has mean E(x) = µ and variance Var(x) = ν, and the dropout variable d follows a binomial distribution B(1, q), then the mean E(1/qdx) = µ is kept. Dropout fits well to rectified linear units, since zero is in the low variance region and corresponds to the default value. For scaled exponential linear units, the default and low variance value is limx→−∞ selu(x) = −λα = α′. Therefore, we propose “alpha dropout”, that randomly sets inputs to α′. The new mean and new variance is E(xd + α′(1 − d)) = qµ + (1 − q)α′, and Var(xd + α′(1 − d)) = q((1 − q)(α′ − µ)2 + ν). We aim at keeping mean and variance to their original values after “alpha dropout”, in order to ensure the self-normalizing property even for “alpha dropout”. The affine transformation a(xd + α′(1 − d)) + b allows to determine parameters a and b such that mean and variance are kept to their values: E(a(xd + α′(1 − d)) + b) = µ and Var(a(xd + α′(1 − d)) + b) = ν . In contrast to dropout, a and b will depend on µ and ν, however our SNNs converge to activations with zero mean and unit variance. With µ = 0 and ν = 1, we obtain a = ( q + α′2q(1− q) )−1/2 and\nb = − ( q + α′2q(1− q) )−1/2 ((1− q)α′). The parameters a and b only depend on the dropout rate 1− q and the most negative activation α′. Empirically, we found that dropout rates 1− q = 0.05 or 0.10 lead to models with good performance. “Alpha-dropout” fits well to scaled exponential linear units by randomly setting activations to the negative saturation value.\nApplicability of the central limit theorem and independence assumption. In the derivative of the mapping (Eq. (3)), we used the central limit theorem (CLT) to approximate the network inputs z = ∑n i=1 wixi with a normal distribution. We justified normality because network inputs represent a weighted sum of the inputs xi, where for Deep Learning n is typically large. The Berry-Esseen\ntheorem states that the convergence rate to normality is n−1/2 [22]. In the classical version of the CLT, the random variables have to be independent and identically distributed, which typically does not hold for neural networks. However, the Lyapunov CLT does not require the variable to be identically distributed anymore. Furthermore, even under weak dependence, sums of random variables converge in distribution to a Gaussian distribution [5].\nExperiments\nWe compare SNNs to other deep networks at different benchmarks. Hyperparameters such as number of layers (blocks), neurons per layer, learning rate, and dropout rate, are adjusted by grid-search for each dataset on a separate validation set (see Section A4). We compare the following FNN methods:\n• “MSRAinit”: FNNs without normalization and with ReLU activations and “Microsoft weight initialization” [17].\n• “MSRAinit”: FNNs without normalization and with ReLU activations and “Microsoft weight initialization” [17].\n• “BatchNorm”: FNNs with batch normalization [20]. • “LayerNorm”: FNNs with layer normalization [2]. • “WeightNorm”: FNNs with weight normalization [32]. • “Highway”: Highway networks [35]. • “ResNet”: Residual networks [16] adapted to FNNs using residual blocks with 2 or 3 layers\nwith rectangular or diavolo shape.\n• “SNNs”: Self normalizing networks with SELUs with α = α01 and λ = λ01 and the proposed dropout technique and initialization strategy.\n121 UCI Machine Learning Repository datasets. The benchmark comprises 121 classification datasets from the UCI Machine Learning repository [10] from diverse application areas, such as physics, geology, or biology. The size of the datasets ranges between 10 and 130, 000 data points and the number of features from 4 to 250. In abovementioned work [10], there were methodological mistakes [37] which we avoided here. Each compared FNN method was optimized with respect to its architecture and hyperparameters on a validation set that was then removed from the subsequent analysis. The selected hyperparameters served to evaluate the methods in terms of accuracy on the pre-defined test sets (details on the hyperparameter selection are given in Section A4). The accuracies are reported in the Table A11. We ranked the methods by their accuracy for each prediction task and compared their average ranks. SNNs significantly outperform all competing networks in pairwise comparisons (paired Wilcoxon test across datasets) as reported in Table 1 (left panel).\nWe further included 17 machine learning methods representing diverse method groups [10] in the comparison and the grouped the data sets into “small” and “large” data sets (for details see Section A4). On 75 small datasets with less than 1000 data points, random forests and SVMs outperform SNNs and other FNNs. On 46 larger datasets with at least 1000 data points, SNNs show the highest performance followed by SVMs and random forests (see right panel of Table 1, for complete results see Tables A12 and A12). Overall, SNNs have outperformed state of the art machine learning methods on UCI datasets with more than 1,000 data points.\nTypically, hyperparameter selection chose SNN architectures that were much deeper than the selected architectures of other FNNs, with an average depth of 10.8 layers, compared to average depths of 6.0 for BatchNorm, 3.8 WeightNorm, 7.0 LayerNorm, 5.9 Highway, and 7.1 for MSRAinit networks. For ResNet, the average number of blocks was 6.35. SNNs with many more than 4 layers often provide the best predictive accuracies across all neural networks.\nDrug discovery: The Tox21 challenge dataset. The Tox21 challenge dataset comprises about 12,000 chemical compounds whose twelve toxic effects have to be predicted based on their chemical structure. We used the validation sets of the challenge winners for hyperparameter selection (see\nSection A4) and the challenge test set for performance comparison. We repeated the whole evaluation procedure 5 times to obtain error bars. The results in terms of average AUC are given in Table 2. In 2015, the challenge organized by the US NIH was won by an ensemble of shallow ReLU FNNs which achieved an AUC of 0.846 [28]. Besides FNNs, this ensemble also contained random forests and SVMs. Single SNNs came close with an AUC of 0.845±0.003. The best performing SNNs have 8 layers, compared to the runner-ups ReLU networks with layer normalization with 2 and 3 layers. Also batchnorm and weightnorm networks, typically perform best with shallow networks of 2 to 4 layers (Table 2). The deeper the networks, the larger the difference in performance between SNNs and other methods (see columns 5–8 of Table 2). The best performing method is an SNN with 8 layers.\nSNN 83.7 ± 0.3 84.4 ± 0.5 84.2 ± 0.4 83.9 ± 0.5 84.5 ± 0.2 83.5 ± 0.5 82.5 ± 0.7 Batchnorm 80.0 ± 0.5 79.8 ± 1.6 77.2 ± 1.1 77.0 ± 1.7 75.0 ± 0.9 73.7 ± 2.0 76.0 ± 1.1 WeightNorm 83.7 ± 0.8 82.9 ± 0.8 82.2 ± 0.9 82.5 ± 0.6 81.9 ± 1.2 78.1 ± 1.3 56.6 ± 2.6 LayerNorm 84.3 ± 0.3 84.3 ± 0.5 84.0 ± 0.2 82.5 ± 0.8 80.9 ± 1.8 78.7 ± 2.3 78.8 ± 0.8 Highway 83.3 ± 0.9 83.0 ± 0.5 82.6 ± 0.9 82.4 ± 0.8 80.3 ± 1.4 80.3 ± 2.4 79.6 ± 0.8 MSRAinit 82.7 ± 0.4 81.6 ± 0.9 81.1 ± 1.7 80.6 ± 0.6 80.9 ± 1.1 80.2 ± 1.1 80.4 ± 1.9 ResNet 82.2 ± 1.1 80.0 ± 2.0 80.5 ± 1.2 81.2 ± 0.7 81.8 ± 0.6 81.2 ± 0.6 na\nAstronomy: Prediction of pulsars in the HTRU2 dataset. Since a decade, machine learning methods have been used to identify pulsars in radio wave signals [27]. Recently, the High Time Resolution Universe Survey (HTRU2) dataset has been released with 1,639 real pulsars and 16,259 spurious signals. Currently, the highest AUC value of a 10-fold cross-validation is 0.976 which has been achieved by Naive Bayes classifiers followed by decision tree C4.5 with 0.949 and SVMs with 0.929. We used eight features constructed by the PulsarFeatureLab as used previously [27]. We assessed the performance of FNNs using 10-fold nested cross-validation, where the hyperparameters were selected in the inner loop on a validation set (for details on the hyperparameter selection see\nSection A4). Table 3 reports the results in terms of AUC. SNNs outperform all other methods and have pushed the state-of-the-art to an AUC of 0.98.\nWe have introduced self-normalizing neural networks for which we have proved that neuron activations are pushed towards zero mean and unit variance when propagated through the network. Additionally, for activations not close to unit variance, we have proved an upper and lower bound on the variance mapping. Consequently, SNNs do not face vanishing and exploding gradient problems. Therefore, SNNs work well for architectures with many layers, allowed us to introduce a novel regularization scheme, and learn very robustly. On 121 UCI benchmark datasets, SNNs have outperformed other FNNs with and without normalization techniques, such as batch, layer, and weight normalization, or specialized architectures, such as Highway or Residual networks. SNNs also yielded the best results on drug discovery and astronomy tasks. The best performing SNN architectures are typically very deep in contrast to other FNNs.\nReferences\nThe references are provided in Section A7.\nAppendix\nContents\nA1 Background 10\nA2 Theorems 12\nA2.1 Theorem 1: Stable and Attracting Fixed Points Close to (0,1) . . . . . . . . . . . . 12\nA2.2 Theorem 2: Decreasing Variance from Above . . . . . . . . . . . . . . . . . . . . 12\nA2.3 Theorem 3: Increasing Variance from Below . . . . . . . . . . . . . . . . . . . . . 12\nA3 Proofs of the Theorems 13\nA3.1 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\nA3.2 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nA3.3 Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nA3.4 Lemmata and Other Tools Required for the Proofs . . . . . . . . . . . . . . . . . . 19\nA3.4.1 Lemmata for proofing Theorem 1 (part 1): Jacobian norm smaller than one 19\nA3.4.2 Lemmata for proofing Theorem 1 (part 2): Mapping within domain . . . . 28\nA3.4.3 Lemmata for proofing Theorem 2: The variance is contracting . . . . . . . 29\nA3.4.4 Lemmata for proofing Theorem 3: The variance is expanding . . . . . . . 32\nA3.4.5 Computer-assisted proof details for main Lemma 12 in Section A3.4.1. . . 33\nA3.4.6 Intermediate Lemmata and Proofs . . . . . . . . . . . . . . . . . . . . . . 37\nA4 Additional information on experiments 84\nA4.1 121 UCI Machine Learning Repository data sets: Hyperparameters . . . . . . . . . 85\nA4.2 121 UCI Machine Learning Repository data sets: detailed results . . . . . . . . . . 87\nA4.3 Tox21 challenge data set: Hyperparameters . . . . . . . . . . . . . . . . . . . . . 92\nA4.4 HTRU2 data set: Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . 95\nA5 Other fixed points 97\nA6 Bounds determined by numerical methods 97\nA7 References 98\nList of figures 100\nList of tables 100\nBrief index 102\nThis appendix is organized as follows: the first section sets the background, definitions, and formulations. The main theorems are presented in the next section. The following section is devoted to the proofs of these theorems. The next section reports additional results and details on the performed computational experiments, such as hyperparameter selection. The last section shows that our theoretical bounds can be confirmed by numerical methods as a sanity check.\nThe proof of theorem 1 is based on the Banach’s fixed point theorem for which we require (1) a contraction mapping, which is proved in Subsection A3.4.1 and (2) that the mapping stays within its domain, which is proved in Subsection A3.4.2 For part (1), the proof relies on the main Lemma 12, which is a computer-assisted proof, and can be found in Subsection A3.4.1. The validity of the computer-assisted proof is shown in Subsection A3.4.5 by error analysis and the precision of the functions’ implementation. The last Subsection A3.4.6 compiles various lemmata with intermediate results that support the proofs of the main lemmata and theorems.\nA1 Background\nWe consider a neural network with activation function f and two consecutive layers that are connected by weight matrix W . Since samples that serve as input to the neural network are chosen according to a distribution, the activations x in the lower layer, the network inputs z = Wx, and activations y = f(z) in the higher layer are all random variables. We assume that all units xi in the lower layer have mean activation µ := E(xi) and variance of the activation ν := Var(xi) and a unit y in the higher layer has mean activation µ̃ := E(y) and variance ν̃ := Var(y). Here E(.)\ndenotes the expectation and Var(.) the variance of a random variable. For activation of unit y, we have net input z = wTx and the scaled exponential linear unit (SELU) activation y = selu(z), with\nselu(x) = λ { x if x > 0 αex − α if x 6 0 . (7)\nFor n units xi, 1 6 i 6 n in the lower layer and the weight vector w ∈ Rn, we define n times the mean by ω := ∑n i=1 wi and n times the second moment by τ := ∑n i=1 w 2 i .\nWe define a mapping g from mean µ and variance ν of one layer to the mean µ̃ and variance ν̃ in the next layer:\ng : (µ, ν) 7→ (µ̃, ν̃) . (8)\nFor neural networks with scaled exponential linear units, the mean is of the activations in the next layer computed according to\nµ̃ = ∫ 0 −∞ λα(exp(z)− 1)pGauss(z;µω, √ ντ)dz + ∫ ∞ 0 λzpGauss(z;µω, √ ντ)dz , (9)\nand the second moment of the activations in the next layer is computed according to\nξ̃ = ∫ 0 −∞ λ2α2(exp(z)− 1)2pGauss(z;µω, √ ντ)dz + ∫ ∞ 0 λ2z2pGauss(z;µω, √ ντ)dz . (10)\nTherefore, the expressions µ̃ and ν̃ have the following form:\nµ̃(µ, ω, ν, τ, λ, α) = 1\n2 λ\n( −(α+ µω) erfc ( µω√ 2 √ ντ ) + (11)\nαeµω+ ντ 2 erfc ( µω + ντ√\n2 √ ντ\n) + √ 2\nπ\n√ ντe− µ2ω2 2ντ + 2µω ) ν̃(µ, ω, ν, τ, λ, α) = ξ̃(µ, ω, ν, τ, λ, α)− (µ̃(µ, ω, ν, τ, λ, α))2 (12)\nξ̃(µ, ω, ν, τ, λ, α) = 1 2 λ2 (( (µω)2 + ντ )( erf ( µω√ 2 √ ντ ) + 1 ) + (13)\nα2 ( −2eµω+ ντ2 erfc ( µω + ντ√\n2 √ ντ\n) + e2(µω+ντ) erfc ( µω + 2ντ√\n2 √ ντ\n) +\nerfc ( µω√ 2 √ ντ )) + √ 2 π (µω) √ ντe− (µω)2 2(ντ) )\nWe solve equations Eq. 4 and Eq. 5 for fixed points µ̃ = µ and ν̃ = ν. For a normalized weight vector with ω = 0 and τ = 1 and the fixed point (µ, ν) = (0, 1), we can solve equations Eq. 4 and Eq. 5 for α and λ. We denote the solutions to fixed point (µ, ν) = (0, 1) by α01 and λ01.\nα01 = −\n√ 2 π\nerfc (\n1√ 2\n) exp ( 1 2 ) − 1 ≈ 1.67326 (14)\nλ01 =\n( 1− erfc ( 1√ 2 )√ e ) 2π(\n2 erfc (√ 2 ) e2 + π erfc ( 1√ 2 )2 e− 2(2 + π) erfc ( 1√ 2 )√ e+ π + 2 )−1/2 λ01 ≈ 1.0507 .\nThe parameters α01 and λ01 ensure\nµ̃(0, 0, 1, 1, λ01, α01) = 0\nν̃(0, 0, 1, 1, λ01, α01) = 1\nSince we focus on the fixed point (µ, ν) = (0, 1), we assume throughout the analysis that α = α01 and λ = λ01. We consider the functions µ̃(µ, ω, ν, τ, λ01, α01), ν̃(µ, ω, ν, τ, λ01, α01), and ξ̃(µ, ω, ν, τ, λ01, α01) on the domain Ω = {(µ, ω, ν, τ) | µ ∈ [µmin, µmax] = [−0.1, 0.1], ω ∈ [ωmin, ωmax] = [−0.1, 0.1], ν ∈ [νmin, νmax] = [0.8, 1.5], τ ∈ [τmin, τmax] = [0.95, 1.1]}. Figure 2 visualizes the mapping g for ω = 0 and τ = 1 and α01 and λ01 at few pre-selected points. It can be seen that (0, 1) is an attracting fixed point of the mapping g.\nA2 Theorems\nA2.1 Theorem 1: Stable and Attracting Fixed Points Close to (0,1)\nTheorem 1 shows that the mapping g defined by Eq. (4) and Eq. (5) exhibits a stable and attracting fixed point close to zero mean and unit variance. Theorem 1 establishes the self-normalizing property of self-normalizing neural networks (SNNs). The stable and attracting fixed point leads to robust learning through many layers. Theorem 1 (Stable and Attracting Fixed Points). We assume α = α01 and λ = λ01. We restrict the range of the variables to the domain µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.95, 1.1]. For ω = 0 and τ = 1, the mapping Eq. (4) and Eq. (5) has the stable fixed point (µ, ν) = (0, 1). For other ω and τ the mapping Eq. (4) and Eq. (5) has a stable and attracting fixed point depending on (ω, τ) in the (µ, ν)-domain: µ ∈ [−0.03106, 0.06773] and ν ∈ [0.80009, 1.48617]. All points within the (µ, ν)-domain converge when iteratively applying the mapping Eq. (4) and Eq. (5) to this fixed point.\nA2.2 Theorem 2: Decreasing Variance from Above\nThe next Theorem 2 states that the variance of unit activations does not explode through consecutive layers of self-normalizing networks. Even more, a large variance of unit activations decreases when propagated through the network. In particular this ensures that exploding gradients will never be observed. In contrast to the domain in previous subsection, in which ν ∈ [0.8, 1.5], we now consider a domain in which the variance of the inputs is higher ν ∈ [3, 16] and even the range of the mean is increased µ ∈ [−1, 1]. We denote this new domain with the symbol Ω++ to indicate that the variance lies above the variance of the original domain Ω. In Ω++, we can show that the variance ν̃ in the next layer is always smaller then the original variance ν. Concretely, this theorem states that: Theorem 2 (Decreasing ν). For λ = λ01, α = α01 and the domain Ω++: −1 6 µ 6 1, −0.1 6 ω 6 0.1, 3 6 ν 6 16, and 0.8 6 τ 6 1.25 we have for the mapping of the variance ν̃(µ, ω, ν, τ, λ, α) given in Eq. (5)\nν̃(µ, ω, ν, τ, λ01, α01) < ν . (15) The variance decreases in [3, 16] and all fixed points (µ, ν) of mapping Eq. (5) and Eq. (4) have ν < 3.\nA2.3 Theorem 3: Increasing Variance from Below\nThe next Theorem 3 states that the variance of unit activations does not vanish through consecutive layers of self-normalizing networks. Even more, a small variance of unit activations increases when propagated through the network. In particular this ensures that vanishing gradients will never be observed. In contrast to the first domain, in which ν ∈ [0.8, 1.5], we now consider two domains Ω−1 and Ω−2 in which the variance of the inputs is lower 0.05 6 ν 6 0.16 and 0.05 6 ν 6 0.24, and even the parameter τ is different 0.9 6 τ 6 1.25 to the original Ω. We denote this new domain with the symbol Ω−i to indicate that the variance lies below the variance of the original domain Ω. In Ω − 1 and Ω−2 , we can show that the variance ν̃ in the next layer is always larger then the original variance ν, which means that the variance does not vanish through consecutive layers of self-normalizing networks. Concretely, this theorem states that:\nTheorem 3 (Increasing ν). We consider λ = λ01, α = α01 and the two domains Ω−1 = {(µ, ω, ν, τ) | − 0.1 6 µ 6 0.1,−0.1 6 ω 6 0.1, 0.05 6 ν 6 0.16, 0.8 6 τ 6 1.25} and Ω−2 = {(µ, ω, ν, τ) | − 0.1 6 µ 6 0.1,−0.1 6 ω 6 0.1, 0.05 6 ν 6 0.24, 0.9 6 τ 6 1.25}. The mapping of the variance ν̃(µ, ω, ν, τ, λ, α) given in Eq. (5) increases\nν̃(µ, ω, ν, τ, λ01, α01) > ν (16)\nin both Ω−1 and Ω − 2 . All fixed points (µ, ν) of mapping Eq. (5) and Eq. (4) ensure for 0.8 6 τ that ν̃ > 0.16 and for 0.9 6 τ that ν̃ > 0.24. Consequently, the variance mapping Eq. (5) and Eq. (4) ensures a lower bound on the variance ν.\nA3 Proofs of the Theorems\nA3.1 Proof of Theorem 1\nWe have to show that the mapping g defined by Eq. (4) and Eq. (5) has a stable and attracting fixed point close to (0, 1). To proof this statement and Theorem 1, we apply the Banach fixed point theorem which requires (1) that g is a contraction mapping and (2) that g does not map outside the function’s domain, concretely:\nTheorem 4 (Banach Fixed Point Theorem). Let (X, d) be a non-empty complete metric space with a contraction mapping f : X → X . Then f has a unique fixed-point xf ∈ X with f(xf ) = xf . Every sequence xn = f(xn−1) with starting element x0 ∈ X converges to the fixed point: xn −−−−→\nn→∞ xf .\nContraction mappings are functions that map two points such that their distance is decreasing:\nDefinition 2 (Contraction mapping). A function f : X → X on a metric space X with distance d is a contraction mapping, if there is a 0 6 δ < 1, such that for all points u and v in X: d(f(u), f(v)) 6 δd(u,v).\nTo show that g is a contraction mapping in Ω with distance ‖.‖2, we use the Mean Value Theorem for u, v ∈ Ω\n‖g(u)− g(v)‖2 6M ‖u− v‖2, (17)\nin which M is an upper bound on the spectral norm the JacobianH of g. The spectral norm is given by the largest singular value of the Jacobian of g. If the largest singular value of the Jacobian is smaller than 1, the mapping g of the mean and variance to the mean and variance in the next layer is contracting. We show that the largest singular value is smaller than 1 by evaluating the function for the singular value S(µ, ω, ν, τ, λ, α) on a grid. Then we use the Mean Value Theorem to bound the deviation of the function S between grid points. To this end, we have to bound the gradient of S with respect to (µ, ω, ν, τ). If all function values plus gradient times the deltas (differences between grid points and evaluated points) is still smaller than 1, then we have proofed that the function is below 1 (Lemma 12). To show that the mapping does not map outside the function’s domain, we derive bounds on the expressions for the mean and the variance (Lemma 13). Section A3.4.1 and Section A3.4.2 are concerned with the contraction mapping and the image of the function domain of g, respectively.\nWith the results that the largest singular value of the Jacobian is smaller than one (Lemma 12) and that the mapping stays in the domain Ω (Lemma 13), we can prove Theorem 1. We first recall Theorem 1:\nTheorem (Stable and Attracting Fixed Points). We assume α = α01 and λ = λ01. We restrict the range of the variables to the domain µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.95, 1.1]. For ω = 0 and τ = 1, the mapping Eq. (4) and Eq. (5) has the stable fixed point (µ, ν) = (0, 1). For other ω and τ the mapping Eq. (4) and Eq. (5) has a stable and attracting fixed point depending on (ω, τ) in the (µ, ν)-domain: µ ∈ [−0.03106, 0.06773] and ν ∈ [0.80009, 1.48617]. All points within the (µ, ν)-domain converge when iteratively applying the mapping Eq. (4) and Eq. (5) to this fixed point.\nProof. According to Lemma 12 the mapping g (Eq. (4) and Eq. (5)) is a contraction mapping in the given domain, that is, it has a Lipschitz constant smaller than one. We showed that (µ, ν) = (0, 1) is a fixed point of the mapping for (ω, τ) = (0, 1).\nThe domain is compact (bounded and closed), therefore it is a complete metric space. We further have to make sure the mapping g does not map outside its domain Ω. According to Lemma 13, the mapping maps into the domain µ ∈ [−0.03106, 0.06773] and ν ∈ [0.80009, 1.48617]. Now we can apply the Banach fixed point theorem given in Theorem 4 from which the statement of the theorem follows.\nA3.2 Proof of Theorem 2\nFirst we recall Theorem 2: Theorem (Decreasing ν). For λ = λ01, α = α01 and the domain Ω++: −1 6 µ 6 1, −0.1 6 ω 6 0.1, 3 6 ν 6 16, and 0.8 6 τ 6 1.25 we have for the mapping of the variance ν̃(µ, ω, ν, τ, λ, α) given in Eq. (5)\nν̃(µ, ω, ν, τ, λ01, α01) < ν . (18)\nThe variance decreases in [3, 16] and all fixed points (µ, ν) of mapping Eq. (5) and Eq. (4) have ν < 3.\nProof. We start to consider an even larger domain −1 6 µ 6 1, −0.1 6 ω 6 0.1, 1.5 6 ν 6 16, and 0.8 6 τ 6 1.25. We prove facts for this domain and later restrict to 3 6 ν 6 16, i.e. Ω++. We consider the function g of the difference between the second moment ξ̃ in the next layer and the variance ν in the lower layer:\ng(µ, ω, ν, τ, λ01, α01) = ξ̃(µ, ω, ν, τ, λ01, α01) − ν . (19)\nIf we can show that g(µ, ω, ν, τ, λ01, α01) < 0 for all (µ, ω, ν, τ) ∈ Ω++, then we would obtain our desired result ν̃ 6 ξ̃ < ν. The derivative with respect to ν is according to Theorem 16:\n∂\n∂ν g(µ, ω, ν, τ, λ01, α01) =\n∂\n∂ν ξ̃(µ, ω, ν, τ, λ01, α01) − 1 < 0 . (20)\nTherefore g is strictly monotonically decreasing in ν. Since ξ̃ is a function in ντ (these variables only appear as this product), we have for x = ντ\n∂\n∂ν ξ̃ =\n∂ ∂x ξ̃ ∂x ∂ν = ∂ ∂x ξ̃ τ (21)\nand ∂\n∂τ ξ̃ =\n∂ ∂x ξ̃ ∂x ∂τ = ∂ ∂x ξ̃ ν . (22)\nTherefore we have according to Theorem 16:\n∂\n∂τ ξ̃(µ, ω, ν, τ, λ01, α01) =\nν\nτ\n∂\n∂ν ξ̃(µ, ω, ν, τ, λ01, α01) > 0 . (23)\nTherefore ∂\n∂τ g(µ, ω, ν, τ, λ01, α01) =\n∂\n∂τ ξ̃(µ, ω, ν, τ, λ01, α01) > 0 . (24)\nConsequently, g is strictly monotonically increasing in τ . Now we consider the derivative with respect to µ and ω. We start with ∂∂µ ξ̃(µ, ω, ν, τ, λ, α), which is\n∂\n∂µ ξ̃(µ, ω, ν, τ, λ, α) = (25)\nλ2ω ( α2 ( −eµω+ ντ2 ) erfc ( µω + ντ√\n2 √ ντ\n) +\nα2e2µω+2ντ erfc ( µω + 2ντ√\n2 √ ντ\n) + µω ( 2− erfc ( µω√ 2 √ ντ )) + √ 2 π √ ντe− µ2ω2 2ντ ) .\nWe consider the sub-function√ 2\nπ\n√ ντ − α2 ( e ( µω+ντ√ 2 √ ντ )2 erfc ( µω + ντ√\n2 √ ντ\n) − e ( µω+2ντ√ 2 √ ντ )2 erfc ( µω + 2ντ√\n2 √ ντ\n)) . (26)\nWe set x = ντ and y = µω and obtain√ 2\nπ\n√ x− α2 ( e ( x+y√ 2 √ x )2 erfc ( x+ y√\n2 √ x\n) − e ( 2x+y√ 2 √ x )2 erfc ( 2x+ y√\n2 √ x\n)) . (27)\nThe derivative to this sub-function with respect to y is α2 ( e (2x+y)2 2x (2x+ y) erfc (\n2x+y√ 2 √ x\n) − e (x+y)2 2x (x+ y) erfc ( x+y√ 2 √ x )) x = (28)\n√ 2α2 √ x  e (2x+y)22x (x+y) erfc( x+y√2√x)√ 2 √ x − e (x+y)2 2x (x+y) erfc ( x+y√ 2 √ x ) √ 2 √ x  x > 0 .\nThe inequality follows from Lemma 24, which states that zez 2\nerfc(z) is monotonically increasing in z. Therefore the sub-function is increasing in y. The derivative to this sub-function with respect to x is\n1 2 √ πx2 √ πα2\n( e (2x+y)2 2x ( 4x2 − y2 ) erfc ( 2x+ y√\n2 √ x\n) (29)\n−e (x+y)2 2x (x− y)(x+ y) erfc ( x+ y√\n2 √ x\n)) − √ 2 ( α2 − 1 ) x3/2.\nThe sub-function is increasing in x, since the derivative is larger than zero: √ πα2 ( e (2x+y)2 2x ( 4x2 − y2 ) erfc ( 2x+y√\n2 √ x\n) − e (x+y)2 2x (x− y)(x+ y) erfc ( x+y√ 2 √ x )) − √ 2x3/2 ( α2 − 1 ) 2 √ πx2 >\n(30)\n√ πα2  (2x−y)(2x+y)2√ π ( 2x+y√ 2 √ x + √( 2x+y√ 2 √ x )2 +2 ) − (x−y)(x+y)2 √ π ( x+y√ 2 √ x + √( x+y√ 2 √ x )2 + 4π ) −√2x3/2 (α2 − 1)\n2 √ πx2\n=\n√ πα2\n( (2x−y)(2x+y)2( √ 2 √ x)\n√ π ( 2x+y+ √ (2x+y)2+4x ) − (x−y)(x+y)2(√2√x)√ π ( x+y+ √ (x+y)2+ 8xπ ))−√2x3/2 (α2 − 1) 2 √ πx2 =\n√ πα2\n( (2x−y)(2x+y)2\n√ π ( 2x+y+ √ (2x+y)2+4x ) − (x−y)(x+y)2√ π ( x+y+ √ (x+y)2+ 8xπ ))− x (α2 − 1) √\n2 √ πx3/2\n>\n√ πα2\n( (2x−y)(2x+y)2\n√ π ( 2x+y+ √ (2x+y)2+2(2x+y)+1 ) − (x−y)(x+y)2√ π ( x+y+ √ (x+y)2+0.878·2(x+y)+0.8782 ))− x (α2 − 1) √\n2 √ πx3/2\n=\n√ πα2\n( (2x−y)(2x+y)2\n√ π ( 2x+y+ √ (2x+y+1)2 ) − (x−y)(x+y)2√ π ( x+y+ √ (x+y+0.878)2 ))− x (α2 − 1) √\n2 √ πx3/2\n=\n√ πα2 ( (2x−y)(2x+y)2√ π(2(2x+y)+1) − (x−y)(x+y)2√ π(2(x+y)+0.878) ) − x ( α2 − 1 ) √\n2 √ πx3/2\n=\n√ πα2 ( (2(x+y)+0.878)(2x−y)(2x+y)2√\nπ − (x−y)(x+y)(2(2x+y)+1)2√ π ) (2(2x+ y) + 1)(2(x+ y) + 0.878) √ 2 √ πx3/2 +\n√ πα2 ( −x ( α2 − 1 ) (2(2x+ y) + 1)(2(x+ y) + 0.878) ) (2(2x+ y) + 1)(2(x+ y) + 0.878) √ 2 √ πx3/2 =\n8x3 + 12x2y + 4.14569x2 + 4xy2 − 6.76009xy − 1.58023x+ 0.683154y2\n(2(2x+ y) + 1)(2(x+ y) + 0.878) √ 2 √ πx3/2\n>\n8x3 − 0.1 · 12x2 + 4.14569x2 + 4 · (0.0)2x− 6.76009 · 0.1x− 1.58023x+ 0.683154 · (0.0)2\n(2(2x+ y) + 1)(2(x+ y) + 0.878) √ 2 √ πx3/2\n=\n8x2 + 2.94569x− 2.25624 (2(2x+ y) + 1)(2(x+ y) + 0.878) √ 2 √ π √ x =\n8(x− 0.377966)(x+ 0.746178) (2(2x+ y) + 1)(2(x+ y) + 0.878) √ 2 √ π √ x > 0 .\nWe explain this chain of inequalities:\n• First inequality: We applied Lemma 22 two times. • Equalities factor out √ 2 √ x and reformulate.\n• Second inequality part 1: we applied\n0 < 2y =⇒ (2x+ y)2 + 4x+ 1 < (2x+ y)2 + 2(2x+ y) + 1 = (2x+ y + 1)2 . (31)\n• Second inequality part 2: we show that for a = 110 (√ 960+169π π − 13 ) following holds:\n8x π −\n( a2 + 2a(x+ y) ) > 0. We have ∂∂x 8x π − ( a2 + 2a(x+ y) ) = 8π − 2a > 0 and\n∂ ∂y 8x π −\n( a2 + 2a(x+ y) ) = −2a < 0. Therefore the minimum is at border for minimal x\nand maximal y:\n8 · 1.2 π −  2 10 (√ 960 + 169π π − 13 ) (1.2 + 0.1) + ( 1 10 (√ 960 + 169π π − 13 ))2 = 0 . (32)\nThus\n8x\nπ > a2 + 2a(x+ y) . (33)\nfor a = 110 (√ 960+169π π − 13 ) > 0.878.\n• Equalities only solve square root and factor out the resulting terms (2(2x + y) + 1) and (2(x+ y) + 0.878).\n• We set α = α01 and multiplied out. Thereafter we also factored out x in the numerator. Finally a quadratic equations was solved.\nThe sub-function has its minimal value for minimal x = ντ = 1.5 · 0.8 = 1.2 and minimal y = µω = −1 · 0.1 = −0.1. We further minimize the function\nµωe µ2ω2 2ντ ( 2− erfc ( µω√ 2 √ ντ )) > −0.1e 0.1 2 2·1.2 ( 2− erfc ( 0.1√ 2 √ 1.2 )) . (34)\nWe compute the minimum of the term in brackets of ∂∂µ ξ̃(µ, ω, ν, τ, λ, α) in Eq. (25):\nµωe µ2ω2 2ντ ( 2− erfc ( µω√ 2 √ ντ )) + (35)\nα201\n( − ( e ( µω+ντ√ 2 √ ντ )2 erfc ( µω + ντ√\n2 √ ντ\n) − e ( µω+2ντ√ 2 √ ντ )2 erfc ( µω + 2ντ√\n2 √ ντ\n))) + √ 2\nπ\n√ ντ >\nα201\n( − ( e ( 1.2−0.1√ 2 √ 1.2 )2 erfc ( 1.2− 0.1√\n2 √ 1.2\n) − e ( 2·1.2−0.1√ 2 √ 1.2 )2 erfc ( 2 · 1.2− 0.1√\n2 √ 1.2\n))) −\n0.1e 0.12 2·1.2 ( 2− erfc ( 0.1√ 2 √ 1.2 )) + √ 1.2 √ 2 π = 0.212234 .\nTherefore the term in brackets of Eq. (25) is larger than zero. Thus, ∂∂µ ξ̃(µ, ω, ν, τ, λ, α) has the sign\nof ω. Since ξ̃ is a function in µω (these variables only appear as this product), we have for x = µω ∂\n∂ν ξ̃ =\n∂ ∂x ξ̃ ∂x ∂µ = ∂ ∂x ξ̃ ω (36)\nand ∂\n∂ω ξ̃ =\n∂ ∂x ξ̃ ∂x ∂ω = ∂ ∂x ξ̃ µ . (37)\n∂\n∂ω ξ̃(µ, ω, ν, τ, λ01, α01) =\nµ\nω\n∂\n∂µ ξ̃(µ, ω, ν, τ, λ01, α01) . (38)\nSince ∂∂µ ξ̃ has the sign of ω, ∂ ∂µ ξ̃ has the sign of µ. Therefore\n∂\n∂ω g(µ, ω, ν, τ, λ01, α01) =\n∂\n∂ω ξ̃(µ, ω, ν, τ, λ01, α01) (39)\nhas the sign of µ.\nWe now divide the µ-domain into −1 6 µ 6 0 and 0 6 µ 6 1. Analogously we divide the ω-domain into −0.1 6 ω 6 0 and 0 6 ω 6 0.1. In this domains g is strictly monotonically. For all domains g is strictly monotonically decreasing in ν and strictly monotonically increasing in τ . Note that we now consider the range 3 6 ν 6 16. For the maximal value of g we set ν = 3 (we set it to 3!) and τ = 1.25.\nWe consider now all combination of these domains:\n• −1 6 µ 6 0 and −0.1 6 ω 6 0: g is decreasing in µ and decreasing in ω. We set µ = −1 and ω = −0.1.\ng(−1,−0.1, 3, 1.25, λ01, α01) = −0.0180173 . (40)\n• −1 6 µ 6 0 and 0 6 ω 6 0.1: g is increasing in µ and decreasing in ω. We set µ = 0 and ω = 0.\ng(0, 0, 3, 1.25, λ01, α01) = −0.148532 . (41)\n• 0 6 µ 6 1 and −0.1 6 ω 6 0: g is decreasing in µ and increasing in ω. We set µ = 0 and ω = 0.\ng(0, 0, 3, 1.25, λ01, α01) = −0.148532 . (42)\n• 0 6 µ 6 1 and 0 6 ω 6 0.1: g is increasing in µ and increasing in ω. We set µ = 1 and ω = 0.1.\ng(1, 0.1, 3, 1.25, λ01, α01) = −0.0180173 . (43)\nTherefore the maximal value of g is −0.0180173.\nA3.3 Proof of Theorem 3\nFirst we recall Theorem 3:\nTheorem (Increasing ν). We consider λ = λ01, α = α01 and the two domains Ω−1 = {(µ, ω, ν, τ) | − 0.1 6 µ 6 0.1,−0.1 6 ω 6 0.1, 0.05 6 ν 6 0.16, 0.8 6 τ 6 1.25} and Ω−2 = {(µ, ω, ν, τ) | − 0.1 6 µ 6 0.1,−0.1 6 ω 6 0.1, 0.05 6 ν 6 0.24, 0.9 6 τ 6 1.25} . The mapping of the variance ν̃(µ, ω, ν, τ, λ, α) given in Eq. (5) increases\nν̃(µ, ω, ν, τ, λ01, α01) > ν (44)\nin both Ω−1 and Ω − 2 . All fixed points (µ, ν) of mapping Eq. (5) and Eq. (4) ensure for 0.8 6 τ that ν̃ > 0.16 and for 0.9 6 τ that ν̃ > 0.24. Consequently, the variance mapping Eq. (5) and Eq. (4) ensures a lower bound on the variance ν.\nProof. The mean value theorem states that there exists a t ∈ [0, 1] for which\nξ̃(µ, ω, ν, τ, λ01, α01) − ξ̃(µ, ω, νmin, τ, λ01, α01) = (45) ∂ ∂ν ξ̃(µ, ω, ν + t(νmin − ν), τ, λ01, α01) (ν − νmin) .\nTherefore\nξ̃(µ, ω, ν, τ, λ01, α01) = ξ̃(µ, ω, νmin, τ, λ01, α01) + (46) ∂ ∂ν ξ̃(µ, ω, ν + t(νmin − ν), τ, λ01, α01) (ν − νmin) .\nTherefore we are interested to bound the derivative of the ξ-mapping Eq. (13) with respect to ν:\n∂\n∂ν ξ̃(µ, ω, ν, τ, λ01, α01) = (47)\n1 2 λ2τe− µ2ω2 2ντ\n( α2 ( − ( e ( µω+ντ√ 2 √ ντ )2 erfc ( µω + ντ√\n2 √ ντ\n) − 2e ( µω+2ντ√ 2 √ ντ )2 erfc ( µω + 2ντ√\n2 √ ντ\n))) −\nerfc ( µω√ 2 √ ντ ) + 2 ) .\nThe sub-term Eq. (308) enters the derivative Eq. (47) with a negative sign! According to Lemma 18, the minimal value of sub-term Eq. (308) is obtained by the largest largest ν, by the smallest τ , and the largest y = µω = 0.01. Also the positive term erfc ( µω√ 2 √ ντ ) + 2 is multiplied by τ , which is minimized by using the smallest τ . Therefore we can use the smallest τ in whole formula Eq. (47) to lower bound it.\nFirst we consider the domain 0.05 6 ν 6 0.16 and 0.8 6 τ 6 1.25. The factor consisting of the exponential in front of the brackets has its smallest value for e− 0.01·0.01 2·0.05·0.8 . Since erfc is monotonically decreasing we inserted the smallest argument via erfc ( − 0.01√\n2 √ 0.05·0.8\n) in order to obtain the maximal\nnegative contribution. Thus, applying Lemma 18, we obtain the lower bound on the derivative:\n1 2 λ2τe− µ2ω2 2ντ\n( α2 ( − ( e ( µω+ντ√ 2 √ ντ )2 erfc ( µω + ντ√\n2 √ ντ\n) − 2e ( µω+2ντ√ 2 √ ντ )2 erfc ( µω + 2ντ√\n2 √ ντ\n))) −\n(48)\nerfc ( µω√ 2 √ ντ ) + 2 ) >\n1 2 0.8e− 0.01·0.01 2·0.05·0.8λ201\n( α201 ( − ( e ( 0.16·0.8+0.01√ 2 √ 0.16·0.8 )2 erfc ( 0.16 · 0.8 + 0.01√\n2 √ 0.16 · 0.8\n) −\n2e\n( 2·0.16·0.8+0.01√\n2 √ 0.16·0.8 )2 erfc ( 2 · 0.16 · 0.8 + 0.01√\n2 √ 0.16 · 0.8\n))) − erfc ( − 0.01√\n2 √ 0.05 · 0.8\n) + 2 ) ) > 0.969231 .\nFor applying the mean value theorem, we require the smallest ν̃(ν). We follow the proof of Lemma 8, which shows that at the minimum y = µω must be maximal and x = ντ must be minimal. Thus, the smallest ξ̃(µ, ω, ν, τ, λ01, α01) is ξ̃(0.01, 0.01, 0.05, 0.8, λ01, α01) = 0.0662727 for 0.05 6 ν and 0.8 6 τ .\nTherefore the mean value theorem and the bound on (µ̃)2 (Lemma 43) provide\nν̃ = ξ̃(µ, ω, ν, τ, λ01, α01)− (µ̃(µ, ω, ν, τ, λ01, α01))2 > (49) 0.0662727 + 0.969231(ν − 0.05)− 0.005 = 0.01281115 + 0.969231ν > 0.08006969 · 0.16 + 0.969231ν > 1.049301ν > ν .\nNext we consider the domain 0.05 6 ν 6 0.24 and 0.9 6 τ 6 1.25. The factor consisting of the exponential in front of the brackets has its smallest value for e− 0.01·0.01 2·0.05·0.9 . Since erfc is monotonically decreasing we inserted the smallest argument via erfc ( − 0.01√\n2 √ 0.05·0.9\n) in order to obtain the maximal\nnegative contribution.\nThus, applying Lemma 18, we obtain the lower bound on the derivative: 1 2 λ2τe− µ2ω2 2ντ ( α2 ( − ( e ( µω+ντ√ 2 √ ντ )2 erfc ( µω + ντ√ 2 √ ντ ) − 2e ( µω+2ντ√ 2 √ ντ )2 erfc ( µω + 2ντ√ 2 √ ντ ))) −\n(50)\nerfc ( µω√ 2 √ ντ ) + 2 ) >\n1 2 0.9e− 0.01·0.01 2·0.05·0.9λ201\n( α201 ( − ( e ( 0.24·0.9+0.01√ 2 √ 0.24·0.9 )2 erfc ( 0.24 · 0.9 + 0.01√\n2 √ 0.24 · 0.9\n) −\n2e\n( 2·0.24·0.9+0.01√\n2 √ 0.24·0.9 )2 erfc ( 2 · 0.24 · 0.9 + 0.01√\n2 √ 0.24 · 0.9\n))) − erfc ( − 0.01√\n2 √ 0.05 · 0.9\n) + 2 ) ) > 0.976952 .\nFor applying the mean value theorem, we require the smallest ν̃(ν). We follow the proof of Lemma 8, which shows that at the minimum y = µω must be maximal and x = ντ must be minimal. Thus, the smallest ξ̃(µ, ω, ν, τ, λ01, α01) is ξ̃(0.01, 0.01, 0.05, 0.9, λ01, α01) = 0.0738404 for 0.05 6 ν and 0.9 6 τ . Therefore the mean value theorem and the bound on (µ̃)2 (Lemma 43) gives\nν̃ = ξ̃(µ, ω, ν, τ, λ01, α01)− (µ̃(µ, ω, ν, τ, λ01, α01))2 > (51) 0.0738404 + 0.976952(ν − 0.05)− 0.005 = 0.0199928 + 0.976952ν > 0.08330333 · 0.24 + 0.976952ν > 1.060255ν > ν .\nA3.4 Lemmata and Other Tools Required for the Proofs\nA3.4.1 Lemmata for proofing Theorem 1 (part 1): Jacobian norm smaller than one\nIn this section, we show that the largest singular value of the Jacobian of the mapping g is smaller than one. Therefore, g is a contraction mapping. This is even true in a larger domain than the original Ω. We do not need to restrict τ ∈ [0.95, 1.1], but we can extend to τ ∈ [0.8, 1.25]. The range of the other variables is unchanged such that we consider the following domain throughout this section: µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.8, 1.25].\nJacobian of the mapping. In the following, we denote two Jacobians: (1) the Jacobian J of the mapping h : (µ, ν) 7→ (µ̃, ξ̃), and (2) the JacobianH of the mapping g : (µ, ν) 7→ (µ̃, ν̃) because the influence of µ̃ on ν̃ is small, and many properties of the system can already be seen on J .\nJ = ( J11 J12 J21 J22 ) = ( ∂ ∂µ µ̃ ∂ ∂ν µ̃\n∂ ∂µ ξ̃ ∂ ∂ν ξ̃\n) (52)\nH = ( H11 H12 H21 H22 ) = ( J11 J12 J21 − 2µ̃J11 J22 − 2µ̃J12 ) (53)\nThe definition of the entries of the Jacobian J is:\nJ11(µ, ω, ν, τ, λ, α) = ∂\n∂µ µ̃(µ, ω, ν, τ, λ, α) = (54)\n1 2 λω\n( αeµω+ ντ 2 erfc ( µω + ντ√\n2 √ ντ\n) − erfc ( µω√ 2 √ ντ ) + 2 ) J12(µ, ω, ν, τ, λ, α) = ∂\n∂ν µ̃(µ, ω, ν, τ, λ, α) = (55)\n1 4 λτ\n( αeµω+ ντ 2 erfc ( µω + ντ√\n2 √ ντ\n) − (α− 1) √ 2\nπντ e−\nµ2ω2\n2ντ\n)\nJ21(µ, ω, ν, τ, λ, α) = ∂\n∂µ ξ̃(µ, ω, ν, τ, λ, α) = (56)\nλ2ω ( α2 ( −eµω+ ντ2 ) erfc ( µω + ντ√\n2 √ ντ\n) +\nα2e2µω+2ντ erfc ( µω + 2ντ√\n2 √ ντ\n) + µω ( 2− erfc ( µω√ 2 √ ντ )) + √ 2 π √ ντe− µ2ω2 2ντ )\nJ22(µ, ω, ν, τ, λ, α) = ∂\n∂ν ξ̃(µ, ω, ν, τ, λ, α) = (57)\n1 2 λ2τ\n( α2 ( −eµω+ ντ2 ) erfc ( µω + ντ√\n2 √ ντ\n) +\n2α2e2µω+2ντ erfc ( µω + 2ντ√\n2 √ ντ\n) − erfc ( µω√ 2 √ ντ ) + 2 )\nProof sketch: Bounding the largest singular value of the Jacobian. If the largest singular value of the Jacobian is smaller than 1, then the spectral norm of the Jacobian is smaller than 1. Then the mapping Eq. (4) and Eq. (5) of the mean and variance to the mean and variance in the next layer is contracting.\nWe show that the largest singular value is smaller than 1 by evaluating the function S(µ, ω, ν, τ, λ, α) on a grid. Then we use the Mean Value Theorem to bound the deviation of the function S between grid points. Toward this end we have to bound the gradient of S with respect to (µ, ω, ν, τ). If all function values plus gradient times the deltas (differences between grid points and evaluated points) is still smaller than 1, then we have proofed that the function is below 1.\nThe singular values of the 2× 2 matrix\nA = ( a11 a12 a21 a22 ) (58)\nare\ns1 = 1\n2\n(√ (a11 + a22)2 + (a21 − a12)2 + √ (a11 − a22)2 + (a12 + a21)2 ) (59)\ns2 = 1\n2\n(√ (a11 + a22)2 + (a21 − a12)2 − √ (a11 − a22)2 + (a12 + a21)2 ) . (60)\nWe used an explicit formula for the singular values [4]. We now setH11 = a11,H12 = a12,H21 = a21,H22 = a22 to obtain a formula for the largest singular value of the Jacobian depending on (µ, ω, ν, τ, λ, α). The formula for the largest singular value for the Jacobian is: S(µ, ω, ν, τ, λ, α) = (√ (H11 +H22)2 + (H21 −H12)2 + √ (H11 −H22)2 + (H12 +H21)2 ) =\n(61)\n= 1\n2 (√ (J11 + J22 − 2µ̃J12)2 + (J21 − 2µ̃J11 − J12)2 +√\n(J11 − J22 + 2µ̃J12)2 + (J12 + J21 − 2µ̃J11)2 ) ,\nwhere J are defined in Eq. (54) and we left out the dependencies on (µ, ω, ν, τ, λ, α) in order to keep the notation uncluttered, e.g. we wrote J11 instead of J11(µ, ω, ν, τ, λ, α).\nBounds on the derivatives of the Jacobian entries. In order to bound the gradient of the singular value, we have to bound the derivatives of the Jacobian entries J11(µ, ω, ν, τ, λ, α), J12(µ, ω, ν, τ, λ, α), J21(µ, ω, ν, τ, λ, α), and J22(µ, ω, ν, τ, λ, α) with respect to µ, ω, ν, and τ . The values λ and α are fixed to λ01 and α01. The 16 derivatives of the 4 Jacobian entries with respect to the 4 variables are:\n∂J11 ∂µ = 1 2 λω2e− µ2ω2 2ντ αe (µω+ντ)22ντ erfc(µω + ντ√ 2 √ ντ ) − √ 2 π (α− 1)√ ντ  (62) ∂J11 ∂ω = 1 2 λ −e−µ2ω22ντ  √ 2 π (α− 1)µω√ ντ − α(µω + 1)e (µω+ντ)2 2ντ erfc ( µω + ντ√ 2 √ ντ\n) − erfc ( µω√ 2 √ ντ ) + 2\n) ∂J11 ∂ν = 1 4 λτωe− µ2ω2 2ντ ( αe (µω+ντ)2 2ντ erfc ( µω + ντ√ 2 √ ντ ) + √ 2 π ( (α− 1)µω (ντ)3/2 − α√ ντ\n)) ∂J11 ∂τ = 1 4 λνωe− µ2ω2 2ντ ( αe (µω+ντ)2 2ντ erfc ( µω + ντ√ 2 √ ντ ) + √ 2 π ( (α− 1)µω (ντ)3/2 − α√ ντ\n)) ∂J12 ∂µ = ∂J11 ∂ν\n∂J12 ∂ω = 1 4 λµτe− µ2ω2 2ντ\n( αe (µω+ντ)2 2ντ erfc ( µω + ντ√\n2 √ ντ\n) + √ 2\nπ\n( (α− 1)µω\n(ντ)3/2 − α√ ντ )) ∂J12 ∂ν = 1 8 λe− µ2ω2 2ντ ( ατ2e (µω+ντ)2 2ντ erfc ( µω + ντ√ 2 √ ντ ) +√\n2\nπ\n( (−1)(α− 1)µ2ω2\nν5/2 √ τ\n+\n√ τ(α+ αµω − 1)\nν3/2 − ατ\n3/2\n√ ν )) ∂J12 ∂τ = 1 8 λe− µ2ω2 2ντ ( 2αe (µω+ντ)2 2ντ erfc ( µω + ντ√ 2 √ ντ ) + αντe (µω+ντ)2 2ντ erfc ( µω + ντ√ 2 √ ντ ) +√\n2\nπ\n( (−1)(α− 1)µ2ω2\n(ντ)3/2 + −α+ αµω + 1√ ντ − α √ ντ )) ∂J21 ∂µ = λ2ω2 ( α2 ( −e− µ2ω2 2ντ ) e (µω+ντ)2 2ντ erfc ( µω + ντ√ 2 √ ντ ) +\n2α2e (µω+2ντ)2 2ντ e− µ2ω2 2ντ erfc ( µω + 2ντ√\n2 √ ντ\n) − erfc ( µω√ 2 √ ντ ) + 2 ) ∂J21 ∂ω = λ2 ( α2(µω + 1) ( −e− µ2ω2 2ντ ) e (µω+ντ)2 2ντ erfc ( µω + ντ√ 2 √ ντ ) +\nα2(2µω + 1)e (µω+2ντ)2 2ντ e− µ2ω2 2ντ erfc ( µω + 2ντ√\n2 √ ντ\n) +\n2µω ( 2− erfc ( µω√ 2 √ ντ )) + √ 2 π √ ντe− µ2ω2 2ντ ) ∂J21 ∂ν = 1 2 λ2τωe− µ2ω2 2ντ ( α2 ( −e (µω+ντ)2 2ντ ) erfc ( µω + ντ√ 2 √ ντ ) +\n4α2e (µω+2ντ)2 2ντ erfc ( µω + 2ντ√\n2 √ ντ\n) + √ 2 π (−1) ( α2 − 1 ) √ ντ  ∂J21 ∂τ = 1 2 λ2νωe− µ2ω2 2ντ ( α2 ( −e (µω+ντ)2 2ντ ) erfc ( µω + ντ√ 2 √ ντ ) +\n4α2e (µω+2ντ)2 2ντ erfc ( µω + 2ντ√\n2 √ ντ\n) + √ 2 π (−1) ( α2 − 1 ) √ ντ  ∂J22 ∂µ = ∂J21 ∂ν\n∂J22 ∂ω = 1 2 λ2µτe− µ2ω2 2ντ\n( α2 ( −e (µω+ντ)2 2ντ ) erfc ( µω + ντ√\n2 √ ντ\n) +\n4α2e (µω+2ντ)2 2ντ erfc ( µω + 2ντ√\n2 √ ντ\n) + √ 2 π (−1) ( α2 − 1 ) √ ντ  ∂J22 ∂ν = 1 4 λ2τ2e− µ2ω2 2ντ ( α2 ( −e (µω+ντ)2 2ντ ) erfc ( µω + ντ√ 2 √ ντ ) +\n8α2e (µω+2ντ)2 2ντ erfc ( µω + 2ντ√\n2 √ ντ\n) + √ 2\nπ\n(( α2 − 1 ) µω\n(ντ)3/2 − 3α\n2 √ ντ )) ∂J22 ∂τ = 1 4 λ2 ( −2α2e− µ2ω2 2ντ e (µω+ντ)2 2ντ erfc ( µω + ντ√ 2 √ ντ ) −\nα2ντe− µ2ω2 2ντ e (µω+ντ)2 2ντ erfc ( µω + ντ√\n2 √ ντ\n) + 4α2e (µω+2ντ)2 2ντ e− µ2ω2 2ντ erfc ( µω + 2ντ√\n2 √ ντ\n) +\n8α2ντe (µω+2ντ)2 2ντ e− µ2ω2 2ντ erfc ( µω + 2ντ√\n2 √ ντ\n) + 2 ( 2− erfc ( µω√ 2 √ ντ )) +√\n2 π e− µ2ω2 2ντ\n(( α2 − 1 ) µω\n√ ντ\n− 3α2 √ ντ )) Lemma 5 (Bounds on the Derivatives). The following bounds on the absolute values of the derivatives of the Jacobian entries J11(µ, ω, ν, τ, λ, α), J12(µ, ω, ν, τ, λ, α), J21(µ, ω, ν, τ, λ, α), and J22(µ, ω, ν, τ, λ, α) with respect to µ, ω, ν, and τ hold:∣∣∣∣∂J11∂µ\n∣∣∣∣ < 0.0031049101995398316 (63)∣∣∣∣∂J11∂ω ∣∣∣∣ < 1.055872374194189∣∣∣∣∂J11∂ν ∣∣∣∣ < 0.031242911235461816∣∣∣∣∂J11∂τ ∣∣∣∣ < 0.03749149348255419\n∣∣∣∣∂J12∂µ ∣∣∣∣ < 0.031242911235461816\n∣∣∣∣∂J12∂ω ∣∣∣∣ < 0.031242911235461816∣∣∣∣∂J12∂ν ∣∣∣∣ < 0.21232788238624354∣∣∣∣∂J12∂τ ∣∣∣∣ < 0.2124377655377270\n∣∣∣∣∂J21∂µ ∣∣∣∣ < 0.02220441024325437∣∣∣∣∂J21∂ω ∣∣∣∣ < 1.146955401845684∣∣∣∣∂J21∂ν ∣∣∣∣ < 0.14983446469110305∣∣∣∣∂J21∂τ ∣∣∣∣ < 0.17980135762932363\n∣∣∣∣∂J22∂µ ∣∣∣∣ < 0.14983446469110305∣∣∣∣∂J22∂ω ∣∣∣∣ < 0.14983446469110305∣∣∣∣∂J22∂ν ∣∣∣∣ < 1.805740052651535∣∣∣∣∂J22∂τ ∣∣∣∣ < 2.396685907216327\nProof. See proof 39.\nBounds on the entries of the Jacobian. Lemma 6 (Bound on J11). The absolute value of the function J11 = 12λω ( αeµω+ ντ 2 erfc ( µω+ντ√ 2 √ ντ ) − erfc ( µω√ 2 √ ντ ) + 2 )\nis bounded by |J11| 6 0.104497 in the domain −0.1 6 µ 6 0.1, −0.1 6 ω 6 0.1, 0.8 6 ν 6 1.5, and 0.8 6 τ 6 1.25 for α = α01 and λ = λ01.\nProof.\n|J11| = ∣∣∣∣12λω ( αeµω+ ντ 2 erfc ( µω + ντ√ 2 √ ντ ) + 2− erfc ( µω√ 2 √ ντ ))∣∣∣∣ 6 |1\n2 ||λ||ω| (|α|0.587622 + 1.00584) 6 0.104497,\n(64) where we used that (a) J11 is strictly monotonically increasing in µω and |2 − erfc (\n0.01√ 2 √ ντ\n) | 6\n1.00584 and (b) Lemma 47 that |eµω+ ντ2 erfc ( µω+ντ√\n2 √ ντ\n) | 6 e0.01+ 0.642 erfc ( 0.01+0.64√\n2 √ 0.64\n) = 0.587622\nLemma 7 (Bound on J12). The absolute value of the function J12 = 14λτ ( αeµω+ ντ 2 erfc ( µω+ντ√ 2 √ ντ ) − (α− 1) √ 2 πντ e −µ 2ω2 2ντ ) is bounded by |J12| 6 0.194145 in the domain −0.1 6 µ 6 0.1, −0.1 6 ω 6 0.1, 0.8 6 ν 6 1.5, and 0.8 6 τ 6 1.25 for α = α01 and λ = λ01.\nProof.\n|J12| 6 1\n4 |λ||τ | ∣∣∣∣∣ ( αeµω+ ντ 2 erfc ( µω + ντ√ 2 √ ντ ) − (α− 1) √ 2 πντ e− µ2ω2 2ντ )∣∣∣∣∣ 6 1 4 |λ||τ | |0.983247− 0.392294| 6\n0.194035 (65)\nFor the first term we have 0.434947 6 eµω+ ντ 2 erfc ( µω+ντ√\n2 √ ντ\n) 6 0.587622 after Lemma 47 and for\nthe second term 0.582677 6 √\n2 πντ e\n−µ 2ω2\n2ντ 6 0.997356, which can easily be seen by maximizing or minimizing the arguments of the exponential or the square root function. The first term scaled by α is 0.727780 6 αeµω+ ντ 2 erfc ( µω+ντ√\n2 √ ντ\n) 6 0.983247 and the second term scaled by α − 1 is\n0.392294 6 (α − 1) √\n2 πντ e\n−µ 2ω2\n2ντ 6 0.671484. Therefore, the absolute difference between these terms is at most 0.983247− 0.392294 leading to the derived bound.\nBounds on mean, variance and second moment. For deriving bounds on µ̃, ξ̃, and ν̃, we need the following lemma. Lemma 8 (Derivatives of the Mapping). We assume α = α01 and λ = λ01. We restrict the range of the variables to the domain µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.8, 1.25].\nThe derivative ∂∂µ µ̃(µ, ω, ν, τ, λ, α) has the sign of ω.\nThe derivative ∂∂ν µ̃(µ, ω, ν, τ, λ, α) is positive.\nThe derivative ∂∂µ ξ̃(µ, ω, ν, τ, λ, α) has the sign of ω.\nThe derivative ∂∂ν ξ̃(µ, ω, ν, τ, λ, α) is positive.\nProof. See 40.\nLemma 9 (Bounds on mean, variance and second moment). The expressions µ̃, ξ̃, and ν̃ for α = α01 and λ = λ01 are bounded by −0.041160 < µ̃ < 0.087653, 0.703257 < ξ̃ < 1.643705 and 0.695574 < ν̃ < 1.636023 in the domain µ ∈ [−0.1, 0.1], ν ∈ [0.8, 15], ω ∈ [−0.1, 0.1], τ ∈ [0.8, 1.25].\nProof. We use Lemma 8 which states that with given sign the derivatives of the mapping Eq. (4) and Eq. (5) with respect to ν and µ are either positive or have the sign of ω. Therefore with given sign of ω the mappings are strict monotonic and the their maxima and minima are found at the borders. The minimum of µ̃ is obtained at µω = −0.01 and its maximum at µω = 0.01 and σ and τ at minimal or maximal values, respectively. It follows that −0.041160 < µ̃(−0.1, 0.1, 0.8, 0.8, λ01, α01) 6µ̃ 6 µ̃(0.1, 0.1, 1.5, 1.25, λ01, α01) < 0.087653.\n(66)\nSimilarly, the maximum and minimum of ξ̃ is obtained at the values mentioned above:\n0.703257 < ξ̃(−0.1, 0.1, 0.8, 0.8, λ01, α01) 6ξ̃ 6 ξ̃(0.1, 0.1, 1.5, 1.25, λ01, α01) < 1.643705. (67)\nHence we obtain the following bounds on ν̃:\n0.703257− µ̃2 < ξ̃ − µ̃2 < 1.643705− µ̃2 (68) 0.703257− 0.007683 < ν̃ < 1.643705− 0.007682\n0.695574 < ν̃ < 1.636023.\nUpper Bounds on the Largest Singular Value of the Jacobian. Lemma 10 (Upper Bounds on Absolute Derivatives of Largest Singular Value). We set α = α01 and λ = λ01 and restrict the range of the variables to µ ∈ [µmin, µmax] = [−0.1, 0.1], ω ∈ [ωmin, ωmax] = [−0.1, 0.1], ν ∈ [νmin, νmax] = [0.8, 1.5], and τ ∈ [τmin, τmax] = [0.8, 1.25]. The absolute values of derivatives of the largest singular value S(µ, ω, ν, τ, λ, α) given in Eq. (61) with respect to (µ, ω, ν, τ) are bounded as follows:\n∣∣∣∣∂S∂µ ∣∣∣∣ < 0.32112 , (69)∣∣∣∣∂S∂ω ∣∣∣∣ < 2.63690 , (70)∣∣∣∣∂S∂ν ∣∣∣∣ < 2.28242 , (71)∣∣∣∣∂S∂τ ∣∣∣∣ < 2.98610 . (72)\nProof. The Jacobian of our mapping Eq. (4) and Eq. (5) is defined as\nH = ( H11 H12 H21 H22 ) = ( J11 J12 J21 − 2µ̃J11 J22 − 2µ̃J12 ) (73)\nand has the largest singular value\nS(µ, ω, ν, τ, λ, α) = 1\n2\n(√ (H11 −H22)2 + (H12 +H21)2 + √ (H11 +H22)2 + (H12 −H21)2 ) ,\n(74)\naccording to the formula of Blinn [4].\nWe obtain∣∣∣∣ ∂S∂H11 ∣∣∣∣ = ∣∣∣∣∣12 ( H11 −H22√ (H11 −H22)2 + (H12 +H21)2 + H11 +H22√ (H11 +H22)2 + (H21 −H12)2 )∣∣∣∣∣ < (75)\n1\n2 ∣∣∣∣∣∣ 1√ (H12+H21)2 (H11−H22)2 + 1 ∣∣∣∣∣∣+ ∣∣∣∣∣∣ 1√ (H21−H12)2 (H11+H22)2 + 1 ∣∣∣∣∣∣  < 1 + 1 2 = 1\nand analogously∣∣∣∣ ∂S∂H12 ∣∣∣∣ = ∣∣∣∣∣12 ( H12 +H21√ (H11 −H22)2 + (H12 +H21)2 − H21 −H12√ (H11 +H22)2 + (H21 −H12)2 )∣∣∣∣∣ < 1 (76)\nand∣∣∣∣ ∂S∂H21 ∣∣∣∣ = ∣∣∣∣∣12 ( H21 −H12√ (H11 +H22)2 + (H21 −H12)2 + H12 +H21√ (H11 −H22)2 + (H12 +H21)2 )∣∣∣∣∣ < 1 (77)\nand∣∣∣∣ ∂S∂H22 ∣∣∣∣ = ∣∣∣∣∣12 ( H11 +H22√ (H11 +H22)2 + (H21 −H12)2 − H11 −H22√ (H11 −H22)2 + (H12 +H21)2 )∣∣∣∣∣ < 1 . (78)\nWe have ∂S\n∂µ =\n∂S ∂H11 ∂H11 ∂µ + ∂S ∂H12 ∂H12 ∂µ + ∂S ∂H21 ∂H21 ∂µ + ∂S ∂H22 ∂H22 ∂µ\n(79)\n∂S ∂ω = ∂S ∂H11 ∂H11 ∂ω + ∂S ∂H12 ∂H12 ∂ω + ∂S ∂H21 ∂H21 ∂ω + ∂S ∂H22 ∂H22 ∂ω\n(80)\n∂S ∂ν = ∂S ∂H11 ∂H11 ∂ν + ∂S ∂H12 ∂H12 ∂ν + ∂S ∂H21 ∂H21 ∂ν + ∂S ∂H22 ∂H22 ∂ν\n(81)\n∂S ∂τ = ∂S ∂H11 ∂H11 ∂τ + ∂S ∂H12 ∂H12 ∂τ + ∂S ∂H21 ∂H21 ∂τ + ∂S ∂H22 ∂H22 ∂τ\n(82)\n(83) from which follows using the bounds from Lemma 5: Derivative of the singular value w.r.t. µ:∣∣∣∣∂S∂µ ∣∣∣∣ 6 (84)∣∣∣∣ ∂S∂H11 ∣∣∣∣ ∣∣∣∣∂H11∂µ ∣∣∣∣+ ∣∣∣∣ ∂S∂H12 ∣∣∣∣ ∣∣∣∣∂H12∂µ ∣∣∣∣+ ∣∣∣∣ ∂S∂H21 ∣∣∣∣ ∣∣∣∣∂H21∂µ ∣∣∣∣+ ∣∣∣∣ ∂S∂H22 ∣∣∣∣ ∣∣∣∣∂H22∂µ\n∣∣∣∣ 6∣∣∣∣∂H11∂µ ∣∣∣∣+ ∣∣∣∣∂H12∂µ ∣∣∣∣+ ∣∣∣∣∂H21∂µ ∣∣∣∣+ ∣∣∣∣∂H22∂µ\n∣∣∣∣ 6∣∣∣∣∂J11∂µ ∣∣∣∣+ ∣∣∣∣∂J12∂µ ∣∣∣∣+ ∣∣∣∣∂J21 − 2µ̃J11∂µ ∣∣∣∣+ ∣∣∣∣∂J22 − 2µ̃J12∂µ\n∣∣∣∣ 6∣∣∣∣∂J11∂µ ∣∣∣∣+ ∣∣∣∣∂J12∂µ ∣∣∣∣+ ∣∣∣∣∂J21∂µ ∣∣∣∣+ ∣∣∣∣∂J22∂µ ∣∣∣∣+ 2 ∣∣∣∣∂J11∂µ ∣∣∣∣ |µ̃|+ 2 |J11|2 + 2 ∣∣∣∣∂J12∂µ\n∣∣∣∣ |µ̃|+ 2 |J12| |J11| 6 0.0031049101995398316 + 0.031242911235461816 + 0.02220441024325437 + 0.14983446469110305+\n2 · 0.104497 · 0.087653 + 2 · 0.1044972+ 2 · 0.194035 · 0.087653 + 2 · 0.104497 · 0.194035 < 0.32112,\nwhere we used the results from the lemmata 5, 6, 7, and 9. Derivative of the singular value w.r.t. ω:∣∣∣∣∂S∂ω ∣∣∣∣ 6 (85)∣∣∣∣ ∂S∂H11 ∣∣∣∣ ∣∣∣∣∂H11∂ω ∣∣∣∣+ ∣∣∣∣ ∂S∂H12 ∣∣∣∣ ∣∣∣∣∂H12∂ω ∣∣∣∣+ ∣∣∣∣ ∂S∂H21 ∣∣∣∣ ∣∣∣∣∂H21∂ω ∣∣∣∣+ ∣∣∣∣ ∂S∂H22 ∣∣∣∣ ∣∣∣∣∂H22∂ω\n∣∣∣∣ 6∣∣∣∣∂H11∂ω ∣∣∣∣+ ∣∣∣∣∂H12∂ω ∣∣∣∣+ ∣∣∣∣∂H21∂ω ∣∣∣∣+ ∣∣∣∣∂H22∂ω\n∣∣∣∣ 6∣∣∣∣∂J11∂ω ∣∣∣∣+ ∣∣∣∣∂J12∂ω ∣∣∣∣+ ∣∣∣∣∂J21 − 2µ̃J11∂ω ∣∣∣∣+ ∣∣∣∣∂J22 − 2µ̃J12∂ω\n∣∣∣∣ 6∣∣∣∣∂J11∂ω ∣∣∣∣+ ∣∣∣∣∂J12∂ω ∣∣∣∣+ ∣∣∣∣∂J21∂ω ∣∣∣∣+ ∣∣∣∣∂J22∂ω ∣∣∣∣+ 2 ∣∣∣∣∂J11∂ω ∣∣∣∣ |µ̃|+ 2 |J11| ∣∣∣∣∂µ̃∂ω\n∣∣∣∣+ 2 ∣∣∣∣∂J12∂ω ∣∣∣∣ |µ̃|+ 2 |J12| ∣∣∣∣∂µ̃∂ω\n∣∣∣∣ 6 (86) 2.38392 + 2 · 1.055872374194189 · 0.087653 + 2 · 0.1044972 + 2 · 0.031242911235461816 · 0.087653 + 2 · 0.194035 · 0.104497 < 2.63690 ,\nwhere we used the results from the lemmata 5, 6, 7, and 9 and that µ̃ is symmetric for µ, ω. Derivative of the singular value w.r.t. ν:∣∣∣∣∂S∂ν ∣∣∣∣ 6 (87)\n∣∣∣∣ ∂S∂H11 ∣∣∣∣ ∣∣∣∣∂H11∂ν ∣∣∣∣+ ∣∣∣∣ ∂S∂H12 ∣∣∣∣ ∣∣∣∣∂H12∂ν ∣∣∣∣+ ∣∣∣∣ ∂S∂H21 ∣∣∣∣ ∣∣∣∣∂H21∂ν ∣∣∣∣+ ∣∣∣∣ ∂S∂H22 ∣∣∣∣ ∣∣∣∣∂H22∂ν ∣∣∣∣ 6∣∣∣∣∂H11∂ν ∣∣∣∣+ ∣∣∣∣∂H12∂ν ∣∣∣∣+ ∣∣∣∣∂H21∂ν ∣∣∣∣+ ∣∣∣∣∂H22∂ν\n∣∣∣∣ 6∣∣∣∣∂J11∂ν ∣∣∣∣+ ∣∣∣∣∂J12∂ν ∣∣∣∣+ ∣∣∣∣∂J21 − 2µ̃J11∂ν ∣∣∣∣+ ∣∣∣∣∂J22 − 2µ̃J12∂ν\n∣∣∣∣ 6∣∣∣∣∂J11∂ν ∣∣∣∣+ ∣∣∣∣∂J12∂ν ∣∣∣∣+ ∣∣∣∣∂J21∂ν ∣∣∣∣+ ∣∣∣∣∂J22∂ν ∣∣∣∣+ 2 ∣∣∣∣∂J11∂ν ∣∣∣∣ |µ̃|+ 2 |J11| |J12|+ 2 ∣∣∣∣∂J12∂ν\n∣∣∣∣ |µ̃|+ 2 |J12|2 6 2.19916 + 2 · 0.031242911235461816 · 0.087653 + 2 · 0.104497 · 0.194035+ 2 · 0.21232788238624354 · 0.087653 + 2 · 0.1940352 < 2.28242 ,\nwhere we used the results from the lemmata 5, 6, 7, and 9.\nDerivative of the singular value w.r.t. τ :∣∣∣∣∂S∂τ ∣∣∣∣ 6 (88)∣∣∣∣ ∂S∂H11 ∣∣∣∣ ∣∣∣∣∂H11∂τ ∣∣∣∣+ ∣∣∣∣ ∂S∂H12 ∣∣∣∣ ∣∣∣∣∂H12∂τ ∣∣∣∣+ ∣∣∣∣ ∂S∂H21 ∣∣∣∣ ∣∣∣∣∂H21∂τ ∣∣∣∣+ ∣∣∣∣ ∂S∂H22 ∣∣∣∣ ∣∣∣∣∂H22∂τ\n∣∣∣∣ 6∣∣∣∣∂H11∂τ ∣∣∣∣+ ∣∣∣∣∂H12∂τ ∣∣∣∣+ ∣∣∣∣∂H21∂τ ∣∣∣∣+ ∣∣∣∣∂H22∂τ\n∣∣∣∣ 6∣∣∣∣∂J11∂τ ∣∣∣∣+ ∣∣∣∣∂J12∂τ ∣∣∣∣+ ∣∣∣∣∂J21 − 2µ̃J11∂τ ∣∣∣∣+ ∣∣∣∣∂J22 − 2µ̃J12∂τ\n∣∣∣∣ 6∣∣∣∣∂J11∂τ ∣∣∣∣+ ∣∣∣∣∂J12∂τ ∣∣∣∣+ ∣∣∣∣∂J21∂τ ∣∣∣∣+ ∣∣∣∣∂J22∂τ ∣∣∣∣+ 2 ∣∣∣∣∂J11∂τ ∣∣∣∣ |µ̃|+ 2 |J11| ∣∣∣∣∂µ̃∂τ\n∣∣∣∣+ 2 ∣∣∣∣∂J12∂τ ∣∣∣∣ |µ̃|+ 2 |J12| ∣∣∣∣∂µ̃∂τ\n∣∣∣∣ 6 (89) 2.82643 + 2 · 0.03749149348255419 · 0.087653 + 2 · 0.104497 · 0.194035+ 2 · 0.2124377655377270 · 0.087653 + 2 · 0.1940352 < 2.98610 ,\nwhere we used the results from the lemmata 5, 6, 7, and 9 and that µ̃ is symmetric for ν, τ .\nLemma 11 (Mean Value Theorem Bound on Deviation from Largest Singular Value). We set α = α01 and λ = λ01 and restrict the range of the variables to µ ∈ [µmin, µmax] = [−0.1, 0.1], ω ∈ [ωmin, ωmax] = [−0.1, 0.1], ν ∈ [νmin, νmax] = [0.8, 1.5], and τ ∈ [τmin, τmax] = [0.8, 1.25]. The distance of the singular value at S(µ, ω, ν, τ, λ01, α01) and that at S(µ + ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ01, α01) is bounded as follows:\n|S(µ+ ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ01, α01) − S(µ, ω, ν, τ, λ01, α01)| < (90) 0.32112 |∆µ|+ 2.63690 |∆ω|+ 2.28242 |∆ν|+ 2.98610 |∆τ | .\nProof. The mean value theorem states that a t ∈ [0, 1] exists for which S(µ+ ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ01, α01) − S(µ, ω, ν, τ, λ01, α01) = (91) ∂S\n∂µ (µ+ t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ01, α01) ∆µ +\n∂S ∂ω (µ+ t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ01, α01) ∆ω +\n∂S ∂ν (µ+ t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ01, α01) ∆ν +\n∂S ∂τ (µ+ t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ01, α01) ∆τ\nfrom which immediately follows that\n|S(µ+ ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ01, α01) − S(µ, ω, ν, τ, λ01, α01)| 6 (92)∣∣∣∣∂S∂µ (µ+ t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ01, α01) ∣∣∣∣ |∆µ| +∣∣∣∣∂S∂ω (µ+ t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ01, α01) ∣∣∣∣ |∆ω| +∣∣∣∣∂S∂ν (µ+ t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ01, α01) ∣∣∣∣ |∆ν| +∣∣∣∣∂S∂τ (µ+ t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ01, α01) ∣∣∣∣ |∆τ | .\nWe now apply Lemma 10 which gives bounds on the derivatives, which immediately gives the statement of the lemma.\nLemma 12 (Largest Singular Value Smaller Than One). We set α = α01 and λ = λ01 and restrict the range of the variables to µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.8, 1.25]. The the largest singular value of the Jacobian is smaller than 1:\nS(µ, ω, ν, τ, λ01, α01) < 1 . (93)\nTherefore the mapping Eq. (4) and Eq. (5) is a contraction mapping.\nProof. We set ∆µ = 0.0068097371, ∆ω = 0.0008292885, ∆ν = 0.0009580840, and ∆τ = 0.0007323095.\nAccording to Lemma 11 we have\n|S(µ+ ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ01, α01) − S(µ, ω, ν, τ, λ01, α01)| < (94) 0.32112 · 0.0068097371 + 2.63690 · 0.0008292885+ 2.28242 · 0.0009580840 + 2.98610 · 0.0007323095 < 0.008747 .\nFor a grid with grid length ∆µ = 0.0068097371, ∆ω = 0.0008292885, ∆ν = 0.0009580840, and ∆τ = 0.0007323095, we evaluated the function Eq. (61) for the largest singular value in the domain µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.8, 1.25]. We did this using a computer. According to Subsection A3.4.5 the precision if regarding error propagation and precision of the implemented functions is larger than 10−13. We performed the evaluation on different operating systems and different hardware architectures including CPUs and GPUs. In all cases the function Eq. (61) for the largest singular value of the Jacobian is bounded by 0.9912524171058772.\nWe obtain from Eq. (94):\nS(µ+ ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ01, α01) 6 0.9912524171058772 + 0.008747 < 1 . (95)\nA3.4.2 Lemmata for proofing Theorem 1 (part 2): Mapping within domain\nWe further have to investigate whether the the mapping Eq. (4) and Eq. (5) maps into a predefined domains. Lemma 13 (Mapping into the domain). The mapping Eq. (4) and Eq. (5) map for α = α01 and λ = λ01 into the domain µ ∈ [−0.03106, 0.06773] and ν ∈ [0.80009, 1.48617] with ω ∈ [−0.1, 0.1] and τ ∈ [0.95, 1.1].\nProof. We use Lemma 8 which states that with given sign the derivatives of the mapping Eq. (4) and Eq. (5) with respect to α = α01 and λ = λ01 are either positive or have the sign of ω. Therefore with given sign of ω the mappings are strict monotonic and the their maxima and minima are found at the\nborders. The minimum of µ̃ is obtained at µω = −0.01 and its maximum at µω = 0.01 and σ and τ at their minimal and maximal values, respectively. It follows that:\n−0.03106 < µ̃(−0.1, 0.1, 0.8, 0.95, λ01, α01) 6µ̃ 6 µ̃(0.1, 0.1, 1.5, 1.1, λ01, α01) < 0.06773, (96)\nand that µ̃ ∈ [−0.1, 0.1].\nSimilarly, the maximum and minimum of ξ̃( is obtained at the values mentioned above:\n0.80467 < ξ̃(−0.1, 0.1, 0.8, 0.95, λ01, α01) 6ξ̃ 6 ξ̃(0.1, 0.1, 1.5, 1.1, λ01, α01) < 1.48617. (97)\nSince |ξ̃ − ν̃| = |µ̃2| < 0.004597, we can conclude that 0.80009 < ν̃ < 1.48617 and the variance remains in [0.8, 1.5].\nCorollary 14. The image g(Ω′) of the mapping g : (µ, ν) 7→ (µ̃, ν̃) (Eq. (8)) and the domain Ω′ = {(µ, ν)| − 0.1 6 µ 6 0.1, 0.8 6 µ 6 1.5} is a subset of Ω′:\ng(Ω′) ⊆ Ω′, (98)\nfor all ω ∈ [−0.1, 0.1] and τ ∈ [0.95, 1.1].\nProof. Directly follows from Lemma 13.\nA3.4.3 Lemmata for proofing Theorem 2: The variance is contracting\nMain Sub-Function. We consider the main sub-function of the derivate of second moment, J22 (Eq. (54)):\n∂\n∂ν ξ̃ =\n1 2 λ2τ\n( −α2eµω+ ντ2 erfc ( µω + ντ√\n2 √ ντ\n) + 2α2e2µω+2ντ erfc ( µω + 2ντ√\n2 √ ντ\n) − erfc ( µω√ 2 √ ντ ) + 2 ) (99)\nthat depends on µω and ντ , therefore we set x = ντ and y = µω. Algebraic reformulations provide the formula in the following form:\n∂\n∂ν ξ̃ =\n1 2 λ2τ\n( α2 ( −e− y2 2x )( e (x+y)2 2x erfc ( y + x√\n2 √ x\n) − 2e (2x+y)2 2x erfc ( y + 2x√\n2 √ x\n)) − erfc ( y√ 2 √ x ) + 2 ) (100)\nFor λ = λ01 and α = α01, we consider the domain −1 6 µ 6 1, −0.1 6 ω 6 0.1, 1.5 6 ν 6 16, and, 0.8 6 τ 6 1.25.\nFor x and y we obtain: 0.8 · 1.5 = 1.2 6 x 6 20 = 1.25 · 16 and 0.1 · (−1) = −0.1 6 y 6 0.1 = 0.1 · 1. In the following we assume to remain within this domain. Lemma 15 (Main subfunction). For 1.2 6 x 6 20 and −0.1 6 y 6 0.1, the function\ne (x+y)2 2x erfc ( x+ y√\n2 √ x\n) − 2e (2x+y)2 2x erfc ( 2x+ y√\n2 √ x\n) (101)\nis smaller than zero, is strictly monotonically increasing in x, and strictly monotonically decreasing in y for the minimal x = 12/10 = 1.2.\nProof. See proof 44.\nThe graph of the subfunction in the specified domain is displayed in Figure A3.\nX y\nf(x,y)\n−0.132\n−0.130\n−0.128\n−0.126\n−0.10 −0.05 0.00 0.05 0.10\nf( 1\n.2 ,y\n)\ny\nFigure A3: Left panel: Graphs of the main subfunction f(x, y) = e (x+y)2 2x erfc ( x+y√ 2 √ x ) − 2e (2x+y)2 2x erfc (\n2x+y√ 2 √ x\n) treated in Lemma 15. The function is negative and monotonically increasing\nwith x independent of y. Right panel: Graphs of the main subfunction at minimal x = 1.2. The graph shows that the function f(1.2, y) is strictly monotonically decreasing in y.\nTheorem 16 (Contraction ν-mapping). The mapping of the variance ν̃(µ, ω, ν, τ, λ, α) given in Eq. (5) is contracting for λ = λ01, α = α01 and the domain Ω+: −0.1 6 µ 6 0.1, −0.1 6 ω 6 0.1, 1.5 6 ν 6 16, and 0.8 6 τ 6 1.25, that is,∣∣∣∣ ∂∂ν ν̃(µ, ω, ν, τ, λ01, α01)\n∣∣∣∣ < 1 . (102) Proof. In this domain Ω+ we have the following three properties (see further below): ∂∂ν ξ̃ < 1, µ̃ > 0, and ∂∂ν µ̃ > 0. Therefore, we have\n∣∣∣∣ ∂∂ν ν̃ ∣∣∣∣ = ∣∣∣∣ ∂∂ν ξ̃ − 2µ̃ ∂∂ν µ̃ ∣∣∣∣ < ∣∣∣∣ ∂∂ν ξ̃ ∣∣∣∣ < 1 (103)\n• We first proof that ∂∂ν ξ̃ < 1 in an even larger domain that fully contains Ω +. According to\nEq. (54), the derivative of the mapping Eq. (5) with respect to the variance ν is\n∂\n∂ν ξ̃(µ, ω, ν, τ, λ01, α01) = (104)\n1 2 λ2τ\n( α2 ( −eµω+ ντ2 ) erfc ( µω + ντ√\n2 √ ντ\n) +\n2α2e2µω+2ντ erfc ( µω + 2ντ√\n2 √ ντ\n) − erfc ( µω√ 2 √ ντ ) + 2 ) .\nFor λ = λ01, α = α01, −1 6 µ 6 1, −0.1 6 ω 6 0.1 1.5 6 ν 6 16, and 0.8 6 τ 6 1.25, we first show that the derivative is positive and then upper bound it.\nAccording to Lemma 15, the expression\ne (µω+ντ)2 2ντ erfc ( µω + ντ√\n2 √ ντ\n) − 2e (µω+2ντ)2 2ντ erfc ( µω + 2ντ√\n2 √ ντ\n) (105)\nis negative. This expression multiplied by positive factors is subtracted in the derivative Eq. (104), therefore, the whole term is positive. The remaining term\n2− erfc (\nµω√ 2 √ ντ\n) (106)\nof the derivative Eq. (104) is also positive according to Lemma 21. All factors outside the brackets in Eq. (104) are positive. Hence, the derivative Eq. (104) is positive.\nThe upper bound of the derivative is:\n1 2 λ201τ\n( α201 ( −eµω+ ντ2 ) erfc ( µω + ντ√\n2 √ ντ\n) + (107)\n2α201e 2µω+2ντ erfc\n( µω + 2ντ√\n2 √ ντ\n) − erfc ( µω√ 2 √ ντ ) + 2 ) =\n1 2 λ201τ\n( α201 ( −e− µ2ω2 2ντ )( e (µω+ντ)2 2ντ erfc ( µω + ντ√\n2 √ ντ\n) −\n2e (µω+2ντ)2 2ντ erfc ( µω + 2ντ√\n2 √ ντ\n)) − erfc ( µω√ 2 √ ντ ) + 2 ) 6\n1 2 1.25λ201\n( α201 ( −e− µ2ω2 2ντ )( e (µω+ντ)2 2ντ erfc ( µω + ντ√\n2 √ ντ\n) −\n2e (µω+2ντ)2 2ντ erfc ( µω + 2ντ√\n2 √ ντ\n)) − erfc ( µω√ 2 √ ντ ) + 2 ) 6\n1 2 1.25λ201\n( α201 ( e ( 1.2+0.1√ 2 √ 1.2 )2 erfc ( 1.2 + 0.1√\n2 √ 1.2\n) −\n2e\n( 2·1.2+0.1√\n2 √ 1.2 )2 erfc ( 2 · 1.2 + 0.1√\n2 √ 1.2\n))( −e− µ2ω2 2ντ ) − erfc ( µω√ 2 √ ντ ) + 2 ) 6\n1 2 1.25λ201\n( −e0.0α201 ( e ( 1.2+0.1√ 2 √ 1.2 )2 erfc ( 1.2 + 0.1√\n2 √ 1.2\n) −\n2e\n( 2·1.2+0.1√\n2 √ 1.2 )2 erfc ( 2 · 1.2 + 0.1√\n2 √ 1.2\n)) − erfc ( µω√ 2 √ ντ ) + 2 ) 6\n1 2 1.25λ201\n( −e0.0α201 ( e ( 1.2+0.1√ 2 √ 1.2 )2 erfc ( 1.2 + 0.1√\n2 √ 1.2\n) −\n2e\n( 2·1.2+0.1√\n2 √ 1.2 )2 erfc ( 2 · 1.2 + 0.1√\n2 √ 1.2\n)) − erfc ( 0.1√ 2 √ 1.2 ) + 2 ) 6\n0.995063 < 1 .\nWe explain the chain of inequalities:\n– First equality brings the expression into a shape where we can apply Lemma 15 for the the function Eq. (101).\n– First inequality: The overall factor τ is bounded by 1.25. – Second inequality: We apply Lemma 15. According to Lemma 15 the function\nEq. (101) is negative. The largest contribution is to subtract the most negative value of the function Eq. (101), that is, the minimum of function Eq. (101). According to Lemma 15 the function Eq. (101) is strictly monotonically increasing in x and strictly monotonically decreasing in y for x = 1.2. Therefore the function Eq. (101) has its minimum at minimal x = ντ = 1.5 ·0.8 = 1.2 and maximal y = µω = 1.0 ·0.1 = 0.1. We insert these values into the expression.\n– Third inequality: We use for the whole expression the maximal factor e− µ2ω2\n2ντ < 1 by setting this factor to 1.\n– Fourth inequality: erfc is strictly monotonically decreasing. Therefore we maximize its argument to obtain the least value which is subtracted. We use the minimal x = ντ = 1.5 · 0.8 = 1.2 and the maximal y = µω = 1.0 · 0.1 = 0.1.\n– Sixth inequality: evaluation of the terms.\n• We now show that µ̃ > 0. The expression µ̃(µ, ω, ν, τ) (Eq. (4)) is strictly monotonically increasing im µω and ντ . Therefore, the minimal value in Ω+ is obtained at µ̃(0.01, 0.01, 1.5, 0.8) = 0.008293 > 0.\n• Last we show that ∂∂ν µ̃ > 0. The expression ∂ ∂ν µ̃(µ, ω, ν, τ) = J12(µ, ω, ν, τ) (Eq. (54))\ncan we reformulated as follows:\nJ12(µ, ω, ν, τ, λ, α) = λτe−\nµ2ω2\n2ντ (√ παe (µω+ντ)2 2ντ erfc ( µω+ντ√\n2 √ ντ\n) − √\n2(α−1)√ ντ ) 4 √ π (108)\nis larger than zero when the term √ παe (µω+ντ)2 2ντ erfc ( µω+ντ√\n2 √ ντ\n) − √\n2(α−1)√ ντ\nis larger than zero. This term obtains its minimal value at µω = 0.01 and ντ = 16 · 1.25, which can easily be shown using the Abramowitz bounds (Lemma 22) and evaluates to 0.16, therefore J12 > 0 in Ω+.\nA3.4.4 Lemmata for proofing Theorem 3: The variance is expanding\nMain Sub-Function From Below. We consider functions in µω and ντ , therefore we set x = µω and y = ντ .\nFor λ = λ01 and α = α01, we consider the domain −0.1 6 µ 6 0.1, −0.1 6 ω 6 0.1 0.00875 6 ν 6 0.7, and 0.8 6 τ 6 1.25.\nFor x and y we obtain: 0.8 ·0.00875 = 0.007 6 x 6 0.875 = 1.25 ·0.7 and 0.1 · (−0.1) = −0.01 6 y 6 0.01 = 0.1 · 0.1. In the following we assume to be within this domain. In this domain, we consider the main sub-function of the derivate of second moment in the next layer, J22 (Eq. (54)):\n∂\n∂ν ξ̃ =\n1 2 λ2τ\n( −α2eµω+ ντ2 erfc ( µω + ντ√\n2 √ ντ\n) + 2α2e2µω+2ντ erfc ( µω + 2ντ√\n2 √ ντ\n) − erfc ( µω√ 2 √ ντ ) + 2 ) (109)\nthat depends on µω and ντ , therefore we set x = ντ and y = µω. Algebraic reformulations provide the formula in the following form:\n∂\n∂ν ξ̃ = (110)\n1 2 λ2τ\n( α2 ( −e− y2 2x )( e (x+y)2 2x erfc ( y + x√\n2 √ x\n) − 2e (2x+y)2 2x erfc ( y + 2x√\n2 √ x\n)) − erfc ( y√ 2 √ x ) + 2 ) Lemma 17 (Main subfunction Below). For 0.007 6 x 6 0.875 and−0.01 6 y 6 0.01, the function\ne (x+y)2 2x erfc ( x+ y√\n2 √ x\n) − 2e (2x+y)2 2x erfc ( 2x+ y√\n2 √ x\n) (111)\nsmaller than zero, is strictly monotonically increasing in x and strictly monotonically increasing in y for the minimal x = 0.007 = 0.00875 · 0.8, x = 0.56 = 0.7 · 0.8, x = 0.128 = 0.16 · 0.8, and x = 0.216 = 0.24 · 0.9 (lower bound of 0.9 on τ ).\nProof. See proof 45.\nLemma 18 (Monotone Derivative). For λ = λ01, α = α01 and the domain −0.1 6 µ 6 0.1, −0.1 6 ω 6 0.1, 0.00875 6 ν 6 0.7, and 0.8 6 τ 6 1.25. We are interested of the derivative of\nτ ( e ( µω+ντ√ 2 √ ντ )2 erfc ( µω + ντ√\n2 √ ντ\n) − 2e ( µω+2·ντ√ 2 √ ντ )2 erfc ( µω + 2ντ√\n2 √ ντ\n)) . (112)\nThe derivative of the equation above with respect to\n• ν is larger than zero;\n• τ is smaller than zero for maximal ν = 0.7, ν = 0.16, and ν = 0.24 (with 0.9 6 τ );\n• y = µω is larger than zero for ντ = 0.008750.8 = 0.007, ντ = 0.70.8 = 0.56, ντ = 0.160.8 = 0.128, and ντ = 0.24 · 0.9 = 0.216.\nProof. See proof 46.\nA3.4.5 Computer-assisted proof details for main Lemma 12 in Section A3.4.1.\nError Analysis. We investigate the error propagation for the singular value (Eq. (61)) if the function arguments µ, ω, ν, τ suffer from numerical imprecisions up to . To this end, we first derive error propagation rules based on the mean value theorem and then we apply these rules to the formula for the singular value. Lemma 19 (Mean value theorem). For a real-valued function f which is differentiable in the closed interval [a, b], there exists t ∈ [0, 1] with\nf(a) − f(b) = ∇f(a + t(b− a)) · (a − b) . (113)\nIt follows that for computation with error ∆x, there exists a t ∈ [0, 1] with\n|f(x + ∆x) − f(x)| 6 ‖∇f(x + t∆x)‖ ‖∆x‖ . (114)\nTherefore the increase of the norm of the error after applying function f is bounded by the norm of the gradient ‖∇f(x + t∆x)‖. We now compute for the functions, that we consider their gradient and its 2-norm:\n• addition: f(x) = x1 + x2 and ∇f(x) = (1, 1), which gives ‖∇f(x)‖ = √ 2.\nWe further know that\n|f(x + ∆x)− f(x)| = |x1 + x2 + ∆x1 + ∆x2 − x1 − x2| 6 |∆x1|+ |∆x2| . (115)\nAdding n terms gives:∣∣∣∣∣ n∑ i=1 xi + ∆xi − n∑ i=1 xi ∣∣∣∣∣ 6 n∑ i=1 |∆xi| 6 n |∆xi|max . (116)\n• subtraction: f(x) = x1 − x2 and ∇f(x) = (1,−1), which gives ‖∇f(x)‖ = √ 2.\nWe further know that\n|f(x + ∆x)− f(x)| = |x1 − x2 + ∆x1 −∆x2 − x1 + x2| 6 |∆x1|+ |∆x2| . (117)\nSubtracting n terms gives:∣∣∣∣∣ n∑ i=1 −(xi + ∆xi) + n∑ i=1 xi ∣∣∣∣∣ 6 n∑ i=1 |∆xi| 6 n |∆xi|max . (118)\n• multiplication: f(x) = x1x2 and∇f(x) = (x2, x1), which gives ‖∇f(x)‖ = ‖x‖. We further know that\n|f(x + ∆x)− f(x)| = |x1 · x2 + ∆x1 · x2 + ∆x2 · x1 + ∆x1 ·∆xs − x1 · x2| 6 (119)\n|∆x1| |x2|+ |∆x2| |x1|+O(∆2) .\nMultiplying n terms gives:∣∣∣∣∣ n∏ i=1 (xi + ∆xi) − n∏ i=1 xi ∣∣∣∣∣ = ∣∣∣∣∣ n∏ i=1 xi n∑ i=1 ∆xi xi + O(∆2) ∣∣∣∣∣ 6 (120) n∏ i=1 |xi| n∑ i=1 ∣∣∣∣∆xixi ∣∣∣∣ + O(∆2) 6 n n∏ i=1 |xi| ∣∣∣∣∆xixi ∣∣∣∣ max + O(∆2) .\n• division: f(x) = x1x2 and ∇f(x) = ( 1 x2 ,−x1\nx22\n) , which gives ‖∇f(x)‖ = ‖x‖\nx22 .\nWe further know that |f(x + ∆x)− f(x)| = ∣∣∣∣x1 + ∆x1x2 + ∆x2 − x1x2 ∣∣∣∣ = ∣∣∣∣ (x1 + ∆x1)x2 − x1(x2 + ∆x2)(x2 + ∆x2)x2 ∣∣∣∣ = (121)∣∣∣∣∆x1 · x2 −∆x2 · x1x22 + ∆x2 · x2 ∣∣∣∣ = ∣∣∣∣∆x1x2 − ∆x2 · x1x22 ∣∣∣∣+O(∆2) .\n• square root: f(x) = √ x and f ′(x) = 1\n2 √ x , which gives |f ′(x)| = 1 2 √ x .\n• exponential function: f(x) = exp(x) and f ′(x) = exp(x), which gives |f ′(x)| = exp(x).\n• error function: f(x) = erf(x) and f ′(x) = 2√\nπ exp(−x2), which gives |f ′(x)| = 2√ π exp(−x2).\n• complementary error function: f(x) = erfc(x) and f ′(x) = − 2√\nπ exp(−x2), which gives |f ′(x)| = 2√ π exp(−x2).\nLemma 20. If the values µ, ω, ν, τ have a precision of , the singular value (Eq. (61)) evaluated with the formulas given in Eq. (54) and Eq. (61) has a precision better than 292 .\nThis means for a machine with a typical precision of 2−52 = 2.220446 · 10−16, we have the rounding error ≈ 10−16, the evaluation of the singular value (Eq. (61)) with the formulas given in Eq. (54) and Eq. (61) has a precision better than 10−13 > 292 .\nProof. We have the numerical precision of the parameters µ, ω, ν, τ , that we denote by ∆µ,∆ω,∆ν,∆τ together with our domain Ω.\nWith the error propagation rules that we derived in Subsection A3.4.5, we can obtain bounds for the numerical errors on the following simple expressions:\n∆ (µω) 6 ∆µ |ω|+ ∆ω |µ| 6 0.2 (122) ∆ (ντ) 6 ∆ν |τ |+ ∆τ |ν| 6 1.5 + 1.5 = 3\n∆ (ντ\n2\n) 6 (∆(ντ)2 + ∆2 |ντ |) 1\n22 6 (6 + 1.25 · 1.5 )/4 < 2\n∆ (µω + ντ) 6 ∆ (µω) + ∆ (ντ) = 3.2 ∆ ( µω + ντ\n2\n) 6 ∆ (µω) + ∆ (ντ 2 ) < 2.2\n∆ (√ ντ ) 6 ∆ (ντ)\n2 √ ντ\n6 3\n2 √ 0.64 = 1.875\n∆ (√ 2 ) 6 ∆2\n2 √ 2 6\n1 2 √ 2\n∆ (√ 2 √ ντ ) 6 √ 2∆ (√ ντ ) + ντ∆ (√ 2 ) 6 √\n2 · 1.875 + 1.5 · 1.25 · 1 2 √ 2 < 3.5\n∆ ( µω√ 2 √ ντ ) 6 ( ∆ (µω) √ 2 √ ντ + |µω|∆ (√ 2 √ ντ )) 1(√ 2 √ ντ )2 6\n( 0.2 √ 2 √ 0.64 + 0.01 · 3.5 ) 1\n2 · 0.64 < 0.25\n∆ ( µω + ντ√\n2 √ ντ\n) 6 ( ∆ (µω + ντ) √ 2 √ ντ + |µω + ντ |∆ (√ 2 √ ντ )) 1(√\n2 √ ντ )2 6(\n3.2 √ 2 √ 0.64 + 1.885 · 3.5 ) 1\n2 · 0.64 < 8 .\nUsing these bounds on the simple expressions, we can now calculate bounds on the numerical errors of compound expressions:\n∆ ( erfc ( µω√ 2 √ ντ )) 6 2√ π e − ( µω√ 2 √ ντ )2 ∆ ( µω√ 2 √ ντ ) < (123)\n2√ π 0.25 < 0.3\n∆ ( erfc ( µω + ντ√\n2 √ ντ\n)) 6 2√ π e − ( µω+ντ√ 2 √ ντ )2 ∆ ( µω + ντ√ 2 √ ντ ) < (124)\n2√ π 8 < 10\n∆ ( eµω+ ντ 2 ) 6 ( eµω+ ντ 2 ) ∆ ( eµω+ ντ 2 ) < (125)\ne0.94752.2 < 5.7 (126)\nSubsequently, we can use the above results to get bounds for the numerical errors on the Jacobian entries (Eq. (54)), applying the rules from Subsection A3.4.5 again:\n∆ (J11) = ∆ ( 1\n2 λω\n( αeµω+ ντ 2 erfc ( µω + ντ√\n2 √ ντ\n) − erfc ( µω√ 2 √ ντ ) + 2 )) < 6 , (127)\nand we obtain ∆ (J12) < 78 , ∆ (J21) < 189 , ∆ (J22) < 405 and ∆ (µ̃) < 52 . We also have bounds on the absolute values on Jij and µ̃ (see Lemma 6, Lemma 7, and Lemma 9), therefore we can propagate the error also through the function that calculates the singular value (Eq. (61)).\n∆ (S(µ, ω, ν, τ, λ, α)) = (128)\n∆\n( 1\n2 (√ (J11 + J22 − 2µ̃J12)2 + (J21 − 2µ̃J11 − J12)2 +√\n(J11 − J22 + 2µ̃J12)2 + (J12 + J21 − 2µ̃J11)2 )) < 292 .\nPrecision of Implementations. We will show that our computations are correct up to 3 ulps. For our implementation in GNU C library and the hardware architectures that we used, the precision of all mathematical functions that we used is at least one ulp. The term “ulp” (acronym for “unit in the last place”) was coined by W. Kahan in 1960. It is the highest precision (up to some factor smaller 1), which can be achieved for the given hardware and floating point representation.\nKahan defined ulp as [21]:\n“Ulp(x) is the gap between the two finite floating-point numbers nearest x, even if x is one of them. (But ulp(NaN) is NaN.)”\nHarrison defined ulp as [15]:\n“an ulp in x is the distance between the two closest straddling floating point numbers a and b, i.e. those with a 6 x 6 b and a 6= b assuming an unbounded exponent range.”\nIn the literature we find also slightly different definitions [29].\nAccording to [29] who refers to [11]:\n“IEEE-754 mandates four standard rounding modes:” “Round-to-nearest: r(x) is the floating-point value closest to x with the usual distance; if two floating-point value are equally close to x, then r(x) is the one whose least significant bit is equal to zero.” “IEEE-754 standardises 5 operations: addition (which we shall note ⊕ in order to distinguish it from the operation over the reals), subtraction ( ), multiplication (⊗), division ( ), and also square root.” “IEEE-754 specifies em exact rounding [Goldberg, 1991, §1.5]: the result of a floating-point operation is the same as if the operation were performed on the real numbers with the given inputs, then rounded according to the rules in the preceding section. Thus, x ⊕ y is defined as r(x + y), with x and y taken as elements of R ∪ {−∞,+∞}; the same applies for the other operators.”\nConsequently, the IEEE-754 standard guarantees that addition, subtraction, multiplication, division, and squared root is precise up to one ulp.\nWe have to consider transcendental functions. First the is the exponential function, and then the complementary error function erfc(x), which can be computed via the error function erf(x).\nIntel states [29]:\n“With the Intel486 processor and Intel 387 math coprocessor, the worst- case, transcendental function error is typically 3 or 3.5 ulps, but is some- times as large as 4.5 ulps.”\nAccording to https://www.mirbsd.org/htman/i386/man3/exp.htm and http: //man.openbsd.org/OpenBSD-current/man3/exp.3:\n“exp(x), log(x), expm1(x) and log1p(x) are accurate to within an ulp”\nwhich is the same for freebsd https://www.freebsd.org/cgi/man.cgi?query=exp&sektion= 3&apropos=0&manpath=freebsd:\n“The values of exp(0), expm1(0), exp2(integer), and pow(integer, integer) are exact provided that they are representable. Otherwise the error in these functions is generally below one ulp.”\nThe same holds for “FDLIBM” http://www.netlib.org/fdlibm/readme:\n“FDLIBM is intended to provide a reasonably portable (see assumptions below), reference quality (below one ulp for major functions like sin,cos,exp,log) math library (libm.a).”\nIn http://www.gnu.org/software/libc/manual/html_node/ Errors-in-Math-Functions.html we find that both exp and erf have an error of 1 ulp while erfc has an error up to 3 ulps depending on the architecture. For the most common architectures as used by us, however, the error of erfc is 1 ulp.\nWe implemented the function in the programming language C. We rely on the GNU C Library [26]. According to the GNU C Library manual which can be obtained from http://www.gnu.org/ software/libc/manual/pdf/libc.pdf, the errors of the math functions exp, erf , and erfc are not larger than 3 ulps for all architectures [26, pp. 528]. For the architectures ix86, i386/i686/fpu, and m68k/fpmu68k/m680x0/fpu that we used the error are at least one ulp [26, pp. 528].\n2*exp(−x^2)/(sqrt(pi)*(sqrt(x^2+4/pi)+x))\n2*exp(−x^2)/(sqrt(pi)*(sqrt(x^2+2)+x))\nFunction\nerfc(x)\n0.00\n0.25\n0.50\n0.75\n1.00\n0.0 0.5 1.0 1.5 2.0\nx\ny\nFigure A4: Graphs of the upper and lower bounds on erfc. The lower bound 2e −x2\n√ π( √ x2+2+x) (red), the\nupper bound 2e −x2 √ π (√ x2+ 4π+x ) (green) and the function erfc(x) (blue) as treated in Lemma 22.\nA3.4.6 Intermediate Lemmata and Proofs\nSince we focus on the fixed point (µ, ν) = (0, 1), we assume for our whole analysis that α = α01 and λ = λ01. Furthermore, we restrict the range of the variables µ ∈ [µmin, µmax] = [−0.1, 0.1], ω ∈ [ωmin, ωmax] = [−0.1, 0.1], ν ∈ [νmin, νmax] = [0.8, 1.5], and τ ∈ [τmin, τmax] = [0.8, 1.25]. For bounding different partial derivatives we need properties of different functions. We will bound a the absolute value of a function by computing an upper bound on its maximum and a lower bound on its minimum. These bounds are computed by upper or lower bounding terms. The bounds get tighter if we can combine terms to a more complex function and bound this function. The following lemmata give some properties of functions that we will use in bounding complex functions.\nThroughout this work, we use the error function erf(x) := 1√ π ∫ x −x e\n−t2 and the complementary error function erfc(x) = 1− erf(x). Lemma 21 (Basic functions). exp(x) is strictly monotonically increasing from 0 at −∞ to∞ at∞ and has positive curvature.\nAccording to its definition erfc(x) is strictly monotonically decreasing from 2 at −∞ to 0 at∞.\nNext we introduce a bound on erfc:\nLemma 22 (Erfc bound from Abramowitz).\n2e−x 2 √ π (√ x2 + 2 + x ) < erfc(x) 6 2e−x2√ π (√ x2 + 4π + x ) , (129)\nfor x > 0.\nProof. The statement follows immediately from [1] (page 298, formula 7.1.13).\nThese bounds are displayed in figure A4.\nLemma 23 (Function ex 2 erfc(x)). ex 2\nerfc(x) is strictly monotonically decreasing for x > 0 and has positive curvature (positive 2nd order derivative), that is, the decreasing slowes down.\nA graph of the function is displayed in Figure A5.\n0 1\n2\n3\n4\n5\n−1 0 1 2 3\ne x p (x\n^2 )*\ne rf\nc (x\n)\nx\n−5\n−4\n−3\n−2\n−1\n0\n−1 0 1 2 3\nx *e\nx p\n(x ^2\n)* e rf\nc (x\n)\nx\nFigure A5: Graphs of the functions ex 2 erfc(x) (left) and xex 2\nerfc(x) (right) treated in Lemma 23 and Lemma 24, respectively.\nProof. The derivative of ex 2 erfc(x) is\n∂ex 2 erfc(x)\n∂x = 2ex\n2 x erfc(x)− 2√ π . (130)\nUsing Lemma 22, we get\n∂ex 2 erfc(x)\n∂x = 2ex\n2 x erfc(x)− 2√ π < 4x √ π (√ x2 + 4π + x ) − 2√ π =\n2\n( 2√\n4 πx2\n+1+1 − 1 ) √ π < 0\n(131)\nThus ex 2 erfc(x) is strictly monotonically decreasing for x > 0.\nThe second order derivative of ex 2 erfc(x) is\n∂2ex 2 erfc(x)\n∂x2 = 4ex\n2 x2 erfc(x) + 2ex 2 erfc(x)− 4x√ π . (132)\nAgain using Lemma 22 (first inequality), we get\n2 (( 2x2 + 1 ) ex 2\nerfc(x)− 2x√ π\n) > (133)\n4 ( 2x2 + 1 ) √ π (√ x2 + 2 + x ) − 4x√ π =\n4 ( x2 − √ x2 + 2x+ 1 ) √ π (√ x2 + 2 + x\n) = 4 ( x2 − √ x4 + 2x2 + 1\n) √ π (√ x2 + 2 + x\n) > 4 ( x2 − √ x4 + 2x2 + 1 + 1\n) √ π (√ x2 + 2 + x\n) = 0 For the last inequality we added 1 in the numerator in the square root which is subtracted, that is, making a larger negative term in the numerator.\nLemma 24 (Properties of xex 2 erfc(x)). The function xex 2\nerfc(x) has the sign of x and is monotonically increasing to 1√\nπ .\nProof. The derivative of xex 2 erfc(x) is\n2ex 2 x2 erfc(x) + ex 2 erfc(x)− 2x√ π . (134)\nThis derivative is positive since\n2ex 2 x2 erfc(x) + ex 2 erfc(x)− 2x√ π = (135) ex 2 ( 2x2 + 1 )\nerfc(x)− 2x√ π >\n2 ( 2x2 + 1 ) √ π (√ x2 + 2 + x ) − 2x√ π = 2 (( 2x2 + 1 ) − x (√ x2 + 2 + x )) √ π (√ x2 + 2 + x\n) = 2 ( x2 − x √ x2 + 2 + 1\n) √ π (√ x2 + 2 + x ) = 2 (x2 − x√x2 + 2 + 1)√ π (√ x2 + 2 + x ) > 2 ( x2 − x √ x2 + 1x2 + 2 + 1 ) √ π (√ x2 + 2 + x\n) = 2 ( x2 − √ x4 + 2x2 + 1 + 1\n) √ π (√ x2 + 2 + x ) = 2 ( x2 − √ (x2 + 1) 2 + 1 ) √ π (√ x2 + 2 + x\n) = 0 . We apply Lemma 22 to x erfc(x)ex 2 and divide the terms of the lemma by x, which gives\n2 √ π (√\n2 x2 + 1 + 1\n) < x erfc(x)ex2 6 2√ π (√\n4 πx2 + 1 + 1 ) . (136) For limx→∞ both the upper and the lower bound go to 1√π .\nLemma 25 (Function µω). h11(µ, ω) = µω is monotonically increasing in µω. It has minimal value t11 = −0.01 and maximal value T11 = 0.01.\nProof. Obvious.\nLemma 26 (Function ντ ). h22(ν, τ) = ντ is monotonically increasing in ντ and is positive. It has minimal value t22 = 0.64 and maximal value T22 = 1.875.\nProof. Obvious.\nLemma 27 (Function µω+ντ√ 2 √ ντ ). h1(µ, ω, ν, τ) = µω+ντ√2√ντ is larger than zero and increasing in both ντ and µω. It has minimal value t1 = 0.5568 and maximal value T1 = 0.9734.\nProof. The derivative of the function µω+x√ 2 √ x with respect to x is\n1√ 2 √ x − µω + x 2 √ 2x3/2 = 2x− (µω + x) 2 √ 2x3/2 = x− µω 2 √ 2x3/2 > 0 , (137)\nsince x > 0.8 · 0.8 and µω < 0.1 · 0.1.\nLemma 28 (Function µω+2ντ√ 2 √ ντ ). h2(µ, ω, ν, τ) = µω+2ντ√2√ντ is larger than zero and increasing in both ντ and µω. It has minimal value t2 = 1.1225 and maximal value T2 = 1.9417.\nProof. The derivative of the function µω+2x√ 2 √ x with respect to x is √\n2√ x − µω + 2x 2 √ 2x3/2 = 4x− (µω + 2x) 2 √ 2x3/2 = 2x− µω 2 √ 2x3/2 > 0 . (138)\nLemma 29 (Function µω√ 2 √ ντ ). h3(µ, ω, ν, τ) = µω√2√ντ monotonically decreasing in ντ and monotonically increasing in µω. It has minimal value t3 = −0.0088388 and maximal value T3 = 0.0088388.\nProof. Obvious. Lemma 30 (Function (\nµω√ 2 √ ντ\n)2 ). h4(µ, ω, ν, τ) = ( µω√ 2 √ ντ )2 has a minimum at 0 for µ = 0 or\nω = 0 and has a maximum for the smallest ντ and largest |µω| and is larger or equal to zero. It has minimal value t4 = 0 and maximal value T4 = 0.000078126.\nProof. Obvious. Lemma 31 (Function √\n2 π (α−1)√ ντ\n). √\n2 π (α−1)√ ντ > 0 and decreasing in ντ .\nProof. Statements follow directly from elementary functions square root and division. Lemma 32 (Function 2 − erfc (\nµω√ 2 √ ντ\n) ). 2 − erfc ( µω√ 2 √ ντ ) > 0 and decreasing in ντ and\nincreasing in µω.\nProof. Statements follow directly from Lemma 21 and erfc. Lemma 33 (Function √\n2 π ( (α−1)µω (ντ)3/2 − α√ ντ ) ). For λ = λ01 and α = α01,√\n2 π ( (α−1)µω (ντ)3/2 − α√ ντ ) < 0 and increasing in both ντ and µω.\nProof. We consider the function √\n2 π ( (α−1)µω x3/2 − α√ x ) , which has the derivative with respect to x:√\n2\nπ\n( α\n2x3/2 − 3(α− 1)µω 2x5/2\n) . (139)\nThis derivative is larger than zero, since√ 2\nπ\n( α\n2(ντ)3/2 − 3(α− 1)µω 2(ντ)5/2\n) >\n√ 2 π ( α− 3(α−1)µωντ ) 2(ντ)3/2 > 0 . (140)\nThe last inequality follows from α− 3·0.1·0.1(α−1)0.8·0.8 > 0 for α = α01. We next consider the function √\n2 π ( (α−1)x (ντ)3/2 − α√ ντ ) , which has the derivative with respect to x:√\n2 π (α− 1) (ντ)3/2 > 0 . (141)\nLemma 34 (Function √\n2 π\n( (−1)(α−1)µ2ω2\n(ντ)3/2 + −α+αµω+1√ ντ − α √ ντ )\n). The function√ 2 π ( (−1)(α−1)µ2ω2 (ντ)3/2 + −α+αµω+1√ ντ − α √ ντ ) < 0 is decreasing in ντ and increasing in µω.\nProof. We define the function√ 2\nπ\n( (−1)(α− 1)µ2ω2\nx3/2 + −α+ αµω + 1√ x − α √ x\n) (142)\nwhich has as derivative with respect to x:√ 2\nπ\n( 3(α− 1)µ2ω2\n2x5/2 − −α+ αµω + 1 2x3/2 − α 2 √ x\n) = (143)\n1√ 2πx5/2\n( 3(α− 1)µ2ω2 − x(−α+ αµω + 1)− αx2 ) .\nThe derivative of the term 3(α − 1)µ2ω2 − x(−α + αµω + 1) − αx2 with respect to x is −1 + α− µωα− 2αx < 0, since 2αx > 1.6α. Therefore the term is maximized with the smallest value for x, which is x = ντ = 0.8 · 0.8. For µω we use for each term the value which gives maximal contribution. We obtain an upper bound for the term:\n3(−0.1 · 0.1)2(α01 − 1)− (0.8 · 0.8)2α01 − 0.8 · 0.8((−0.1 · 0.1)α01 − α01 + 1) = −0.243569 . (144)\nTherefore the derivative with respect to x = ντ is smaller than zero and the original function is decreasing in ντ\nWe now consider the derivative with respect to x = µω. The derivative with respect to x of the function √\n2\nπ\n( −α √ ντ − (α− 1)x 2\n(ντ)3/2 + −α+ αx+ 1√ ντ\n) (145)\nis √ 2 π (αντ − 2(α− 1)x)\n(ντ)3/2 . (146)\nSince −2x(−1 + α) + ντα > −2 · 0.01 · (−1 + α01) + 0.8 · 0.8α01 > 1.0574 > 0, the derivative is larger than zero. Consequently, the original function is increasing in µω.\nThe maximal value is obtained with the minimal ντ = 0.8 · 0.8 and the maximal µω = 0.1 · 0.1. The maximal value is√\n2\nπ\n( 0.1 · 0.1α01 − α01 + 1√\n0.8 · 0.8 + 0.120.12(−1)(α01 − 1) (0.8 · 0.8)3/2 − √\n0.8 · 0.8α01 ) = −1.72296 .\n(147)\nTherefore the original function is smaller than zero. Lemma 35 (Function √\n2 π\n( (α2−1)µω\n(ντ)3/2 − 3α 2 √ ντ ) ). For λ = λ01 and α = α01,√\n2 π\n( (α2−1)µω\n(ντ)3/2 − 3α 2 √ ντ\n) < 0 and increasing in both ντ and µω.\nProof. The derivative of the function√ 2\nπ\n(( α2 − 1 ) µω\nx3/2 − 3α\n2 √ x\n) (148)\nwith respect to x is√ 2\nπ\n( 3α2\n2x3/2 −\n3 ( α2 − 1 ) µω\n2x5/2\n) = 3 ( α2x− ( α2 − 1 ) µω )\n√ 2πx5/2\n> 0 , (149)\nsince α2x− µω(−1 + α2) > α2010.8 · 0.8− 0.1 · 0.1 · (−1 + α201) > 1.77387 The derivative of the function √\n2\nπ\n(( α2 − 1 ) x\n(ντ)3/2 − 3α\n2 √ ντ\n) (150)\nwith respect to x is √ 2 π ( α2 − 1 ) (ντ)3/2 > 0 . (151)\nThe maximal function value is obtained by maximal ντ = 1.5 · 1.25 and the maximal µω = 0.1 · 0.1. The maximal value is √\n2 π\n( 0.1·0.1(α201−1)\n(1.5·1.25)3/2 − 3α201√ 1.5·1.25\n) = −4.88869. Therefore the function is\nnegative.\nLemma 36 (Function √\n2 π\n( (α2−1)µω√\nντ − 3α2\n√ ντ ) ). The function√\n2 π\n( (α2−1)µω√\nντ − 3α2\n√ ντ ) < 0 is decreasing in ντ and increasing in µω.\nProof. The derivative of the function√ 2\nπ\n(( α2 − 1 ) µω\n√ x\n− 3α2 √ x ) (152)\nwith respect to x is√ 2\nπ\n( − ( α2 − 1 ) µω\n2x3/2 − 3α\n2\n2 √ x\n) = − ( α2 − 1 ) µω − 3α2x\n√ 2πx3/2\n< 0 , (153)\nsince −3α2x− µω(−1 + α2) < −3α2010.8 · 0.8 + 0.1 · 0.1(−1 + α201) < −5.35764. The derivative of the function √\n2\nπ\n(( α2 − 1 ) x\n√ ντ\n− 3α2 √ ντ ) (154)\nwith respect to x is √ 2 π ( α2 − 1 ) √ ντ > 0 . (155)\nThe maximal function value is obtained for minimal ντ = 0.8 · 0.8 and the maximal µω = 0.1 · 0.1. The value is √\n2 π\n( 0.1·0.1(α201−1)√\n0.8·0.8 − 3 √ 0.8 · 0.8α201 ) = −5.34347. Thus, the function is\nnegative.\nLemma 37 (Function ντe (µω+ντ)2 2ντ erfc ( µω+ντ√\n2 √ ντ\n) ). The function ντe (µω+ντ)2 2ντ erfc ( µω+ντ√\n2 √ ντ\n) > 0 is\nincreasing in ντ and decreasing in µω.\nProof. The derivative of the function\nxe (µω+x)2 2x erfc ( µω + x√\n2 √ x\n) (156)\nwith respect to x is\ne (µω+x)2 2x ( x(x+ 2)− µ2ω2 ) erfc ( µω+x√\n2 √ x ) 2x + µω − x√ 2π √ x . (157)\nThis derivative is larger than zero, since\ne (µω+ντ)2 2ντ ( ντ(ντ + 2)− µ2ω2 ) erfc ( µω+ντ√\n2 √ ντ ) 2ντ + µω − ντ√ 2π √ ντ > (158)\n0.4349 ( ντ(ντ + 2)− µ2ω2 ) 2ντ + µω − ντ√ 2π √ ντ >\n0.5 ( ντ(ντ + 2)− µ2ω2 ) √\n2πντ + µω − ντ√ 2π √ ντ =\n0.5 ( ντ(ντ + 2)− µ2ω2 ) + √ ντ(µω − ντ)\n√ 2πντ\n=\n−0.5µ2ω2 + µω √ ντ + 0.5(ντ)2 − ντ √ ντ + ντ√\n2πντ =\n−0.5µ2ω2 + µω √ ντ + (0.5ντ − √ ντ)\n2 + 0.25(ντ)2√\n2πντ > 0 .\nWe explain this chain of inequalities:\n• The first inequality follows by applying Lemma 23 which says that e (µω+ντ)2 2ντ erfc ( µω+ντ√\n2 √ ντ ) is strictly monotonically decreasing. The minimal value that is larger than 0.4349 is taken on at the maximal values ντ = 1.5 · 1.25 and µω = 0.1 · 0.1.\n• The second inequality uses 120.4349 √ 2π = 0.545066 > 0.5.\n• The equalities are just algebraic reformulations. • The last inequality follows from −0.5µ2ω2 + µω √ ντ + 0.25(ντ)2 > 0.25(0.8 · 0.8)2 −\n0.5 · (0.1)2(0.1)2 − 0.1 · 0.1 · √ 0.8 · 0.8 = 0.09435 > 0.\nTherefore the function is increasing in ντ .\nDecreasing in µω follows from decreasing of ex 2\nerfc(x) according to Lemma 23. Positivity follows form the fact that erfc and the exponential function are positive and that ντ > 0.\nLemma 38 (Function ντe (µω+2ντ)2 2ντ erfc ( µω+2ντ√\n2 √ ντ\n) ). The function ντe (µω+2ντ)2 2ντ erfc ( µω+2ντ√\n2 √ ντ\n) > 0\nis increasing in ντ and decreasing in µω.\nProof. The derivative of the function\nxe (µω+2x)2 2x erfc ( µω + 2x√\n2 √ 2x\n) (159)\nis\ne (µω+2x)2 4x (√ πe (µω+2x)2 4x ( 2x(2x+ 1)− µ2ω2 ) erfc ( µω+2x\n2 √ x\n) + √ x(µω − 2x) ) 2 √ πx . (160)\nWe only have to determine the sign of √ πe (µω+2x)2 4x ( 2x(2x+ 1)− µ2ω2 ) erfc ( µω+2x\n2 √ x\n) + √ x(µω−\n2x) since all other factors are obviously larger than zero.\nThis derivative is larger than zero, since\n√ πe (µω+2ντ)2 4ντ ( 2ντ(2ντ + 1)− µ2ω2 ) erfc\n( µω + 2ντ\n2 √ ντ\n) + √ ντ(µω − 2ντ) > (161)\n0.463979 ( 2ντ(2ντ + 1)− µ2ω2 ) + √ ντ(µω − 2ντ) =\n− 0.463979µ2ω2 + µω √ ντ + 1.85592(ντ)2 + 0.927958ντ − 2ντ √ ντ =\nµω (√ ντ − 0.463979µω ) + 0.85592(ντ)2 + ( ντ − √ ντ )2 − 0.0720421ντ > 0 .\nWe explain this chain of inequalities:\n• The first inequality follows by applying Lemma 23 which says that e (µω+2ντ)2 2ντ erfc ( µω+2ντ√\n2 √ ντ\n) is strictly monotonically decreasing. The minimal value\nthat is larger than 0.261772 is taken on at the maximal values ντ = 1.5 · 1.25 and µω = 0.1 · 0.1. 0.261772 √ π > 0.463979.\n• The equalities are just algebraic reformulations.\n• The last inequality follows from µω ( √ ντ − 0.463979µω) + 0.85592(ντ)2 − 0.0720421ντ > 0.85592 · (0.8 · 0.8)2 − 0.1 · 0.1 (√ 1.5 · 1.25 + 0.1 · 0.1 · 0.463979 ) −\n0.0720421 · 1.5 · 1.25 > 0.201766.\nTherefore the function is increasing in ντ .\nDecreasing in µω follows from decreasing of ex 2\nerfc(x) according to Lemma 23. Positivity follows from the fact that erfc and the exponential function are positive and that ντ > 0.\nLemma 39 (Bounds on the Derivatives). The following bounds on the absolute values of the derivatives of the Jacobian entries J11(µ, ω, ν, τ, λ, α), J12(µ, ω, ν, τ, λ, α), J21(µ, ω, ν, τ, λ, α), and J22(µ, ω, ν, τ, λ, α) with respect to µ, ω, ν, and τ hold:∣∣∣∣∂J11∂µ\n∣∣∣∣ < 0.0031049101995398316 (162)∣∣∣∣∂J11∂ω ∣∣∣∣ < 1.055872374194189∣∣∣∣∂J11∂ν ∣∣∣∣ < 0.031242911235461816∣∣∣∣∂J11∂τ ∣∣∣∣ < 0.03749149348255419\n∣∣∣∣∂J12∂µ ∣∣∣∣ < 0.031242911235461816∣∣∣∣∂J12∂ω ∣∣∣∣ < 0.031242911235461816∣∣∣∣∂J12∂ν ∣∣∣∣ < 0.21232788238624354∣∣∣∣∂J12∂τ ∣∣∣∣ < 0.2124377655377270\n∣∣∣∣∂J21∂µ ∣∣∣∣ < 0.02220441024325437∣∣∣∣∂J21∂ω ∣∣∣∣ < 1.146955401845684∣∣∣∣∂J21∂ν ∣∣∣∣ < 0.14983446469110305∣∣∣∣∂J21∂τ ∣∣∣∣ < 0.17980135762932363\n∣∣∣∣∂J22∂µ ∣∣∣∣ < 0.14983446469110305∣∣∣∣∂J22∂ω ∣∣∣∣ < 0.14983446469110305∣∣∣∣∂J22∂ν ∣∣∣∣ < 1.805740052651535∣∣∣∣∂J22∂τ ∣∣∣∣ < 2.396685907216327\nProof. For each derivative we compute a lower and an upper bound and take the maximum of the absolute value. A lower bound is determined by minimizing the single terms of the functions that represents the derivative. An upper bound is determined by maximizing the single terms of the functions that represent the derivative. Terms can be combined to larger terms for which the maximum and the minimum must be known. We apply many previous lemmata which state properties of functions representing single or combined terms. The more terms are combined, the tighter the bounds can be made.\nNext we go through all the derivatives, where we use Lemma 25, Lemma 26, Lemma 27, Lemma 28, Lemma 29, Lemma 30, Lemma 21, and Lemma 23 without citing. Furthermore, we use the bounds on the simple expressions t11,t22, ..., and T4 as defined the aforementioned lemmata:\n• ∂J11∂µ\nWe use Lemma 31 and consider the expression αe (µω+ντ)2 2ντ erfc ( µω+ντ√\n2 √ ντ\n) − √\n2 π (α−1)√ ντ\nin brackets. An upper bound on the maximum of is\nα01e t21 erfc(t1)− √ 2 π (α01 − 1)√\nT22 = 0.591017 . (163)\nA lower bound on the minimum is\nα01e T 21 erfc(T1)− √ 2 π (α01 − 1)√\nt22 = 0.056318 . (164)\nThus, an upper bound on the maximal absolute value is\n1 2 λ01ω 2 maxe t4\nα01et21 erfc(t1)− √ 2 π (α01 − 1)√\nT22\n = 0.0031049101995398316 . (165)\n• ∂J11∂ω\nWe use Lemma 31 and consider the expression √ 2 π (α−1)µω√\nντ − α(µω +\n1)e (µω+ντ)2 2ντ erfc ( µω+ντ√\n2 √ ντ\n) in brackets.\nAn upper bound on the maximum is√ 2 π (α01 − 1)T11√\nt22 − α01(t11 + 1)eT 2 1 erfc(T1) = −0.713808 . (166)\nA lower bound on the minimum is√ 2 π (α01 − 1)t11√\nt22 − α01(T11 + 1)et 2 1 erfc(t1) = −0.99987 . (167)\nThis term is subtracted, and 2− erfc(x) > 0, therefore we have to use the minimum and the maximum for the argument of erfc.\nThus, an upper bound on the maximal absolute value is\n1 2 λ01\n−et4  √ 2 π (α01 − 1)t11√\nt22 − α01(T11 + 1)et 2 1 erfc(t1)\n − erfc(T3) + 2  =\n(168) 1.055872374194189 .\n• ∂J11∂ν We consider the term in brackets\nαe (µω+ντ)2 2ντ erfc ( µω + ντ√\n2 √ ντ\n) + √ 2\nπ\n( (α− 1)µω\n(ντ)3/2 − α√ ντ\n) . (169)\nWe apply Lemma 33 for the first sub-term. An upper bound on the maximum is\nα01e t21 erfc(t1) +\n√ 2\nπ\n( (α01 − 1)T11\nT 3/2 22\n− α01√ T22\n) = 0.0104167 . (170)\nA lower bound on the minimum is\nα01e T 21 erfc(T1) +\n√ 2\nπ\n( (α01 − 1)t11\nt 3/2 22\n− α01√ t22\n) = −0.95153 . (171)\nThus, an upper bound on the maximal absolute value is\n− 1 4 λ01τmaxωmaxe t4\n( α01e T 21 erfc(T1) + √ 2\nπ\n( (α01 − 1)t11\nt 3/2 22\n− α01√ t22\n)) = (172)\n0.031242911235461816 .\n• ∂J11∂τ We use the results of item ∂J11∂ν were the brackets are only differently scaled. Thus, an upper bound on the maximal absolute value is\n− 1 4 λ01νmaxωmaxe t4\n( α01e T 21 erfc(T1) + √ 2\nπ\n( (α01 − 1)t11\nt 3/2 22\n− α01√ t22\n)) = (173)\n0.03749149348255419 .\n• ∂J12∂µ Since ∂J12∂µ = ∂J11 ∂ν , an upper bound on the maximal absolute value is\n− 1 4 λ01τmaxωmaxe t4\n( α01e T 21 erfc(T1) + √ 2\nπ\n( (α01 − 1)t11\nt 3/2 22\n− α01√ t22\n)) = (174)\n0.031242911235461816 .\n• ∂J12∂ω We use the results of item ∂J11∂ν were the brackets are only differently scaled. Thus, an upper bound on the maximal absolute value is\n− 1 4 λ01µmaxτmaxe t4\n( α01e T 21 erfc(T1) + √ 2\nπ\n( (α01 − 1)t11\nt 3/2 22\n− α01√ t22\n)) = (175)\n0.031242911235461816 .\n• ∂J12∂ν For the second term in brackets, we see that α01τ2mine\nT 21 erfc(T1) = 0.465793 and α01τ 2 maxe t21 erfc(t1) = 1.53644. We now check different values for√ 2\nπ\n( (−1)(α− 1)µ2ω2\nν5/2 √ τ\n+\n√ τ(α+ αµω − 1)\nν3/2 − ατ\n3/2\n√ ν\n) , (176)\nwhere we maximize or minimize all single terms.\nA lower bound on the minimum of this expression is√ 2\nπ\n( (−1)(α01 − 1)µ2maxω2max\nν 5/2 min √ τmin\n+\n√ τmin(α01 + α01t11 − 1)\nν 3/2 max\n− α01τ 3/2 max√\nνmin\n) = (177)\n− 1.83112 .\nAn upper bound on the maximum of this expression is√ 2\nπ\n( (−1)(α01 − 1)µ2minω2min\nν 5/2 max √ τmax\n+\n√ τmax(α01 + α01T11 − 1)\nν 3/2 min\n− α01τ 3/2 min√\nνmax\n) = (178)\n0.0802158 .\nAn upper bound on the maximum is\n1 8 λ01e t4\n(√ 2\nπ\n( (−1)(α01 − 1)µ2minω2min\nν 5/2 max √ τmax\n− α01τ 3/2 min√\nνmax + (179)\n√ τmax(α01 + α01T11 − 1)\nν 3/2 min\n) + α01τ 2 maxe t21 erfc(t1) ) = 0.212328 .\nA lower bound on the minimum is 1\n8 λ01e\nt4 ( α01τ 2 mine\nT 21 erfc(T1) + (180)√ 2\nπ\n( (−1)(α01 − 1)µ2maxω2max\nν 5/2 min √ τmin\n+\n√ τmin(α01 + α01t11 − 1)\nν 3/2 max\n− α01τ 3/2 max√\nνmin\n)) =\n− 0.179318 .\nThus, an upper bound on the maximal absolute value is\n1 8 λ01e t4\n(√ 2\nπ\n( (−1)(α01 − 1)µ2minω2min\nν 5/2 max √ τmax\n− α01τ 3/2 min√\nνmax + (181)\n√ τmax(α01 + α01T11 − 1)\nν 3/2 min\n) + α01τ 2 maxe t21 erfc(t1) ) = 0.21232788238624354 .\n• ∂J12∂τ We use Lemma 34 to obtain an upper bound on the maximum of the expression of the lemma:√\n2\nπ\n( 0.12 · 0.12(−1)(α01 − 1)\n(0.8 · 0.8)3/2 − √ 0.8 · 0.8α01 + (0.1 · 0.1)α01 − α01 + 1√ 0.8 · 0.8\n) = −1.72296 .\n(182)\nWe use Lemma 34 to obtain an lower bound on the minimum of the expression of the lemma:√ 2\nπ\n( 0.12 · 0.12(−1)(α01 − 1)\n(1.5 · 1.25)3/2 − √ 1.5 · 1.25α01 + (−0.1 · 0.1)α01 − α01 + 1√ 1.5 · 1.25\n) = −2.2302 .\n(183)\nNext we apply Lemma 37 for the expression ντe (µω+ντ)2 2ντ erfc ( µω+ντ√\n2 √ ντ\n) . We use Lemma 37\nto obtain an upper bound on the maximum of this expression:\n1.5 · 1.25e (1.5·1.25−0.1·0.1)2\n2·1.5·1.25 α01 erfc\n( 1.5 · 1.25− 0.1 · 0.1√\n2 √ 1.5 · 1.25\n) = 1.37381 . (184)\nWe use Lemma 37 to obtain an lower bound on the minimum of this expression:\n0.8 · 0.8e (0.8·0.8+0.1·0.1)2\n2·0.8·0.8 α01 erfc\n( 0.8 · 0.8 + 0.1 · 0.1√\n2 √ 0.8 · 0.8\n) = 0.620462 . (185)\nNext we apply Lemma 23 for 2αe (µω+ντ)2 2ντ erfc ( µω+ντ√\n2 √ ντ\n) . An upper bound on this expres-\nsion is\n2e (0.8·0.8−0.1·0.1)2\n20.8·0.8 α01 erfc\n( 0.8 · 0.8− 0.1 · 0.1√\n2 √ 0.8 · 0.8\n) = 1.96664 . (186)\nA lower bound on this expression is\n2e (1.5·1.25+0.1·0.1)2\n2·1.5·1.25 α01 erfc\n( 1.5 · 1.25 + 0.1 · 0.1√\n2 √ 1.5 · 1.25\n) = 1.4556 . (187)\nThe sum of the minimal values of the terms is−2.23019+0.62046+1.45560 = −0.154133. The sum of the maximal values of the terms is −1.72295 + 1.37380 + 1.96664 = 1.61749. Thus, an upper bound on the maximal absolute value is\n1 8 λ01e t4\n( α01T22e (t11+T22) 2 2T22 erfc ( t11 + T22√\n2 √ T22\n) + (188)\n2α01e t21 erfc(t1) +\n√ 2\nπ\n( − (α01 − 1)T 2 11\nt 3/2 22\n+ −α01 + α01T11 + 1√\nt22 −\nα01 √ t22 )) = 0.2124377655377270 .\n• ∂J21∂µ An upper bound on the maximum is\nλ201ω 2 max ( α201e T 21 ( −e−T4 ) erfc(T1) + 2α 2 01e t22et4 erfc(t2) − erfc(T3) + 2 ) = (189)\n0.0222044 .\nA upper bound on the absolute minimum is\nλ201ω 2 max ( α201e t21 ( −e−t4 ) erfc(t1) + 2α 2 01e T 22 eT4 erfc(T2) − erfc(t3) + 2 ) = (190)\n0.00894889 .\nThus, an upper bound on the maximal absolute value is\nλ201ω 2 max ( α201e T 21 ( −e−T4 ) erfc(T1) + 2α 2 01e t22et4 erfc(t2) − erfc(T3) + 2 ) = (191)\n0.02220441024325437 .\n• ∂J21∂ω An upper bound on the maximum is\nλ201 ( α201(2T11 + 1)e t22e−t4 erfc(t2) + 2T11(2− erfc(T3)) + (192)\nα201(t11 + 1)e T 21 ( −e−T4 ) erfc(T1) +\n√ 2\nπ\n√ T22e −t4 ) = 1.14696 .\nA lower bound on the minimum is\nλ201 ( α201(T11 + 1)e t21 ( −e−t4 ) erfc(t1) + (193)\nα201(2t11 + 1)e T 22 e−T4 erfc(T2) + 2t11(2− erfc(T3))+\n√ 2\nπ\n√ t22e −T4 ) = −0.359403 .\nThus, an upper bound on the maximal absolute value is\nλ201 ( α201(2T11 + 1)e t22e−t4 erfc(t2) + 2T11(2− erfc(T3)) + (194)\nα201(t11 + 1)e T 21 ( −e−T4 ) erfc(T1) +\n√ 2\nπ\n√ T22e −t4 ) = 1.146955401845684 .\n• ∂J21∂ν An upper bound on the maximum is\n1 2 λ201τmaxωmaxe −t4\nα201 (−eT 21 ) erfc(T1) + 4α201et22 erfc(t2) + √ 2 π (−1) ( α201 − 1 ) √ T22  = (195)\n0.149834 .\nA lower bound on the minimum is\n1 2 λ201τmaxωmaxe −t4\nα201 (−et21) erfc(t1) + 4α201eT 22 erfc(T2) + √ 2 π (−1) ( α201 − 1 ) √ t22  = (196)\n− 0.0351035 . Thus, an upper bound on the maximal absolute value is\n1 2 λ201τmaxωmaxe −t4\nα201 (−eT 21 ) erfc(T1) + 4α201et22 erfc(t2) + √ 2 π (−1) ( α201 − 1 ) √ T22  = (197)\n0.14983446469110305 .\n• ∂J21∂τ An upper bound on the maximum is\n1 2 λ201νmaxωmaxe −t4\nα201 (−eT 21 ) erfc(T1) + 4α201et22 erfc(t2) + √ 2 π (−1) ( α201 − 1 ) √ T22  = (198)\n0.179801 .\nA lower bound on the minimum is\n1 2 λ201νmaxωmaxe −t4\nα201 (−et21) erfc(t1) + 4α201eT 22 erfc(T2) + √ 2 π (−1) ( α201 − 1 ) √ t22  = (199)\n− 0.0421242 . Thus, an upper bound on the maximal absolute value is\n1 2 λ201νmaxωmaxe −t4\nα201 (−eT 21 ) erfc(T1) + 4α201et22 erfc(t2) + √ 2 π (−1) ( α201 − 1 ) √ T22  = (200)\n0.17980135762932363 .\n• ∂J22∂µ We use the fact that ∂J22∂µ = ∂J21 ∂ν . Thus, an upper bound on the maximal absolute value is\n1 2 λ201τmaxωmaxe −t4\nα201 (−eT 21 ) erfc(T1) + 4α201et22 erfc(t2) + √ 2 π (−1) ( α201 − 1 ) √ T22  = (201)\n0.14983446469110305 .\n• ∂J22∂ω An upper bound on the maximum is\n1 2 λ201µmaxτmaxe −t4\nα201 (−eT 21 ) erfc(T1) + 4α201et22 erfc(t2) + √ 2 π (−1) ( α201 − 1 ) √ T22  = (202)\n0.149834 .\nA lower bound on the minimum is\n1 2 λ201µmaxτmaxe −t4\nα201 (−et21) erfc(t1) + 4α201eT 22 erfc(T2) + √ 2 π (−1) ( α201 − 1 ) √ t22  = (203)\n− 0.0351035 . Thus, an upper bound on the maximal absolute value is\n1 2 λ201µmaxτmaxe −t4\nα201 (−eT 21 ) erfc(T1) + 4α201et22 erfc(t2) + √ 2 π (−1) ( α201 − 1 ) √ T22  = (204)\n0.14983446469110305 .\n• ∂J22∂ν\nWe apply Lemma 35 to the expression √\n2 π\n( (α2−1)µω\n(ντ)3/2 − 3α 2 √ ντ\n) . Using Lemma 35, an\nupper bound on the maximum is 1\n4 λ201τ 2 maxe\n−t4 ( α201 ( −eT 2 1 ) erfc(T1) + 8α 2 01e\nt22 erfc(t2) + (205)√ 2\nπ\n(( α201 − 1 ) T11\nT 3/2 22\n− 3α 2 01√ T22\n)) = 1.19441 .\nUsing Lemma 35, a lower bound on the minimum is 1\n4 λ201τ 2 maxe\n−t4 ( α201 ( −et 2 1 ) erfc(t1) + 8α 2 01e\nT 22 erfc(T2) + (206)√ 2\nπ\n(( α201 − 1 ) t11\nt 3/2 22\n− 3α 2 01√ t22\n)) = −1.80574 .\nThus, an upper bound on the maximal absolute value is\n− 1 4 λ201τ 2 maxe\n−t4 ( α201 ( −et 2 1 ) erfc(t1) + 8α 2 01e\nT 22 erfc(T2) + (207)√ 2\nπ\n(( α201 − 1 ) t11\nt 3/2 22\n− 3α 2 01√ t22\n)) = 1.805740052651535 .\n• ∂J22∂τ\nWe apply Lemma 36 to the expression √\n2 π\n( (α2−1)µω√\nντ − 3α2\n√ ντ ) .\nWe apply Lemma 37 to the expression ντe (µω+ντ)2 2ντ erfc ( µω+ντ√\n2 √ ντ\n) . We apply Lemma 38 to\nthe expression ντe (µω+2ντ)2 2ντ erfc ( µω+2ντ√\n2 √ ντ\n) .\nWe combine the results of these lemmata to obtain an upper bound on the maximum:\n1 4 λ201\n( −α201t22e−T4e (T11+t22) 2 2t22 erfc ( T11 + t22√\n2 √ t22\n) + (208)\n8α201T22e −t4e\n(t11+2T22) 2\n2T22 erfc ( t11 + 2T22√\n2 √ T22\n) −\n2α201e T 21 e−T4 erfc(T1) + 4α 2 01e t22e−t4 erfc(t2) + 2(2− erfc(T3)) +√ 2\nπ e−T4\n(( α201 − 1 ) T11√\nt22 − 3α201\n√ t22 )) = 2.39669 .\nWe combine the results of these lemmata to obtain an lower bound on the minimum: 1\n4 λ201\n( 8α201t22e −T4e (T11+2t22) 2 2t22 erfc ( T11 + 2t22√\n2 √ t22\n) + (209)\nα201T22e −t4e\n(t11+T22) 2\n2T22 erfc ( t11 + T22√\n2 √ T22\n) −\n2α201e t21e−t4 erfc(t1) + 4α 2 01e T 22 e−T4 erfc(T2) + 2(2− erfc(t3)) + √ 2\nπ e−t4\n(( α201 − 1 ) t11√\nT22 − 3α201\n√ T22 )) = −1.17154 .\nThus, an upper bound on the maximal absolute value is\n1 4 λ201\n( −α201t22e−T4e (T11+t22) 2 2t22 erfc ( T11 + t22√\n2 √ t22\n) + (210)\n8α201T22e −t4e\n(t11+2T22) 2\n2T22 erfc ( t11 + 2T22√\n2 √ T22\n) −\n2α201e T 21 e−T4 erfc(T1) + 4α 2 01e t22e−t4 erfc(t2) + 2(2− erfc(T3)) +√ 2\nπ e−T4\n(( α201 − 1 ) T11√\nt22 − 3α201\n√ t22 )) = 2.396685907216327 .\nLemma 40 (Derivatives of the Mapping). We assume α = α01 and λ = λ01. We restrict the range of the variables to the domain µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.8, 1.25].\nThe derivative ∂∂µ µ̃(µ, ω, ν, τ, λ, α) has the sign of ω.\nThe derivative ∂∂ν µ̃(µ, ω, ν, τ, λ, α) is positive.\nThe derivative ∂∂µ ξ̃(µ, ω, ν, τ, λ, α) has the sign of ω.\nThe derivative ∂∂ν ξ̃(µ, ω, ν, τ, λ, α) is positive.\nProof. • ∂∂µ µ̃(µ, ω, ν, τ, λ, α)\n(2− erfc(x) > 0 according to Lemma 21 and ex2 erfc(x) is also larger than zero according to Lemma 23. Consequently, has ∂∂µ µ̃(µ, ω, ν, τ, λ, α) the sign of ω.\n• ∂∂ν µ̃(µ, ω, ν, τ, λ, α)\nLemma 23 says ex 2 erfc(x) is decreasing in µω+ντ√ 2 √ ντ\n. The first term (negative) is increasing in ντ since it is proportional to minus one over the squared root of ντ .\nWe obtain a lower bound by setting µω+ντ√ 2 √ ντ = 1.5·1.25+0.1·0.1√ 2 √ 1.5·1.25 for the e x2 erfc(x)\nterm. The term in brackets is larger than e ( 1.5·1.25+0.1·0.1√ 2 √ 1.5·1.25 )2 α01 erfc ( 1.5·1.25+0.1·0.1√\n2 √ 1.5·1.25 ) −√\n2 π0.8·0.8 (α01 − 1) = 0.056 Consequently, the function is larger than zero.\n• ∂∂µ ξ̃(µ, ω, ν, τ, λ, α)\nWe consider the sub-function√ 2\nπ\n√ ντ − α2 ( e ( µω+ντ√ 2 √ ντ )2 erfc ( µω + ντ√\n2 √ ντ\n) − e ( µω+2ντ√ 2 √ ντ )2 erfc ( µω + 2ντ√\n2 √ ντ\n)) .\n(211)\nWe set x = ντ and y = µω and obtain√ 2\nπ\n√ x− α2 ( e ( x+y√ 2 √ x )2 erfc ( x+ y√\n2 √ x\n) − e ( 2x+y√ 2 √ x )2 erfc ( 2x+ y√\n2 √ x\n)) . (212)\nThe derivative of this sub-function with respect to y is α2 ( e (2x+y)2 2x (2x+ y) erfc (\n2x+y√ 2 √ x\n) − e (x+y)2 2x (x+ y) erfc ( x+y√ 2 √ x )) x = (213)\n√ 2α2 √ x  e (2x+y)22x (x+y) erfc( x+y√2√x)√ 2 √ x − e (x+y)2 2x (x+y) erfc ( x+y√ 2 √ x ) √ 2 √ x  x > 0 .\nThe inequality follows from Lemma 24, which states that zez 2\nerfc(z) is monotonically increasing in z. Therefore the sub-function is increasing in y.\nThe derivative of this sub-function with respect to x is √ πα2 ( e (2x+y)2 2x ( 4x2 − y2 ) erfc ( 2x+y√\n2 √ x\n) − e (x+y)2 2x (x− y)(x+ y) erfc ( x+y√ 2 √ x )) − √ 2 ( α2 − 1 ) x3/2\n2 √ πx2\n.\n(214)\nThe sub-function is increasing in x, since the derivative is larger than zero: √ πα2 ( e (2x+y)2 2x ( 4x2 − y2 ) erfc ( 2x+y√\n2 √ x\n) − e (x+y)2 2x (x− y)(x+ y) erfc ( x+y√ 2 √ x )) − √ 2x3/2 ( α2 − 1 ) 2 √ πx2 >\n(215)\n√ πα2  (2x−y)(2x+y)2√ π ( 2x+y√ 2 √ x + √( 2x+y√ 2 √ x )2 +2 ) − (x−y)(x+y)2 √ π ( x+y√ 2 √ x + √( x+y√ 2 √ x )2 + 4π ) −√2x3/2 (α2 − 1)\n2 √ πx2\n=\n√ πα2\n( (2x−y)(2x+y)2( √ 2 √ x)\n√ π ( 2x+y+ √ (2x+y)2+4x ) − (x−y)(x+y)2(√2√x)√ π ( x+y+ √ (x+y)2+ 8xπ ))−√2x3/2 (α2 − 1) 2 √ πx2 =\n√ πα2\n( (2x−y)(2x+y)2\n√ π ( 2x+y+ √ (2x+y)2+4x ) − (x−y)(x+y)2√ π ( x+y+ √ (x+y)2+ 8xπ ))− x (α2 − 1) √\n2 √ πx3/2\n>\n√ πα2\n( (2x−y)(2x+y)2\n√ π ( 2x+y+ √ (2x+y)2+2(2x+y)+1 ) − (x−y)(x+y)2√ π ( x+y+ √ (x+y)2+0.782·2(x+y)+0.7822 ))− x (α2 − 1) √\n2 √ πx3/2\n=\n√ πα2\n( (2x−y)(2x+y)2\n√ π ( 2x+y+ √ (2x+y+1)2 ) − (x−y)(x+y)2√ π ( x+y+ √ (x+y+0.782)2 ))− x (α2 − 1) √\n2 √ πx3/2\n=\n√ πα2 ( (2x−y)(2x+y)2√ π(2(2x+y)+1) − (x−y)(x+y)2√ π(2(x+y)+0.782) ) − x ( α2 − 1 ) √\n2 √ πx3/2\n=\n√ πα2 ( (2(x+y)+0.782)(2x−y)(2x+y)2√\nπ − (x−y)(x+y)(2(2x+y)+1)2√ π ) (2(2x+ y) + 1)(2(x+ y) + 0.782) √ 2 √ πx3/2 +\n√ πα2 ( −x ( α2 − 1 ) (2(2x+ y) + 1)(2(x+ y) + 0.782) ) (2(2x+ y) + 1)(2(x+ y) + 0.782) √ 2 √ πx3/2 =\n8x3 + (12y + 2.68657)x2 + (y(4y − 6.41452)− 1.40745)x+ 1.22072y2\n(2(2x+ y) + 1)(2(x+ y) + 0.782) √ 2 √ πx3/2\n>\n8x3 + (2.68657− 120.01)x2 + (0.01(−6.41452− 40.01)− 1.40745)x+ 1.22072(0.0)2\n(2(2x+ y) + 1)(2(x+ y) + 0.782) √ 2 √ πx3/2\n=\n8x2 + 2.56657x− 1.472 (2(2x+ y) + 1)(2(x+ y) + 0.782) √ 2 √ π √ x =\n8x2 + 2.56657x− 1.472 (2(2x+ y) + 1)(2(x+ y) + 0.782) √ 2 √ π √ x =\n8(x+ 0.618374)(x− 0.297553) (2(2x+ y) + 1)(2(x+ y) + 0.782) √ 2 √ π √ x > 0 .\nWe explain this chain of inequalities:\n– First inequality: We applied Lemma 22 two times. – Equalities factor out √ 2 √ x and reformulate.\n– Second inequality part 1: we applied\n0 < 2y =⇒ (2x+ y)2 + 4x+ 1 < (2x+ y)2 + 2(2x+ y) + 1 = (2x+ y + 1)2 . (216)\n– Second inequality part 2: we show that for a = 120 (√ 2048+169π π − 13 ) following\nholds: 8xπ − ( a2 + 2a(x+ y) ) > 0. We have ∂∂x 8x π − ( a2 + 2a(x+ y) ) = 8π−2a > 0 and ∂∂y 8x π − ( a2 + 2a(x+ y) ) = −2a > 0. Therefore the minimum is at border for minimal x and maximal y:\n8 · 0.64 π −  2 20 (√ 2048 + 169π π − 13 ) (0.64 + 0.01) + ( 1 20 (√ 2048 + 169π π − 13 ))2 = 0 . (217)\nThus 8x\nπ > a2 + 2a(x+ y) . (218)\nfor a = 120 (√ 2048+169π π − 13 ) > 0.782.\n– Equalities only solve square root and factor out the resulting terms (2(2x + y) + 1) and (2(x+ y) + 0.782).\n– We set α = α01 and multiplied out. Thereafter we also factored out x in the numerator. Finally a quadratic equations was solved.\nThe sub-function has its minimal value for minimal x and minimal y x = ντ = 0.8 · 0.8 = 0.64 and y = µω = −0.1 · 0.1 = −0.01. We further minimize the function\nµωe µ2ω2 2ντ ( 2− erfc ( µω√ 2 √ ντ )) > −0.01e 0.01 2 20.64 ( 2− erfc ( 0.01√ 2 √ 0.64 )) . (219)\nWe compute the minimum of the term in brackets of ∂∂µ ξ̃(µ, ω, ν, τ, λ, α):\nµωe µ2ω2 2ντ ( 2− erfc ( µω√ 2 √ ντ )) + (220)\nα201\n( − ( e ( µω+ντ√ 2 √ ντ )2 erfc ( µω + ντ√\n2 √ ντ\n) − e ( µω+2ντ√ 2 √ ντ )2 erfc ( µω + 2ντ√\n2 √ ντ\n))) + √ 2\nπ\n√ ντ >\nα201\n( − ( e ( 0.64−0.01√ 2 √ 0.64 )2 erfc ( 0.64− 0.01√\n2 √ 0.64\n) − e ( 20.64−0.01√ 2 √ 0.64 )2 erfc ( 2 · 0.64− 0.01√\n2 √ 0.64\n))) −\n0.01e 0.012 20.64 ( 2− erfc ( 0.01√ 2 √ 0.64 )) + √ 0.64 √ 2 π = 0.0923765 .\nTherefore the term in brackets is larger than zero.\nThus, ∂∂µ ξ̃(µ, ω, ν, τ, λ, α) has the sign of ω.\n• ∂∂ν ξ̃(µ, ω, ν, τ, λ, α) We look at the sub-term\n2e\n( 2x+y√ 2 √ x )2 erfc ( 2x+ y√\n2 √ x\n) − e ( x+y√ 2 √ x )2 erfc ( x+ y√\n2 √ x\n) . (221)\nWe obtain a chain of inequalities:\n2e\n( 2x+y√ 2 √ x )2 erfc ( 2x+ y√\n2 √ x\n) − e ( x+y√ 2 √ x )2 erfc ( x+ y√\n2 √ x\n) > (222)\n2 · 2 √ π ( 2x+y√\n2 √ x +\n√( 2x+y√\n2 √ x\n)2 + 2 ) − 2 √ π ( x+y√ 2 √ x + √( x+y√ 2 √ x )2 + 4π ) =\n2 √ 2 √ x ( 2√\n(2x+y)2+4x+2x+y − 1√\n(x+y)2+ 8xπ +x+y ) √ π >\n2 √ 2 √ x ( 2√\n(2x+y)2+2(2x+y)+1+2x+y − 1√ (x+y)2+0.782·2(x+y)+0.7822+x+y ) √ π =\n2 √ 2 √ x (\n2 2(2x+y)+1 − 1 2(x+y)+0.782 ) √ π\n=( 2 √ 2 √ x )\n(2(2(x+ y) + 0.782)− (2(2x+ y) + 1)) √ π((2(x+ y) + 0.782)(2(2x+ y) + 1))\n=( 2 √ 2 √ x )\n(2y + 0.782 · 2− 1) √ π((2(x+ y) + 0.782)(2(2x+ y) + 1)) > 0 .\nWe explain this chain of inequalities:\n– First inequality: We applied Lemma 22 two times. – Equalities factor out √ 2 √ x and reformulate.\n– Second inequality part 1: we applied 0 < 2y =⇒ (2x+ y)2 + 4x+ 1 < (2x+ y)2 + 2(2x+ y) + 1 = (2x+ y + 1)2 .\n(223)\n– Second inequality part 2: we show that for a = 120 (√ 2048+169π π − 13 ) following\nholds: 8xπ − ( a2 + 2a(x+ y) ) > 0. We have ∂∂x 8x π − ( a2 + 2a(x+ y) ) = 8π−2a > 0 and ∂∂y 8x π − ( a2 + 2a(x+ y) ) = −2a < 0. Therefore the minimum is at border for minimal x and maximal y:\n8 · 0.64 π −  2 20 (√ 2048 + 169π π − 13 ) (0.64 + 0.01) + ( 1 20 (√ 2048 + 169π π − 13 ))2 = 0 . (224)\nThus 8x\nπ > a2 + 2a(x+ y) . (225)\nfor a = 120 (√ 2048+169π π − 13 ) > 0.782.\n– Equalities only solve square root and factor out the resulting terms (2(2x + y) + 1) and (2(x+ y) + 0.782).\nWe know that (2− erfc(x) > 0 according to Lemma 21. For the sub-term we derived\n2e\n( 2x+y√ 2 √ x )2 erfc ( 2x+ y√\n2 √ x\n) − e ( x+y√ 2 √ x )2 erfc ( x+ y√\n2 √ x\n) > 0 . (226)\nConsequently, both terms in the brackets of ∂∂ν ξ̃(µ, ω, ν, τ, λ, α) are larger than zero. Therefore ∂∂ν ξ̃(µ, ω, ν, τ, λ, α) is larger than zero.\nLemma 41 (Mean at low variance). The mapping of the mean µ̃ (Eq. (4))\nµ̃(µ, ω, ν, τ, λ, α) = 1\n2 λ\n( −(α+ µω) erfc ( µω√ 2 √ ντ ) + (227)\nαeµω+ ντ 2 erfc ( µω + ντ√\n2 √ ντ\n) + √ 2\nπ\n√ ντe− µ2ω2 2ντ + 2µω ) in the domain −0.1 6 µ 6 −0.1, −0.1 6 ω 6 −0.1, and 0.02 6 ντ 6 0.5 is bounded by\n|µ̃(µ, ω, ν, τ, λ01, α01)| < 0.289324 (228)\nand\nlim ν→0 |µ̃(µ, ω, ν, τ, λ01, α01)| = λµω. (229)\nWe can consider µ̃ with given µω as a function in x = ντ . We show the graph of this function at the maximal µω = 0.01 in the interval x ∈ [0, 1] in Figure A6.\nProof. Since µ̃ is strictly monotonically increasing with µω\nµ̃(µ, ω, ν, τ, λ, α) 6 (230) µ̃(0.1, 0.1, ν, τ, λ, α) 6\n1 2 λ\n( −(α+ 0.01) erfc ( 0.01√ 2 √ ντ ) + αe0.01+ ντ 2 erfc ( 0.01 + ντ√ 2 √ ντ ) + √ 2 π √ ντe− 0.012 2ντ + 2 · 0.01 ) 6\n1 2 λ01\n( e 0.05 2 +0.01α01 erfc ( 0.02 + 0.01√\n2 √ 0.02\n) − (α01 + 0.01) erfc ( 0.01√ 2 √ 0.02 ) + e− 0.012 2·0.5 √ 0.5 √ 2 π + 0.01 · 2 ) < 0.21857,\nwhere we have used the monotonicity of the terms in ντ .\n0.5 1.0 1.5 2.0 2.5 3.0\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nx\nFigure A6: The graph of function µ̃ for low variances x = ντ for µω = 0.01, where x ∈ [0, 3], is displayed in yellow. Lower and upper bounds based on the Abramowitz bounds (Lemma 22) are displayed in green and blue, respectively.\nSimilarly, we can use the monotonicity of the terms in ντ to show that\nµ̃(µ, ω, ν, τ, λ, α) > µ̃(0.1,−0.1, ν, τ, λ, α) > −0.289324, (231)\nsuch that |µ̃| < 0.289324 at low variances. Furthermore, when (ντ)→ 0, the terms with the arguments of the complementary error functions erfc and the exponential function go to infinity, therefore these three terms converge to zero. Hence, the remaining terms are only 2µω 12λ.\nLemma 42 (Bounds on derivatives of µ̃ in Ω−). The derivatives of the function µ̃(µ, ω, ν, τ, λ01, α01 (Eq. (4)) with respect to µ, ω, ν, τ in the domain Ω− = {µ, ω, ν, τ | − 0.1 6 µ 6 0.1,−0.1 6 ω 6 0.1, 0.05 6 ν 6 0.24, 0.8 6 τ 6 1.25} can be bounded as follows:\n∣∣∣∣ ∂∂µµ̃ ∣∣∣∣ < 0.14 (232)∣∣∣∣ ∂∂ω µ̃ ∣∣∣∣ < 0.14∣∣∣∣ ∂∂ν µ̃ ∣∣∣∣ < 0.52∣∣∣∣ ∂∂τ µ̃ ∣∣∣∣ < 0.11.\nProof. The expression\n∂\n∂µ µ̃ = J11 =\n1 2 λωe −(µω)2 2ντ\n( 2e (µω)2 2ντ − e (µω)2 2ντ erfc ( µω√ 2 √ ντ ) + αe (µω+ντ)2 2ντ erfc ( µω + ντ√ 2 √ ντ )) (233)\ncontains the terms e (µω)2 2ντ erfc (\nµω√ 2 √ ντ\n) and e (µω+ντ)2 2ντ erfc ( µω+ντ√\n2 √ ντ\n) which are monotonically de-\ncreasing in their arguments (Lemma 23). We can therefore obtain their minima and maximal at the minimal and maximal arguments. Since the first term has a negative sign in the expression, both terms reach their maximal value at µω = −0.01, ν = 0.05, and τ = 0.8.∣∣∣∣ ∂∂µµ̃\n∣∣∣∣ 6 12 |λω| ∣∣∣(2− e0.03535532 erfc (0.0353553) + αe0.1060662 erfc (0.106066))∣∣∣ < 0.133 (234)\nSince, µ̃ is symmetric in µ and ω, these bounds also hold for the derivate to ω.\n0.0 0.2 0.4 0.6 0.8 1.0 x0.000\n0.001\n0.002\n0.003\n0.004\n0.005\nh(x)\nFigure A7: The graph of the function h(x) = µ̃2(0.1,−0.1, x, 1, λ01, α01) is displayed. It has a local maximum at x = ντ ≈ 0.187342 and h(x) ≈ 0.00451457 in the domain x ∈ [0, 1].\nWe use the argumentation that the term with the error function is monotonically decreasing (Lemma 23) again for the expression\n∂\n∂ν µ̃ = J12 = (235)\n= 1\n4 λτe−\nµ2ω2\n2ντ ( αe (µω+ντ)2 2ντ erfc ( µω + ντ√\n2 √ ντ\n) − (α− 1) √ 2\nπντ ) 6∣∣∣∣14λτ\n∣∣∣∣ (|1.1072− 2.68593|) < 0.52. We have used that the term 1.1072 6 α01e (µω+ντ)2 2ντ erfc ( µω+ντ√\n2 √ ντ\n) 6 1.49042 and the term\n0.942286 6 (α − 1) √\n2 πντ 6 2.68593. Since µ̃ is symmetric in ν and τ , we only have to chance outermost term ∣∣ 1 4λτ ∣∣ to ∣∣ 14λν∣∣ to obtain the estimate ∣∣ ∂∂τ µ̃∣∣ < 0.11.\nLemma 43 (Tight bound on µ̃2 in Ω−). The function µ̃2(µ, ω, ν, τ, λ01, α01) (Eq. (4)) is bounded by∣∣µ̃2∣∣ < 0.005 (236) (237)\nin the domain Ω− = {µ, ω, ν, τ | − 0.1 6 µ 6 0.1,−0.1 6 ω 6 0.1, 0.05 6 ν 6 0.24, 0.8 6 τ 6 1.25}.\nWe visualize the function µ̃2 at its maximal µν = −0.01 and for x = ντ in the form h(x) = µ̃2(0.1,−0.1, x, 1, λ01, α01) in Figure A7.\nProof. We use a similar strategy to the one we have used to show the bound on the singular value (Lemmata 10, 11, and 12), where we evaluted the function on a grid and used bounds on the derivatives together with the mean value theorem. Here we have∣∣µ̃2(µ, ω, ν, τ, λ01, α01)− µ̃2(µ+ ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ01, α01)∣∣ 6 (238)∣∣∣∣ ∂∂µµ̃2 ∣∣∣∣ |∆µ|+ ∣∣∣∣ ∂∂ω µ̃2 ∣∣∣∣ |∆ω|+ ∣∣∣∣ ∂∂ν µ̃2 ∣∣∣∣ |∆ν|+ ∣∣∣∣ ∂∂τ µ̃2 ∣∣∣∣ |∆τ |.\nWe use Lemma 42 and Lemma 41, to obtain∣∣∣∣ ∂∂µµ̃2 ∣∣∣∣ = 2 |µ̃| ∣∣∣∣ ∂∂µµ̃ ∣∣∣∣ < 2 · 0.289324 · 0.14 = 0.08101072 (239)∣∣∣∣ ∂∂ω µ̃2 ∣∣∣∣ = 2 |µ̃| ∣∣∣∣ ∂∂ω µ̃ ∣∣∣∣ < 2 · 0.289324 · 0.14 = 0.08101072\n∣∣∣∣ ∂∂ν µ̃2 ∣∣∣∣ = 2 |µ̃| ∣∣∣∣ ∂∂ν µ̃ ∣∣∣∣ < 2 · 0.289324 · 0.52 = 0.30089696∣∣∣∣ ∂∂τ µ̃2 ∣∣∣∣ = 2 |µ̃| ∣∣∣∣ ∂∂τ µ̃\n∣∣∣∣ < 2 · 0.289324 · 0.11 = 0.06365128 We evaluated the function µ̃2 in a grid G of Ω− with ∆µ = 0.001498041, ∆ω = 0.001498041, ∆ν = 0.0004033190, and ∆τ = 0.0019065994 using a computer and obtained the maximal value maxG(µ̃) 2 = 0.00451457, therefore the maximal value of µ̃2 is bounded by\nmax (µ,ω,ν,τ)∈Ω−\n(µ̃)2 6 (240)\n0.00451457 + 0.001498041 · 0.08101072 + 0.001498041 · 0.08101072+ 0.0004033190 · 0.30089696 + 0.0019065994 · 0.06365128 < 0.005. (241)\nFurthermore we used error propagation to estimate the numerical error on the function evaluation. Using the error propagation rules derived in Subsection A3.4.5, we found that the numerical error is smaller than 10−13 in the worst case.\nLemma 44 (Main subfunction). For 1.2 6 x 6 20 and −0.1 6 y 6 0.1, the function\ne (x+y)2 2x erfc ( x+ y√\n2 √ x\n) − 2e (2x+y)2 2x erfc ( 2x+ y√\n2 √ x\n) (242)\nis smaller than zero, is strictly monotonically increasing in x, and strictly monotonically decreasing in y for the minimal x = 12/10 = 1.2.\nProof. We first consider the derivative of sub-function Eq. (101) with respect to x. The derivative of the function\ne (x+y)2 2x erfc ( x+ y√\n2 √ x\n) − 2e (2x+y)2 2x erfc ( 2x+ y√\n2 √ x\n) (243)\nwith respect to x is √ π ( e (x+y)2 2x (x− y)(x+ y) erfc ( x+y√ 2 √ x ) − 2e (2x+y)2 2x ( 4x2 − y2 ) erfc ( 2x+y√ 2 √ x )) + √ 2 √ x(3x− y)\n2 √ πx2\n=\n(244) √ π ( e (x+y)2 2x (x− y)(x+ y) erfc ( x+y√ 2 √ x ) − 2e (2x+y)2 2x (2x+ y)(2x− y) erfc ( 2x+y√ 2 √ x )) + √ 2 √ x(3x− y)\n2 √ πx2\n=\n√ π  e (x+y)22x (x−y)(x+y) erfc( x+y√2√x)√ 2 √ x − 2e (2x+y)2 2x (2x+y)(2x−y) erfc ( 2x+y√ 2 √ x ) √ 2 √ x + (3x− y) 2 √ 2 √ πx2 √ x .\nWe consider the numerator\n√ π e (x+y) 2 2x (x− y)(x+ y) erfc ( x+y√ 2 √ x ) √\n2 √ x\n− 2e\n(2x+y)2 2x (2x+ y)(2x− y) erfc (\n2x+y√ 2 √ x ) √\n2 √ x\n+ (3x− y) . (245)\nFor bounding this value, we use the approximation\nez 2 erfc(z) ≈ 2.911√ π(2.911− 1)z + √ πz2 + 2.9112 . (246)\nfrom Ren and MacKenzie [30]. We start with an error analysis of this approximation. According to Ren and MacKenzie [30] (Figure 1), the approximation error is positive in the range [0.7, 3.2]. This range contains all possible arguments of erfc that we consider. Numerically we maximized and minimized the approximation error of the whole expression\nE(x, y) = e (x+y) 2 2x (x− y)(x+ y) erfc ( x+y√ 2 √ x ) √\n2 √ x\n− 2e\n(2x+y)2 2x (2x− y)(2x+ y) erfc (\n2x+y√ 2 √ x ) √\n2 √ x  − (247) 2.911(x− y)(x+ y)(√ 2 √ x )(√π(2.911−1)(x+y)√\n2 √ x\n+ √ π ( x+y√ 2 √ x )2 + 2.9112 ) −\n2 · 2.911(2x− y)(2x+ y)(√ 2 √ x )(√π(2.911−1)(2x+y)√\n2 √ x\n+ √ π (\n2x+y√ 2 √ x\n)2 + 2.9112\n)  .\nWe numerically determined 0.0113556 6 E(x, y) 6 0.0169551 for 1.2 6 x 6 20 and −0.1 6 y 6 0.1. We used different numerical optimization techniques like gradient based constraint BFGS algorithms and non-gradient-based Nelder-Mead methods with different start points. Therefore our approximation is smaller than the function that we approximate. We subtract an additional safety gap of 0.0131259 from our approximation to ensure that the inequality via the approximation holds true. With this safety gap the inequality would hold true even for negative x, where the approximation error becomes negative and the safety gap would compensate. Of course, the safety gap of 0.0131259 is not necessary for our analysis but may help or future investigations.\nWe have the sequences of inequalities using the approximation of Ren and MacKenzie [30]:\n(3x− y) + e (x+y) 2 2x (x− y)(x+ y) erfc ( x+y√ 2 √ x ) √\n2 √ x\n− 2e\n(2x+y)2 2x (2x− y)(2x+ y) erfc (\n2x+y√ 2 √ x ) √\n2 √ x\n√π > (248)\n(3x− y) +  2.911(x− y)(x+ y)(√ π ( x+y√ 2 √ x )2 + 2.9112 + (2.911−1) √ π(x+y)√ 2 √ x )(√ 2 √ x ) −\n2(2x− y)(2x+ y)2.911(√ 2 √ x )(√ π (\n2x+y√ 2 √ x\n)2 + 2.9112 + (2.911−1) √ π(2x+y)√\n2 √ x\n) √π − 0.0131259 =\n(3x− y) +  (√2√x2.911) (x− y)(x+ y)(√ π(x+ y)2 + 2 · 2.9112x+ (2.911− 1)(x+ y) √ π ) (√ 2 √ x ) −\n2(2x− y)(2x+ y) (√ 2 √ x2.911 )(√ 2 √ x ) (√ π(2x+ y)2 + 2 · 2.9112x+ (2.911− 1)(2x+ y) √ π ) √π − 0.0131259 =\n(3x− y) + 2.911  (x− y)(x+ y) (2.911− 1)(x+ y) + √ (x+ y)2 + 2·2.911\n2x π\n−\n2(2x− y)(2x+ y) (2.911− 1)(2x+ y) + √ (2x+ y)2 + 2·2.911 2x\nπ − 0.0131259 > (3x− y) + 2.911\n (x− y)(x+ y) (2.911− 1)(x+ y) + √( 2.9112\nπ\n)2 + (x+ y)2 + 2·2.911\n2x π + 2·2.9112y π\n−\n2(2x− y)(2x+ y) (2.911− 1)(2x+ y) + √ (2x+ y)2 + 2·2.911 2x\nπ − 0.0131259 = (3x− y) + 2.911\n (x− y)(x+ y) (2.911− 1)(x+ y) + √( x+ y + 2.911 2\nπ )2 − 2(2x− y)(2x+ y)\n(2.911− 1)(2x+ y) + √ (2x+ y)2 + 2·2.911 2x\nπ − 0.0131259 = (3x− y) + 2.911  (x− y)(x+ y) 2.911(x+ y) + 2.911 2\nπ\n− 2(2x− y)(2x+ y) (2.911− 1)(2x+ y) + √ (2x+ y)2 + 2·2.911 2x\nπ − 0.0131259 = (3x− y) + (x− y)(x+ y)\n(x+ y) + 2.911π − 2(2x− y)(2x+ y)2.911 (2.911− 1)(2x+ y) + √ (2x+ y)2 + 2·2.911 2x\nπ\n− 0.0131259 = (3x− y) + (x− y)(x+ y) (x+ y) + 2.911π − 2(2x− y)(2x+ y)2.911 (2.911− 1)(2x+ y) + √ (2x+ y)2 + 2·2.911 2x\nπ\n− 0.0131259 =\n( −2(2x− y)2.911 ( (x+ y) + 2.911\nπ ) (2x+ y) +(\n(x+ y) + 2.911\nπ\n) (3x− y − 0.0131259) ( (2.911− 1)(2x+ y) + √ (2x+ y)2 +\n2 · 2.9112x π\n) +\n(x− y)(x+ y) ( (2.911− 1)(2x+ y) + √ (2x+ y)2 +\n2 · 2.9112x π )) ((\n(x+ y) + 2.911\nπ\n)( (2.911− 1)(2x+ y) + √ (2x+ y)2 +\n2 · 2.9112x π ))−1 =(\n((x− y)(x+ y) + (3x− y − 0.0131259)(x+ y + 0.9266)) (√ (2x+ y)2 + 5.39467x+ 3.822x+ 1.911y ) −\n(249) 5.822(2x− y)(x+ y + 0.9266)(2x+ y))((\n(x+ y) + 2.911\nπ\n)( (2.911− 1)(2x+ y) + √ (2x+ y)2 + 22.9112x\nπ\n))−1 > 0 .\nWe explain this sequence of inequalities:\n• First inequality: The approximation of Ren and MacKenzie [30] and then subtracting a safety gap (which would not be necessary for the current analysis). • Equalities: The factor √ 2 √ x is factored out and canceled.\n• Second inequality: adds a positive term in the first root to obtain a binomial form. The term containing the root is positive and the root is in the denominator, therefore the whole term becomes smaller.\n• Equalities: solve for the term and factor out. • Bringing all terms to the denominator ( (x+ y) + 2.911π )( (2.911− 1)(2x+ y) + √ (2x+ y)2 + 2·2.911 2x π ) .\n• Equalities: Multiplying out and expanding terms.\n• Last inequality > 0 is proofed in the following sequence of inequalities.\nWe look at the numerator of the last expression of Eq. (248), which we show to be positive in order to show > 0 in Eq. (248). The numerator is\n((x− y)(x+ y) + (3x− y − 0.0131259)(x+ y + 0.9266)) (√ (2x+ y)2 + 5.39467x+ 3.822x+ 1.911y ) −\n(250) 5.822(2x− y)(x+ y + 0.9266)(2x+ y) = − 5.822(2x− y)(x+ y + 0.9266)(2x+ y) + (3.822x+ 1.911y)((x− y)(x+ y)+ (3x− y − 0.0131259)(x+ y + 0.9266)) + ((x− y)(x+ y)+\n(3x− y − 0.0131259)(x+ y + 0.9266)) √ (2x+ y)2 + 5.39467x =\n− 8.0x3 + ( 4x2 + 2xy + 2.76667x− 2y2 − 0.939726y − 0.0121625 )√ (2x+ y)2 + 5.39467x−\n8.0x2y − 11.0044x2 + 2.0xy2 + 1.69548xy − 0.0464849x+ 2.0y3 + 3.59885y2 − 0.0232425y = − 8.0x3 + ( 4x2 + 2xy + 2.76667x− 2y2 − 0.939726y − 0.0121625 )√ (2x+ y)2 + 5.39467x−\n8.0x2y − 11.0044x2 + 2.0xy2 + 1.69548xy − 0.0464849x+ 2.0y3 + 3.59885y2 − 0.0232425y .\nThe factor in front of the root is positive. If the term, that does not contain the root, was positive, then the whole expression would be positive and we would have proofed that the numerator is positive. Therefore we consider the case that the term, that does not contain the root, is negative. The term that contains the root must be larger than the other term in absolute values.\n− ( −8.0x3 − 8.0x2y − 11.0044x2 + 2.xy2 + 1.69548xy − 0.0464849x+ 2.y3 + 3.59885y2 − 0.0232425y ) < (251)( 4x2 + 2xy + 2.76667x− 2y2 − 0.939726y − 0.0121625 )√ (2x+ y)2 + 5.39467x .\nTherefore the squares of the root term have to be larger than the square of the other term to show > 0 in Eq. (248). Thus, we have the inequality:( −8.0x3 − 8.0x2y − 11.0044x2 + 2.xy2 + 1.69548xy − 0.0464849x+ 2.y3 + 3.59885y2 − 0.0232425y )2 < (252)( 4x2 + 2xy + 2.76667x− 2y2 − 0.939726y − 0.0121625 )2 ( (2x+ y)2 + 5.39467x ) .\nThis is equivalent to 0 < ( 4x2 + 2xy + 2.76667x− 2y2 − 0.939726y − 0.0121625 )2 ( (2x+ y)2 + 5.39467x ) − (253)(\n−8.0x3 − 8.0x2y − 11.0044x2 + 2.0xy2 + 1.69548xy − 0.0464849x+ 2.0y3 + 3.59885y2 − 0.0232425y )2 =\n− 1.2227x5 + 40.1006x4y + 27.7897x4 + 41.0176x3y2 + 64.5799x3y + 39.4762x3 + 10.9422x2y3− 13.543x2y2 − 28.8455x2y − 0.364625x2 + 0.611352xy4 + 6.83183xy3 + 5.46393xy2+ 0.121746xy + 0.000798008x− 10.6365y5 − 11.927y4 + 0.190151y3 − 0.000392287y2 .\nWe obtain the inequalities:\n− 1.2227x5 + 40.1006x4y + 27.7897x4 + 41.0176x3y2 + 64.5799x3y + 39.4762x3 + 10.9422x2y3− (254)\n13.543x2y2 − 28.8455x2y − 0.364625x2 + 0.611352xy4 + 6.83183xy3 + 5.46393xy2+ 0.121746xy + 0.000798008x− 10.6365y5 − 11.927y4 + 0.190151y3 − 0.000392287y2 =\n− 1.2227x5 + 27.7897x4 + 41.0176x3y2 + 39.4762x3 − 13.543x2y2 − 0.364625x2+ y ( 40.1006x4 + 64.5799x3 + 10.9422x2y2 − 28.8455x2 + 6.83183xy2 + 0.121746x −\n10.6365y4 + 0.190151y2 )\n+ 0.611352xy4 + 5.46393xy2 + 0.000798008x− 11.927y4 − 0.000392287y2 > − 1.2227x5 + 27.7897x4 + 41.0176 · (0.0)2x3 + 39.4762x3 − 13.543 · (0.1)2x2 − 0.364625x2− 0.1 · ( 40.1006x4 + 64.5799x3 + 10.9422 · (0.1)2x2 − 28.8455x2 + 6.83183 · (0.1)2x+ 0.121746x +\n10.6365 · (0.1)4 + 0.190151 · (0.1)2 ) +\n0.611352 · (0.0)4x+ 5.46393 · (0.0)2x+ 0.000798008x− 11.927 · (0.1)4 − 0.000392287 · (0.1)2 = − 1.2227x5 + 23.7796x4 + (20 + 13.0182)x3 + 2.37355x2 − 0.0182084x− 0.000194074 > − 1.2227x5 + 24.7796x4 + 13.0182x3 + 2.37355x2 − 0.0182084x− 0.000194074 > 13.0182x3 + 2.37355x2 − 0.0182084x− 0.000194074 > 0 .\nWe used 24.7796 · (20)4 − 1.2227 · (20)5 = 52090.9 > 0 and x 6 20. We have proofed the last inequality > 0 of Eq. (248).\nConsequently the derivative is always positive independent of y, thus\ne (x+y)2 2x erfc ( x+ y√\n2 √ x\n) − 2e (2x+y)2 2x erfc ( 2x+ y√\n2 √ x\n) (255)\nis strictly monotonically increasing in x.\nThe main subfunction is smaller than zero. Next we show that the sub-function Eq. (101) is smaller than zero. We consider the limit:\nlim x→∞\ne (x+y)2 2x erfc ( x+ y√\n2 √ x\n) − 2e (2x+y)2 2x erfc ( 2x+ y√\n2 √ x\n) = 0 (256)\nThe limit follows from Lemma 22. Since the function is monotonic increasing in x, it has to approach 0 from below. Thus,\ne (x+y)2 2x erfc ( x+ y√\n2 √ x\n) − 2e (2x+y)2 2x erfc ( 2x+ y√\n2 √ x\n) (257)\nis smaller than zero.\nBehavior of the main subfunction with respect to y at minimal x. We now consider the derivative of sub-function Eq. (101) with respect to y. We proofed that sub-function Eq. (101) is strictly monotonically increasing independent of y. In the proof of Theorem 16, we need the minimum of sub-function Eq. (101). Therefore we are only interested in the derivative of sub-function Eq. (101) with respect to y for the minimum x = 12/10 = 1.2\nConsequently, we insert the minimum x = 12/10 = 1.2 into the sub-function Eq. (101). The main terms become\nx+ y√ 2 √ x = y + 1.2√ 2 √ 1.2 = y√ 2 √ 1.2 + √ 1.2√ 2 = 5y + 6 2 √ 15 (258)\nand 2x+ y√\n2 √ x\n= y + 1.2 · 2√\n2 √ 1.2 = y√ 2 √ 1.2 + √ 1.2 √ 2 = 5y + 12 2 √ 15 . (259)\nSub-function Eq. (101) becomes:\ne\n( y\n√ 2 √\n12 10\n+ √ 12 10√ 2 )2 erfc  y√ 2 √\n12 10\n+ √ 12 10√ 2 − 2e ( y √ 2 √ 12 10 + √ 2 √ 12 10 )2 erfc  y√ 2 √\n12 10\n+ √ 2\n√ 12\n10  . (260)\nThe derivative of this function with respect to y is √ 15π ( e 1 60 (5y+6) 2 (5y + 6) erfc ( 5y+6\n2 √ 15\n) − 2e 160 (5y+12)2(5y + 12) erfc ( 5y+12\n2 √ 15\n)) + 30\n6 √ 15π . (261)\nWe again will use the approximation of Ren and MacKenzie [30]\nez 2 erfc(z) = 2.911\n√ π(2.911− 1)z + √ πz2 + 2.9112 . (262)\nTherefore we first perform an error analysis. We estimated the maximum and minimum of\n√ 15π  2 · 2.911(5y + 12)√ π(2.911−1)(5y+12)\n2 √ 15 +\n√ π ( 5y+12\n2 √ 15\n)2 + 2.9112 − 2.911(5y + 6) √ π(2.911−1)(5y+6)\n2 √ 15 +\n√ π ( 5y+6\n2 √ 15\n)2 + 2.9112 + 30 + (263)\n√ 15π ( e 1 60 (5y+6) 2 (5y + 6) erfc ( 5y + 6\n2 √ 15\n) − 2e 160 (5y+12) 2 (5y + 12) erfc ( 5y + 12\n2 √ 15\n)) + 30 .\nWe obtained for the maximal absolute error the value 0.163052. We added an approximation error of 0.2 to the approximation of the derivative. Since we want to show that the approximation upper bounds the true expression, the addition of the approximation error is required here. We get a sequence of inequalities:\n√ 15π ( e 1 60 (5y+6) 2 (5y + 6) erfc ( 5y + 6\n2 √ 15\n) − 2e 160 (5y+12) 2 (5y + 12) erfc ( 5y + 12\n2 √ 15\n)) + 30 6\n(264)\n√ 15π  2.911(5y + 6)√ π(2.911−1)(5y+6)\n2 √ 15 +\n√ π ( 5y+6\n2 √ 15\n)2 + 2.9112 − 2 · 2.911(5y + 12) √ π(2.911−1)(5y+12)\n2 √ 15 +\n√ π ( 5y+12\n2 √ 15\n)2 + 2.9112 + 30 + 0.2 =\n(30 · 2.911)(5y + 6) (2.911− 1)(5y + 6) + √ (5y + 6)2 + ( 2 √\n15·2.911√ π\n)2 − 2(30 · 2.911)(5y + 12) (2.911− 1)(5y + 12) + √ (5y + 12)2 + ( 2 √\n15·2.911√ π )2 + 30 + 0.2 =(0.2 + 30) (2.911− 1)(5y + 12) + √√√√(5y + 12)2 +(2√15 · 2.911√\nπ )2 (2.911− 1)(5y + 6) + √√√√(5y + 6)2 +(2√15 · 2.911√ π\n)2− 2 · 30 · 2.911(5y + 12) (2.911− 1)(5y + 6) + √√√√(5y + 6)2 +(2√15 · 2.911√\nπ )2+ 2.911 · 30(5y + 6) (2.911− 1)(5y + 12) + √√√√(5y + 12)2 +(2√15 · 2.911√\nπ\n)2 \n (2.911− 1)(5y + 6) + √√√√(5y + 6)2 +(2√15 · 2.911√ π )2 (2.911− 1)(5y + 12) + √√√√(5y + 12)2 +(2√15 · 2.911√ π )2  −1 < 0 .\nWe explain this sequence of inequalities.\n• First inequality: The approximation of Ren and MacKenzie [30] and then adding the error bound to ensure that the approximation is larger than the true value. • First equality: The factor 2 √ 15 and 2 √ π are factored out and canceled.\n• Second equality: Bringing all terms to the denominator(2.911− 1)(5y + 6) + √√√√(5y + 6)2 +(2√152.911√\nπ )2 (265) (2.911− 1)(5y + 12) + √√√√(5y + 12)2 +(2√15 · 2.911√ π\n)2 . • Last inequality < 0 is proofed in the following sequence of inequalities.\nWe look at the numerator of the last term in Eq. (264). We have to proof that this numerator is smaller than zero in order to proof the last inequality of Eq. (264). The numerator is\n(0.2 + 30) (2.911− 1)(5y + 12) + √√√√(5y + 12)2 +(2√15 · 2.911√\nπ )2 (266) (2.911− 1)(5y + 6) + √√√√(5y + 6)2 +(2√15 · 2.911√ π\n)2 − 2 · 30 · 2.911(5y + 12) (2.911− 1)(5y + 6) + √√√√(5y + 6)2 +(2√15 · 2.911√\nπ )2+ 2.911 · 30(5y + 6) (2.911− 1)(5y + 12) + √√√√(5y + 12)2 +(2√15 .2.911√\nπ )2 . We now compute upper bounds for this numerator:\n(0.2 + 30) (2.911− 1)(5y + 12) + √√√√(5y + 12)2 +(2√15 · 2.911√\nπ )2 (267) (2.911− 1)(5y + 6) + √√√√(5y + 6)2 +(2√15 · 2.911√ π )2−\n2 · 30 · 2.911(5y + 12) (2.911− 1)(5y + 6) + √√√√(5y + 6)2 +(2√15 · 2.911√\nπ )2+ 2.911 · 30(5y + 6) (2.911− 1)(5y + 12) + √√√√(5y + 12)2 +(2√15 · 2.911√\nπ )2 = − 1414.99y2 − 584.739 √ (5y + 6)2 + 161.84y + 725.211 √ (5y + 12)2 + 161.84y−\n5093.97y − 1403.37 √ (5y + 6)2 + 161.84 + 30.2 √ (5y + 6)2 + 161.84 √ (5y + 12)2 + 161.84+\n870.253 √ (5y + 12)2 + 161.84− 4075.17 <\n− 1414.99y2 − 584.739 √ (5y + 6)2 + 161.84y + 725.211 √ (5y + 12)2 + 161.84y−\n5093.97y − 1403.37 √ (6 + 5 · (−0.1))2 + 161.84 + 30.2 √ (6 + 5 · 0.1)2 + 161.84 √ (12 + 5 · 0.1)2 + 161.84+\n870.253 √ (12 + 5 · 0.1)2 + 161.84− 4075.17 =\n− 1414.99y2 − 584.739 √ (5y + 6)2 + 161.84y + 725.211 √ (5y + 12)2 + 161.84y − 5093.97y − 309.691 <\ny ( −584.739 √ (5y + 6)2 + 161.84 + 725.211 √ (5y + 12)2 + 161.84− 5093.97 ) − 309.691 <\n− 0.1 ( 725.211 √ (12 + 5 · (−0.1))2 + 161.84− 584.739 √ (6 + 5 · 0.1)2 + 161.84− 5093.97 ) − 309.691 =\n− 208.604 .\nFor the first inequality we choose y in the roots, so that positive terms maximally increase and negative terms maximally decrease. The second inequality just removed the y2 term which is always negative, therefore increased the expression. For the last inequality, the term in brackets is negative for all settings of y. Therefore we make the brackets as negative as possible and make the whole term positive by multiplying with y = −0.1. Consequently\ne (x+y)2 2x erfc ( x+ y√\n2 √ x\n) − 2e (2x+y)2 2x erfc ( 2x+ y√\n2 √ x\n) (268)\nis strictly monotonically decreasing in y for the minimal x = 1.2.\nLemma 45 (Main subfunction below). For 0.007 6 x 6 0.875 and −0.01 6 y 6 0.01, the function\ne (x+y)2 2x erfc ( x+ y√\n2 √ x\n) − 2e (2x+y)2 2x erfc ( 2x+ y√\n2 √ x\n) (269)\nsmaller than zero, is strictly monotonically increasing in x and strictly monotonically increasing in y for the minimal x = 0.007 = 0.00875 · 0.8, x = 0.56 = 0.7 · 0.8, x = 0.128 = 0.16 · 0.8, and x = 0.216 = 0.24 · 0.9 (lower bound of 0.9 on τ ).\nProof. We first consider the derivative of sub-function Eq. (111) with respect to x. The derivative of the function\ne (x+y)2 2x erfc ( x+ y√\n2 √ x\n) − 2e (2x+y)2 2x erfc ( 2x+ y√\n2 √ x\n) (270)\nwith respect to x is √ π ( e (x+y)2 2x (x− y)(x+ y) erfc ( x+y√ 2 √ x ) − 2e (2x+y)2 2x ( 4x2 − y2 ) erfc ( 2x+y√ 2 √ x )) + √ 2 √ x(3x− y)\n2 √ πx2\n=\n(271) √ π ( e (x+y)2 2x (x− y)(x+ y) erfc ( x+y√ 2 √ x ) − 2e (2x+y)2 2x (2x+ y)(2x− y) erfc ( 2x+y√ 2 √ x )) + √ 2 √ x(3x− y)\n2 √ πx2\n=\n√ π  e (x+y)22x (x−y)(x+y) erfc( x+y√2√x)√ 2 √ x − 2e (2x+y)2 2x (2x+y)(2x−y) erfc ( 2x+y√ 2 √ x ) √ 2 √ x + (3x− y) √\n22 √ π √ xx2\n.\nWe consider the numerator\n√ π e (x+y) 2 2x (x− y)(x+ y) erfc ( x+y√ 2 √ x ) √\n2 √ x\n− 2e\n(2x+y)2 2x (2x+ y)(2x− y) erfc (\n2x+y√ 2 √ x ) √\n2 √ x\n+ (3x− y) . (272)\nFor bounding this value, we use the approximation\nez 2 erfc(z) ≈ 2.911√ π(2.911− 1)z + √ πz2 + 2.9112 . (273)\nfrom Ren and MacKenzie [30]. We start with an error analysis of this approximation. According to Ren and MacKenzie [30] (Figure 1), the approximation error is both positive and negative in the range [0.175, 1.33]. This range contains all possible arguments of erfc that we consider in this subsection. Numerically we maximized and minimized the approximation error of the whole expression\nE(x, y) = e (x+y) 2 2x (x− y)(x+ y) erfc ( x+y√ 2 √ x ) √\n2 √ x\n− 2e\n(2x+y)2 2x (2x− y)(2x+ y) erfc (\n2x+y√ 2 √ x ) √\n2 √ x  − (274) 2.911(x− y)(x+ y)(√ 2 √ x )(√π(2.911−1)(x+y)√\n2 √ x\n+ √ π ( x+y√ 2 √ x )2 + 2.9112 ) −\n2 · 2.911(2x− y)(2x+ y)(√ 2 √ x )(√π(2.911−1)(2x+y)√\n2 √ x\n+ √ π (\n2x+y√ 2 √ x\n)2 + 2.9112\n)  .\nWe numerically determined −0.000228141 6 E(x, y) 6 0.00495688 for 0.08 6 x 6 0.875 and −0.01 6 y 6 0.01. We used different numerical optimization techniques like gradient based constraint BFGS algorithms and non-gradient-based Nelder-Mead methods with different start points. Therefore our approximation is smaller than the function that we approximate.\nWe use an error gap of −0.0003 to countermand the error due to the approximation. We have the sequences of inequalities using the approximation of Ren and MacKenzie [30]:\n(3x− y) + e (x+y) 2 2x (x− y)(x+ y) erfc ( x+y√ 2 √ x ) √\n2 √ x\n− 2e\n(2x+y)2 2x (2x− y)(2x+ y) erfc (\n2x+y√ 2 √ x ) √\n2 √ x\n√π > (275)\n(3x− y) +  2.911(x− y)(x+ y)(√ π ( x+y√ 2 √ x )2 + 2.9112 + (2.911−1) √ π(x+y)√ 2 √ x )(√ 2 √ x ) −\n2(2x− y)(2x+ y)2.911(√ 2 √ x )(√ π (\n2x+y√ 2 √ x\n)2 + 2.9112 + (2.911−1) √ π(2x+y)√\n2 √ x\n) √π − 0.0003 =\n(3x− y) +  (√2√x2.911) (x− y)(x+ y)(√ π(x+ y)2 + 2 · 2.9112x+ (2.911− 1)(x+ y) √ π ) (√ 2 √ x ) −\n2(2x− y)(2x+ y) (√ 2 √ x2.911 )(√ 2 √ x ) (√ π(2x+ y)2 + 2 · 2.9112x+ (2.911− 1)(2x+ y) √ π ) √π − 0.0003 =\n(3x− y) + 2.911  (x− y)(x+ y) (2.911− 1)(x+ y) + √ (x+ y)2 + 2·2.911\n2x π\n−\n2(2x− y)(2x+ y) (2.911− 1)(2x+ y) + √ (2x+ y)2 + 2·2.911 2x\nπ − 0.0003 > (3x− y) + 2.911\n (x− y)(x+ y) (2.911− 1)(x+ y) + √( 2.9112\nπ\n)2 + (x+ y)2 + 2·2.911\n2x π + 2·2.9112y π\n−\n2(2x− y)(2x+ y) (2.911− 1)(2x+ y) + √ (2x+ y)2 + 2·2.911 2x\nπ − 0.0003 = (3x− y) + 2.911\n (x− y)(x+ y) (2.911− 1)(x+ y) + √( x+ y + 2.911 2\nπ )2 − 2(2x− y)(2x+ y)\n(2.911− 1)(2x+ y) + √ (2x+ y)2 + 2·2.911 2x\nπ − 0.0003 = (3x− y) + 2.911  (x− y)(x+ y) 2.911(x+ y) + 2.911 2\nπ\n− 2(2x− y)(2x+ y) (2.911− 1)(2x+ y) + √ (2x+ y)2 + 2·2.911 2x\nπ − 0.0003 = (3x− y) + (x− y)(x+ y)\n(x+ y) + 2.911π − 2(2x− y)(2x+ y)2.911 (2.911− 1)(2x+ y) + √ (2x+ y)2 + 2·2.911 2x\nπ\n− 0.0003 =\n(3x− y) + (x− y)(x+ y) (x+ y) + 2.911π − 2(2x− y)(2x+ y)2.911 (2.911− 1)(2x+ y) + √ (2x+ y)2 + 2·2.911 2x\nπ\n− 0.0003 =\n( −2(2x− y)2.911 ( (x+ y) + 2.911\nπ ) (2x+ y) +(\n(x+ y) + 2.911\nπ\n) (3x− y − 0.0003) ( (2.911− 1)(2x+ y) + √ (2x+ y)2 +\n2 · 2.9112x π\n) +\n(x− y)(x+ y) ( (2.911− 1)(2x+ y) + √ (2x+ y)2 +\n2 · 2.9112x π )) ((\n(x+ y) + 2.911\nπ\n)( (2.911− 1)(2x+ y) + √ (2x+ y)2 +\n2 · 2.9112x π\n))−1 =\n( −8x3 − 8x2y + 4x2 √ (2x+ y)2 + 5.39467x− 10.9554x2 + 2xy2 − 2y2 √ (2x+ y)2 + 5.39467x +\n1.76901xy + 2xy √ (2x+ y)2 + 5.39467x+ 2.7795x √ (2x+ y)2 + 5.39467x −\n0.9269y √ (2x+ y)2 + 5.39467x− 0.00027798 √\n(2x+ y)2 + 5.39467x− 0.00106244x + 2y3 + 3.62336y2 − 0.00053122y )(( (x+ y) + 2.911\nπ\n)( (2.911− 1)(2x+ y) + √ (2x+ y)2 +\n2 · 2.9112x π ))−1 =(\n−8x3 + ( 4x2 + 2xy + 2.7795x− 2y2 − 0.9269y − 0.00027798 )√ (2x+ y)2 + 5.39467x −\n8x2y − 10.9554x2 + 2xy2 + 1.76901xy − 0.00106244x+ 2y3 + 3.62336y2 − 0.00053122y )((\n(x+ y) + 2.911\nπ\n)( (2.911− 1)(2x+ y) + √ (2x+ y)2 +\n2 · 2.9112x π\n))−1 > 0 .\nWe explain this sequence of inequalities:\n• First inequality: The approximation of Ren and MacKenzie [30] and then subtracting an error gap of 0.0003. • Equalities: The factor √ 2 √ x is factored out and canceled.\n• Second inequality: adds a positive term in the first root to obtain a binomial form. The term containing the root is positive and the root is in the denominator, therefore the whole term becomes smaller.\n• Equalities: solve for the term and factor out. • Bringing all terms to the denominator ( (x+ y) + 2.911π )( (2.911− 1)(2x+ y) + √ (2x+ y)2 + 2·2.911 2x π ) .\n• Equalities: Multiplying out and expanding terms.\n• Last inequality > 0 is proofed in the following sequence of inequalities.\nWe look at the numerator of the last expression of Eq. (275), which we show to be positive in order to show > 0 in Eq. (275). The numerator is\n− 8x3 + ( 4x2 + 2xy + 2.7795x− 2y2 − 0.9269y − 0.00027798 )√ (2x+ y)2 + 5.39467x −\n(276)\n8x2y − 10.9554x2 + 2xy2 + 1.76901xy − 0.00106244x+ 2y3 + 3.62336y2 − 0.00053122y . The factor 4x2 + 2xy + 2.7795x− 2y2 − 0.9269y − 0.00027798 in front of the root is positive:\n4x2 + 2xy + 2.7795x− 2y2 − 0.9269y − 0.00027798 > (277) −2y2 + 0.007 · 2y − 0.9269y + 4 · 0.0072 + 2.7795 · 0.007− 0.00027798 =\n−2y2 − 0.9129y + 2.77942 = −2(y + 1.42897)(y − 0.972523) > 0 . If the term that does not contain the root would be positive, then everything is positive and we have proofed the the numerator is positive. Therefore we consider the case that the term that does not contain the root is negative. The term that contains the root must be larger than the other term in absolute values. − ( −8x3 − 8x2y − 10.9554x2 + 2xy2 + 1.76901xy − 0.00106244x+ 2y3 + 3.62336y2 − 0.00053122y ) < (278)( 4x2 + 2xy + 2.7795x− 2y2 − 0.9269y − 0.00027798 )√ (2x+ y)2 + 5.39467x .\nTherefore the squares of the root term have to be larger than the square of the other term to show > 0 in Eq. (275). Thus, we have the inequality:( −8x3 − 8x2y − 10.9554x2 + 2xy2 + 1.76901xy − 0.00106244x+ 2y3 + 3.62336y2 − 0.00053122y )2 <\n(279)\n( 4x2 + 2xy + 2.7795x− 2y2 − 0.9269y − 0.00027798 )2 ( (2x+ y)2 + 5.39467x ) .\nThis is equivalent to 0 < ( 4x2 + 2xy + 2.7795x− 2y2 − 0.9269y − 0.00027798 )2 ( (2x+ y)2 + 5.39467x ) − (280)( −8x3 − 8x2y − 10.9554x2 + 2xy2 + 1.76901xy − 0.00106244x+ 2y3 + 3.62336y2 − 0.00053122y )2 =\nx · 4.168614250 · 10−7 − y22.049216091 · 10−7 − 0.0279456x5+ 43.0875x4y + 30.8113x4 + 43.1084x3y2 + 68.989x3y + 41.6357x3 + 10.7928x2y3 − 13.1726x2y2− 27.8148x2y − 0.00833715x2 + 0.0139728xy4 + 5.47537xy3+ 4.65089xy2 + 0.00277916xy − 10.7858y5 − 12.2664y4 + 0.00436492y3 .\nWe obtain the inequalities:\nx · 4.168614250 · 10−7 − y22.049216091 · 10−7 − 0.0279456x5+ (281) 43.0875x4y + 30.8113x4 + 43.1084x3y2 + 68.989x3y + 41.6357x3 + 10.7928x2y3− 13.1726x2y2 − 27.8148x2y − 0.00833715x2+ 0.0139728xy4 + 5.47537xy3 + 4.65089xy2 + 0.00277916xy − 10.7858y5 − 12.2664y4 + 0.00436492y3 > x · 4.168614250 · 10−7 − (0.01)22.049216091 · 10−7 − 0.0279456x5+ 0.0 · 43.0875x4 + 30.8113x4 + 43.1084(0.0)2x3 + 0.0 · 68.989x3 + 41.6357x3+ 10.7928(0.0)3x2 − 13.1726(0.01)2x2 − 27.8148(0.01)x2 − 0.00833715x2+ 0.0139728(0.0)4x+ 5.47537(0.0)3x+ 4.65089(0.0)2x+\n0.0 · 0.00277916x− 10.7858(0.01)5 − 12.2664(0.01)4 + 0.00436492(0.0)3 = x · 4.168614250 · 10−7 − 1.237626189 · 10−7 − 0.0279456x5 + 30.8113x4 + 41.6357x3 − 0.287802x2 >\n− ( x\n0.007\n)3 1.237626189 · 10−7 + 30.8113x4 − (0.875) · 0.0279456x4 + 41.6357x3 − (0.287802x)x 2\n0.007 =\n30.7869x4 + 0.160295x3 > 0 .\nWe used x > 0.007 and x 6 0.875 (reducing the negative x4-term to a x3-term). We have proofed the last inequality > 0 of Eq. (275).\nConsequently the derivative is always positive independent of y, thus\ne (x+y)2 2x erfc ( x+ y√\n2 √ x\n) − 2e (2x+y)2 2x erfc ( 2x+ y√\n2 √ x\n) (282)\nis strictly monotonically increasing in x.\nNext we show that the sub-function Eq. (111) is smaller than zero. We consider the limit:\nlim x→∞\ne (x+y)2 2x erfc ( x+ y√\n2 √ x\n) − 2e (2x+y)2 2x erfc ( 2x+ y√\n2 √ x\n) = 0 (283)\nThe limit follows from Lemma 22. Since the function is monotonic increasing in x, it has to approach 0 from below. Thus,\ne (x+y)2 2x erfc ( x+ y√\n2 √ x\n) − 2e (2x+y)2 2x erfc ( 2x+ y√\n2 √ x\n) (284)\nis smaller than zero.\nWe now consider the derivative of sub-function Eq. (111) with respect to y. We proofed that subfunction Eq. (111) is strictly monotonically increasing independent of y. In the proof of Theorem 3, we need the minimum of sub-function Eq. (111). First, we are interested in the derivative of subfunction Eq. (111) with respect to y for the minimum x = 0.007 = 7/1000.\nConsequently, we insert the minimum x = 0.007 = 7/1000 into the sub-function Eq. (111):\ne\n( y\n√ 2 √\n7 1000\n+\n√ 7\n1000√ 2 )2 erfc  y√ 2 √\n7 1000\n+\n√ 7\n1000√ 2 − (285) 2e ( y √ 2 √ 7 1000 + √ 2 √ 7 1000 )2 erfc\n y√ 2 √\n7 1000\n+ √ 2\n√ 7\n1000  = e 500y2 7 +y+ 7 2000 erfc ( 1000y + 7\n20 √ 35\n) − 2e (500y+7)2 3500 erfc ( 500y + 7\n10 √ 35\n) .\nThe derivative of this function with respect to y is( 1000y\n7 + 1\n) e 500y2 7 +y+ 7 2000 erfc ( 1000y + 7\n20 √ 35\n) − (286)\n1 7 4e (500y+7)2 3500 (500y + 7) erfc\n( 500y + 7\n10 √ 35\n) + 20 √ 5\n7π >(\n1 + 1000 · (−0.01)\n7\n) e−0.01+ 7 2000 + 500·(−0.01)2 7 erfc ( 7 + 1000 + (−0.01)\n20 √ 35\n) −\n1 7 4e (7+500·0.01)2 3500 (7 + 500 · 0.01) erfc\n( 7 + 500 · 0.01\n10 √ 35\n) + 20 √ 5\n7π > 3.56 .\nFor the first inequality, we use Lemma 24. Lemma 24 says that the function xex 2\nerfc(x) has the sign of x and is monotonically increasing to 1√\nπ . Consequently, we inserted the maximal y = 0.01 to\nmake the negative term more negative and the minimal y = −0.01 to make the positive term less positive.\nConsequently\ne (x+y)2 2x erfc ( x+ y√\n2 √ x\n) − 2e (2x+y)2 2x erfc ( 2x+ y√\n2 √ x\n) (287)\nis strictly monotonically increasing in y for the minimal x = 0.007.\nNext, we consider x = 0.7 · 0.8 = 0.56, which is the maximal ν = 0.7 and minimal τ = 0.8. We insert the minimum x = 0.56 = 56/100 into the sub-function Eq. (111):\ne\n( y\n√ 2 √\n56 100\n+ √ 56 100√ 2 )2 erfc  y√ 2 √\n56 100\n+ √ 56 100√ 2 − (288) 2e ( y √ 2 √ 56 100 + √ 2 √ 56 100 )2 erfc\n y√ 2 √\n56 100\n+ √ 2\n√ 56\n100  . The derivative with respect to y is:\n5e\n( 5y\n2 √ 7 + √ 7 5 )2 ( 5y\n2 √ 7 + √ 7 5\n) erfc ( 5y\n2 √ 7 + √ 7 5 ) √\n7 − (289)\n10e\n( 5y\n2 √ 7 + 2 √ 7 5 )2 ( 5y\n2 √ 7 + 2 √ 7 5\n) erfc ( 5y\n2 √ 7 + 2 √ 7 5 ) √\n7 + 5√ 7π >\n5e\n(√ 7\n5 − 0.01·5 2 √ 7 )2 (√ 7\n5 − 0.01·5 2 √ 7\n) erfc (√ 7\n5 − 0.01·5 2 √ 7 ) √\n7 −\n10e\n( 2 √\n7 5 + 0.01·5 2 √ 7 )2 ( 2 √\n7 5 + 0.01·5 2 √ 7\n) erfc ( 2 √\n7 5 + 0.01·5 2 √ 7 ) √\n7 + 5√ 7π > 0.00746 .\nFor the first inequality we applied Lemma 24 which states that the function xex 2\nerfc(x) is monotonically increasing. Consequently, we inserted the maximal y = 0.01 to make the negative term more negative and the minimal y = −0.01 to make the positive term less positive. Consequently\ne (x+y)2 2x erfc ( x+ y√\n2 √ x\n) − 2e (2x+y)2 2x erfc ( 2x+ y√\n2 √ x\n) (290)\nis strictly monotonically increasing in y for x = 0.56.\nNext, we consider x = 0.16 · 0.8 = 0.128, which is the minimal τ = 0.8. We insert the minimum x = 0.128 = 128/1000 into the sub-function Eq. (111):\ne\n( y\n√ 2 √\n128 1000\n+ √ 128 1000√ 2 )2 erfc  y√ 2 √\n128 1000\n+ √ 128 1000√ 2 − (291) 2e ( y √ 2 √ 128 1000 + √ 2 √ 128 1000 )2 erfc\n y√ 2 √\n128 1000\n+ √ 2\n√ 128\n1000  = e 125y2 32 +y+ 8 125 erfc ( 125y + 16\n20 √ 10\n) − 2e (125y+32)2 4000 erfc ( 125y + 32\n20 √ 10\n) .\nThe derivative with respect to y is:\n1\n16\n( e 125y2 32 +y+ 8 125 (125y + 16) erfc ( 125y + 16\n20 √ 10\n) − (292)\n2e (125y+32)2 4000 (125y + 32) erfc\n( 125y + 32\n20 √ 10\n) + 20 √ 10\nπ\n) >\n1\n16\n( (16 + 125(−0.01))e−0.01+ 8125 + 125(−0.01)2 32 erfc ( 16 + 125(−0.01)\n20 √ 10\n) −\n2e (32+1250.01)2 4000 (32 + 1250.01) erfc\n( 32 + 1250.01\n20 √ 10\n) + 20 √ 10\nπ\n) > 0.4468 .\nFor the first inequality we applied Lemma 24 which states that the function xex 2\nerfc(x) is monotonically increasing. Consequently, we inserted the maximal y = 0.01 to make the negative term more negative and the minimal y = −0.01 to make the positive term less positive. Consequently\ne (x+y)2 2x erfc ( x+ y√\n2 √ x\n) − 2e (2x+y)2 2x erfc ( 2x+ y√\n2 √ x\n) (293)\nis strictly monotonically increasing in y for x = 0.128.\nNext, we consider x = 0.24 · 0.9 = 0.216, which is the minimal τ = 0.9 (here we consider 0.9 as lower bound for τ ). We insert the minimum x = 0.216 = 216/1000 into the sub-function Eq. (111):\ne\n( y\n√ 2 √\n216 1000\n+ √ 216 1000√ 2 )2 erfc  y√ 2 √\n216 1000\n+ √ 216 1000√ 2 − (294) 2e ( y √ 2 √ 216 1000 + √ 2 √ 216 1000 )2 erfc\n y√ 2 √\n216 1000\n+ √ 2\n√ 216\n1000\n =\ne (125y+27)2 6750 erfc\n( 125y + 27\n15 √ 30\n) − 2e (125y+54)2 6750 erfc ( 125y + 54\n15 √ 30 ) The derivative with respect to y is:\n1\n27\n( e (125y+27)2 6750 (125y + 27) erfc ( 125y + 27\n15 √ 30\n) − (295)\n2e (125y+54)2 6750 (125y + 54) erfc\n( 125y + 54\n15 √ 30\n) + 15 √ 30\nπ\n) >\n1\n27\n( (27 + 125(−0.01))e (27+125(−0.01))2 6750 erfc ( 27 + 125(−0.01)\n15 √ 30\n) −\n2e (54+1250.01)2 6750 (54 + 1250.01) erfc\n( 54 + 1250.01\n15 √ 30\n) + 15 √ 30\nπ\n) ) > 0.211288 .\nFor the first inequality we applied Lemma 24 which states that the function xex 2\nerfc(x) is monotonically increasing. Consequently, we inserted the maximal y = 0.01 to make the negative term more negative and the minimal y = −0.01 to make the positive term less positive. Consequently\ne (x+y)2 2x erfc ( x+ y√\n2 √ x\n) − 2e (2x+y)2 2x erfc ( 2x+ y√\n2 √ x\n) (296)\nis strictly monotonically increasing in y for x = 0.216.\nLemma 46 (Monotone Derivative). For λ = λ01, α = α01 and the domain −0.1 6 µ 6 0.1, −0.1 6 ω 6 0.1, 0.00875 6 ν 6 0.7, and 0.8 6 τ 6 1.25. We are interested of the derivative of\nτ ( e ( µω+ντ√ 2 √ ντ )2 erfc ( µω + ντ√\n2 √ ντ\n) − 2e ( µω+2·ντ√ 2 √ ντ )2 erfc ( µω + 2 · ντ√\n2 √ ντ\n)) . (297)\nThe derivative of the equation above with respect to\n• ν is larger than zero;\n• τ is smaller than zero for maximal ν = 0.7, ν = 0.16, and ν = 0.24 (with 0.9 6 τ );\n• y = µω is larger than zero for ντ = 0.00875 · 0.8 = 0.007, ντ = 0.7 · 0.8 = 0.56, ντ = 0.16 · 0.8 = 0.128, and ντ = 0.24 · 0.9 = 0.216.\nProof. We consider the domain: −0.1 6 µ 6 0.1, −0.1 6 ω 6 0.1, 0.00875 6 ν 6 0.7, and 0.8 6 τ 6 1.25.\nWe use Lemma 17 to determine the derivatives. Consequently, the derivative of\nτ ( e ( µω+ντ√ 2 √ ντ )2 erfc ( µω + ντ√\n2 √ ντ\n) − 2e ( µω+2ντ√ 2 √ ντ )2 erfc ( µω + 2ντ√\n2 √ ντ\n)) (298)\nwith respect to ν is larger than zero, which follows directly from Lemma 17 using the chain rule.\nConsequently, the derivative of\nτ ( e ( µω+ντ√ 2 √ ντ )2 erfc ( µω + ντ√\n2 √ ντ\n) − 2e ( µω+2ντ√ 2 √ ντ )2 erfc ( µω + 2ντ√\n2 √ ντ\n)) (299)\nwith respect to y = µω is larger than zero for ντ = 0.00875 · 0.8 = 0.007, ντ = 0.7 · 0.8 = 0.56, ντ = 0.16 · 0.8 = 0.128, and ντ = 0.24 · 0.9 = 0.216, which also follows directly from Lemma 17. We now consider the derivative with respect to τ , which is not trivial since τ is a factor of the whole expression. The sub-expression should be maximized as it appears with negative sign in the mapping for ν.\nFirst, we consider the function for the largest ν = 0.7 and the largest y = µω = 0.01 for determining the derivative with respect to τ .\nThe expression becomes\nτ e ( 7τ 10 + 1 100 √ 2 √ 7τ 10 )2 erfc  7τ10 + 1100√ 2 √\n7τ 10\n− 2e ( 2·7τ 10 + 1 100 √ 2 √ 7τ 10 )2 erfc  2·7τ10 + 1100√ 2 √\n7τ 10\n  . (300)\nThe derivative with respect to τ is(√ π ( e (70τ+1)2 14000τ (700τ(7τ + 20)− 1) erfc ( 70τ + 1\n20 √ 35 √ τ\n) − (301)\n2e (140τ+1)2 14000τ (2800τ(7τ + 5)− 1) erfc ( 140τ + 1\n20 √ 35 √ τ\n)) + 20 √ 35(210τ − 1) √ τ ) ( 14000 √ πτ )−1 .\nWe are considering only the numerator and use again the approximation of Ren and MacKenzie [30]. The error analysis on the whole numerator gives an approximation error 97 < E < 186. Therefore we add 200 to the numerator when we use the approximation Ren and MacKenzie [30]. We obtain the inequalities:\n√ π ( e (70τ+1)2 14000τ (700τ(7τ + 20)− 1) erfc ( 70τ + 1\n20 √ 35 √ τ\n) − (302)\n2e (140τ+1)2 14000τ (2800τ(7τ + 5)− 1) erfc ( 140τ + 1\n20 √ 35 √ τ\n)) + 20 √ 35(210τ − 1) √ τ 6\n√ π  2.911(700τ(7τ + 20)− 1)√ π(2.911−1)(70τ+1)\n20 √ 35 √ τ\n+ √ π (\n70τ+1 20 √ 35 √ τ\n)2 + 2.9112 −\n2 · 2.911(2800τ(7τ + 5)− 1) √ π(2.911−1)(140τ+1)\n20 √ 35 √ τ\n+ √ π (\n140τ+1 20 √ 35 √ τ\n)2 + 2.9112  + 20 √ 35(210τ − 1) √ τ + 200 =\n√ π  (700τ(7τ + 20)− 1) (20 · √35 · 2.911√τ) √ π(2.911− 1)(70τ + 1) + √( 20 · 2.911 √ 35 √ τ )2 + π(70τ + 1)2 −\n2(2800τ(7τ + 5)− 1) ( 20 · √ 35 · 2.911 √ τ )\n√ π(2.911− 1)(140τ + 1) + √( 20 · √ 35 · 2.911 √ τ )2 + π(140τ + 1)2 + (\n20 √ 35(210τ − 1) √ τ + 200 ) =((\n20 √ 35(210τ − 1) √ τ + 200 )(√ π(2.911− 1)(70τ + 1) + √( 20 · √ 35 · 2.911 √ τ )2 + π(70τ + 1)2 ) ( √ π(2.911− 1)(140τ + 1) + √( 20 · √ 35 · 2.911 √ τ )2 + π(140τ + 1)2 ) +\n2.911 · 20 √ 35 √ π(700τ(7τ + 20)− 1) √ τ(\n√ π(2.911− 1)(140τ + 1) + √( 20 · √ 35 · 2.911 √ τ )2 + π(140τ + 1)2 ) −\n√ π2 · 20 · √ 35 · 2.911(2800τ(7τ + 5)− 1)\n√ τ ( √ π(2.911− 1)(70τ + 1) + √( 20 · √ 35 · 2.911 √ τ )2 + π(70τ + 1)2 )) (( √ π(2.911− 1)(70τ + 1) + √( 20 √ 35 · 2.911 · √ τ )2 + π(70τ + 1)2\n) ( √ π(2.911− 1)(140τ + 1) + √( 20 √ 35 · 2.911 · √ τ )2 + π(140τ + 1)2 ))−1 .\nAfter applying the approximation of Ren and MacKenzie [30] and adding 200, we first factored out 20 √ 35 √ τ . Then we brought all terms to the same denominator.\nWe now consider the numerator:( 20 √ 35(210τ − 1) √ τ + 200 )(√ π(2.911− 1)(70τ + 1) + √( 20 · √ 35 · 2.911 √ τ )2 + π(70τ + 1)2 ) (303)(\n√ π(2.911− 1)(140τ + 1) + √( 20 · √ 35 · 2.911 √ τ )2 + π(140τ + 1)2 ) +\n2.911 · 20 √ 35 √ π(700τ(7τ + 20)− 1) √ τ(\n√ π(2.911− 1)(140τ + 1) + √( 20 · √ 35 · 2.911 √ τ )2 + π(140τ + 1)2 ) −\n√ π2 · 20 · √ 35 · 2.911(2800τ(7τ + 5)− 1) √ τ(\n√ π(2.911− 1)(70τ + 1) + √( 20 · √ 35 · 2.911 √ τ )2 + π(70τ + 1)2 ) =\n− 1.70658× 107 √ π(70τ + 1)2 + 118635ττ3/2+\n4200 √ 35 √ π(70τ + 1)2 + 118635τ √ π(140τ + 1)2 + 118635ττ3/2 +\n8.60302× 106 √ π(140τ + 1)2 + 118635ττ3/2 − 2.89498× 107τ3/2 −\n1.21486× 107 √ π(70τ + 1)2 + 118635ττ5/2 + 8.8828× 106 √ π(140τ + 1)2 + 118635ττ5/2 −\n2.43651× 107τ5/2 − 1.46191× 109τ7/2 + 2.24868× 107τ2 + 94840.5 √ π(70τ + 1)2 + 118635ττ +\n47420.2 √ π(140τ + 1)2 + 118635ττ + 481860τ + 710.354 √ τ +\n820.213 √ τ √ π(70τ + 1)2 + 118635τ + 677.432 √ π(70τ + 1)2 + 118635τ −\n1011.27 √ τ √ π(140τ + 1)2 + 118635τ − 20 √ 35 √ τ √ π(70τ + 1)2 + 118635τ √ π(140τ + 1)2 + 118635τ +\n200 √ π(70τ + 1)2 + 118635τ √ π(140τ + 1)2 + 118635τ +\n677.432 √ π(140τ + 1)2 + 118635τ + 2294.57 =\n− 2.89498× 107τ3/2 − 2.43651× 107τ5/2 − 1.46191× 109τ7/2 +( −1.70658× 107τ3/2 − 1.21486× 107τ5/2 + 94840.5τ + 820.213 √ τ + 677.432 ) √ π(70τ + 1)2 + 118635τ +( 8.60302× 106τ3/2 + 8.8828× 106τ5/2 + 47420.2τ − 1011.27 √ τ + 677.432\n) √ π(140τ + 1)2 + 118635τ +( 4200 √ 35τ3/2 − 20 √ 35 √ τ + 200 )√ π(70τ + 1)2 + 118635τ √ π(140τ + 1)2 + 118635τ +\n2.24868× 107τ2 + 481860.τ + 710.354 √ τ + 2294.57 6\n− 2.89498× 107τ3/2 − 2.43651× 107τ5/2 − 1.46191× 109τ7/2+( −1.70658× 107τ3/2 − 1.21486× 107τ5/2 + 820.213 √ 1.25 + 1.25 · 94840.5 + 677.432 ) √ π(70τ + 1)2 + 118635τ+( 8.60302× 106τ3/2 + 8.8828× 106τ5/2 − 1011.27 √ 0.8 + 1.25 · 47420.2 + 677.432\n) √ π(140τ + 1)2 + 118635τ+( 4200 √ 35τ3/2 − 20 √ 35 √ τ + 200\n) √ π(70τ + 1)2 + 118635τ √ π(140τ + 1)2 + 118635τ+ 2.24868× 107τ2 + 710.354 √\n1.25 + 1.25 · 481860 + 2294.57 = − 2.89498× 107τ3/2 − 2.43651× 107τ5/2 − 1.46191× 109τ7/2+( −1.70658× 107τ3/2 − 1.21486× 107τ5/2 + 120145.\n)√ π(70τ + 1)2 + 118635τ+(\n8.60302× 106τ3/2 + 8.8828× 106τ5/2 + 59048.2 )√\nπ(140τ + 1)2 + 118635τ+( 4200 √ 35τ3/2 − 20 √ 35 √ τ + 200 )√ π(70τ + 1)2 + 118635τ √ π(140τ + 1)2 + 118635τ+\n2.24868× 107τ2 + 605413 = − 2.89498× 107τ3/2 − 2.43651× 107τ5/2 − 1.46191× 109τ7/2+( 8.60302× 106τ3/2 + 8.8828× 106τ5/2 + 59048.2\n)√ 19600π(τ + 1.94093)(τ + 0.0000262866)+(\n−1.70658× 107τ3/2 − 1.21486× 107τ5/2 + 120145. )√\n4900π(τ + 7.73521)(τ + 0.0000263835)+( 4200 √ 35τ3/2 − 20 √ 35 √ τ + 200 ) √\n19600π(τ + 1.94093)(τ + 0.0000262866) √ 4900π(τ + 7.73521)(τ + 0.0000263835)+\n2.24868× 107τ2 + 605413 6 − 2.89498× 107τ3/2 − 2.43651× 107τ5/2 − 1.46191× 109τ7/2+( 8.60302× 106τ3/2 + 8.8828× 106τ5/2 + 59048.2\n)√ 19600π(τ + 1.94093)τ+(\n−1.70658× 107τ3/2 − 1.21486× 107τ5/2 + 120145. )√\n4900π1.00003(τ + 7.73521)τ+( 4200 √ 35τ3/2 − 20 √ 35 √ τ + 200 )√ 19600π1.00003(τ + 1.94093)τ√\n4900π1.00003(τ + 7.73521)τ+\n2.24868× 107τ2 + 605413 = − 2.89498× 107τ3/2 − 2.43651× 107τ5/2 − 1.46191× 109τ7/2+( −3.64296× 106τ3/2 + 7.65021× 108τ5/2 + 6.15772× 106τ\n) √ τ + 1.94093\n√ τ + 7.73521 + 2.24868× 107τ2+(\n2.20425× 109τ3 + 2.13482× 109τ2 + 1.46527× 107 √ τ )√\nτ + 1.94093+( −1.5073× 109τ3 − 2.11738× 109τ2 + 1.49066× 107 √ τ )√\nτ + 7.73521 + 605413 6 √\n1.25 + 1.94093 √ 1.25 + 7.73521 ( −3.64296× 106τ3/2 + 7.65021× 108τ5/2 + 6.15772× 106τ ) +\n√ 1.25 + 1.94093 ( 2.20425× 109τ3 + 2.13482× 109τ2 + 1.46527× 107 √ τ )\n+ √ 0.8 + 7.73521 ( −1.5073× 109τ3 − 2.11738× 109τ2 + 1.49066× 107 √ τ ) −\n2.89498× 107τ3/2 − 2.43651× 107τ5/2 − 1.46191× 109τ7/2 + 2.24868× 107τ2 + 605413 = − 4.84561× 107τ3/2 + 4.07198× 109τ5/2 − 1.46191× 109τ7/2− 4.66103× 108τ3 − 2.34999× 109τ2+ 3.29718× 107τ + 6.97241× 107 √ τ + 605413 6\n605413τ3/2\n0.83/2 − 4.84561× 107τ3/2+\n4.07198× 109τ5/2 − 1.46191× 109τ7/2− 4.66103× 108τ3 − 2.34999× 109τ2 + 3.29718× 10 7 √ ττ√\n0.8 +\n6.97241× 107τ √ τ\n0.8 = τ3/2 ( −4.66103× 108τ3/2 − 1.46191× 109τ2 − 2.34999× 109 √ τ+\n4.07198× 109τ + 7.64087× 107 ) 6\nτ3/2 ( −4.66103× 108τ3/2 − 1.46191× 109τ2 + 7.64087× 10 7 √ τ√\n0.8 −\n2.34999× 109 √ τ + 4.07198× 109τ ) =\nτ2 ( −1.46191× 109τ3/2 + 4.07198× 109 √ τ − 4.66103× 108τ − 2.26457× 109 ) 6(\n−2.26457× 109 + 4.07198× 109 √ 0.8− 4.66103× 1080.8− 1.46191× 1090.83/2 ) τ2 =\n− 4.14199× 107τ2 < 0 .\nFirst we expanded the term (multiplied it out). The we put the terms multiplied by the same square root into brackets. The next inequality sign stems from inserting the maximal value of 1.25 for τ for some positive terms and value of 0.8 for negative terms. These terms are then expanded at the =-sign. The next equality factors the terms under the squared root. We decreased the negative term by setting τ = τ + 0.0000263835 under the root. We increased positive terms by setting τ + 0.000026286 = 1.00003τ and τ + 0.000026383 = 1.00003τ under the root for positive terms. The positive terms are increase, since 0.8+0.0000263830.8 = 1.00003, thus τ + 0.000026286 < τ + 0.000026383 6 1.00003τ . For the next inequality we decreased negative terms by inserting τ = 0.8 and increased positive terms by inserting τ = 1.25. The next equality expands the terms. We use upper bound of 1.25 and lower bound of 0.8 to obtain terms with corresponding exponents of τ .\nFor the last 6-sign we used the function\n−1.46191× 109τ3/2 + 4.07198× 109 √ τ − 4.66103× 108τ − 2.26457× 109 (304)\nThe derivative of this function is\n−2.19286× 109 √ τ + 2.03599× 109√ τ − 4.66103× 108 (305)\nand the second order derivative is\n−1.01799× 10 9 τ3/2 − 1.09643× 10 9 √ τ < 0 . (306)\nThe derivative at 0.8 is smaller than zero:\n− 2.19286× 109 √ 0.8− 4.66103× 108 + 2.03599× 10 9\n√ 0.8\n= (307)\n− 1.51154× 108 < 0 .\nSince the second order derivative is negative, the derivative decreases with increasing τ . Therefore the derivative is negative for all values of τ that we consider, that is, the function Eq. (304) is strictly monotonically decreasing. The maximum of the function Eq. (304) is therefore at 0.8. We inserted 0.8 to obtain the maximum.\nConsequently, the derivative of\nτ ( e ( µω+ντ√ 2 √ ντ )2 erfc ( µω + ντ√\n2 √ ντ\n) − 2e ( µω+2ντ√ 2 √ ντ )2 erfc ( µω + 2ντ√\n2 √ ντ\n)) (308)\nwith respect to τ is smaller than zero for maximal ν = 0.7.\nNext, we consider the function for the largest ν = 0.16 and the largest y = µω = 0.01 for determining the derivative with respect to τ .\nThe expression becomes\nτ e ( 16τ 100 + 1 100 √ 2 √ 16τ 100 )2 erfc  16τ100 + 1100√ 2 √\n16τ 100\n− e ( 2 16τ 100 + 1 100 √ 2 √ 16τ 100 )2 erfc  2 16τ100 + 1100√ 2 √\n16τ 100\n  . (309)\nThe derivative with respect to τ is(√ π ( e (16τ+1)2 3200τ (128τ(2τ + 25)− 1) erfc ( 16τ + 1\n40 √ 2 √ τ\n) − (310)\n2e (32τ+1)2 3200τ (128τ(8τ + 25)− 1) erfc ( 32τ + 1\n40 √ 2 √ τ\n)) + 40 √ 2(48τ − 1) √ τ ) ( 3200 √ πτ )−1 .\nWe are considering only the numerator and use again the approximation of Ren and MacKenzie [30]. The error analysis on the whole numerator gives an approximation error 1.1 < E < 12. Therefore we add 20 to the numerator when we use the approximation of Ren and MacKenzie [30]. We obtain the inequalities:\n√ π ( e (16τ+1)2 3200τ (128τ(2τ + 25)− 1) erfc ( 16τ + 1\n40 √ 2 √ τ\n) − (311)\n2e (32τ+1)2 3200τ (128τ(8τ + 25)− 1) erfc ( 32τ + 1\n40 √ 2 √ τ\n)) + 40 √ 2(48τ − 1) √ τ 6\n√ π  2.911(128τ(2τ + 25)− 1)√ π(2.911−1)(16τ+1)\n40 √ 2 √ τ\n+ √ π (\n16τ+1 40 √ 2 √ τ\n)2 + 2.9112 −\n2 · 2.911(128τ(8τ + 25)− 1) √ π(2.911−1)(32τ+1)\n40 √ 2 √ τ\n+ √ π (\n32τ+1 40 √ 2 √ τ\n)2 + 2.9112  + 40 √ 2(48τ − 1) √ τ + 20 =\n√ π  (128τ(2τ + 25)− 1) (40√22.911√τ) √ π(2.911− 1)(16τ + 1) + √( 40 √ 22.911 √ τ )2 + π(16τ + 1)2 −\n2(128τ(8τ + 25)− 1) ( 40 √ 22.911 √ τ )\n√ π(2.911− 1)(32τ + 1) + √( 40 √ 22.911 √ τ )2 + π(32τ + 1)2 + 40 √\n2(48τ − 1) √ τ + 20 =((\n40 √ 2(48τ − 1) √ τ + 20 )(√ π(2.911− 1)(16τ + 1) + √( 40 √ 22.911 √ τ )2 + π(16τ + 1)2 ) ( √ π(2.911− 1)(32τ + 1) + √( 40 √ 22.911 √ τ )2 + π(32τ + 1)2 ) + +\n2.911 · 40 √ 2 √ π(128τ(2τ + 25)− 1) √ τ(\n√ π(2.911− 1)(32τ + 1) + √( 40 √ 22.911 √ τ )2 + π(32τ + 1)2 ) −\n2 √ π40 √ 22.911(128τ(8τ + 25)− 1)\n√ τ ( √ π(2.911− 1)(16τ + 1) + √( 40 √ 22.911 √ τ )2 + π(16τ + 1)2 )) (( √ π(2.911− 1)(32τ + 1) + √( 40 √ 22.911 √ τ )2 + π(32τ + 1)2\n) ( √ π(2.911− 1)(32τ + 1) + √( 40 √ 22.911 √ τ )2 + π(32τ + 1)2 ))−1 .\nAfter applying the approximation of Ren and MacKenzie [30] and adding 20, we first factored out 40 √ 2 √ τ . Then we brought all terms to the same denominator.\nWe now consider the numerator:( 40 √ 2(48τ − 1) √ τ + 20 )(√ π(2.911− 1)(16τ + 1) + √( 40 √ 22.911 √ τ )2 + π(16τ + 1)2 ) (312)(\n√ π(2.911− 1)(32τ + 1) + √( 40 √ 22.911 √ τ )2 + π(32τ + 1)2 ) +\n2.911 · 40 √ 2 √ π(128τ(2τ + 25)− 1) √ τ(\n√ π(2.911− 1)(32τ + 1) + √( 40 √ 22.911 √ τ )2 + π(32τ + 1)2 ) −\n2 √ π40 √ 22.911(128τ(8τ + 25)− 1) √ τ(\n√ π(2.911− 1)(16τ + 1) + √( 40 √ 22.911 √ τ )2 + π(16τ + 1)2 ) =\n− 1.86491× 106 √ π(16τ + 1)2 + 27116.5ττ3/2+\n1920 √ 2 √ π(16τ + 1)2 + 27116.5τ √ π(32τ + 1)2 + 27116.5ττ3/2+\n940121 √ π(32τ + 1)2 + 27116.5ττ3/2 − 3.16357× 106τ3/2−\n303446 √ π(16τ + 1)2 + 27116.5ττ5/2 + 221873 √ π(32τ + 1)2 + 27116.5ττ5/2 − 608588τ5/2−\n8.34635× 106τ7/2 + 117482.τ2 + 2167.78 √ π(16τ + 1)2 + 27116.5ττ+\n1083.89 √ π(32τ + 1)2 + 27116.5ττ+ 11013.9τ + 339.614 √ τ + 392.137 √ τ √ π(16τ + 1)2 + 27116.5τ+\n67.7432 √ π(16τ + 1)2 + 27116.5τ − 483.478 √ τ √ π(32τ + 1)2 + 27116.5τ− 40 √ 2 √ τ √ π(16τ + 1)2 + 27116.5τ √ π(32τ + 1)2 + 27116.5τ+\n20 √ π(16τ + 1)2 + 27116.5τ √ π(32τ + 1)2 + 27116.5τ+\n67.7432 √ π(32τ + 1)2 + 27116.5τ + 229.457 =\n− 3.16357× 106τ3/2 − 608588τ5/2 − 8.34635× 106τ7/2+( −1.86491× 106τ3/2 − 303446τ5/2 + 2167.78τ + 392.137 √ τ + 67.7432 ) √ π(16τ + 1)2 + 27116.5τ+( 940121τ3/2 + 221873τ5/2 + 1083.89τ − 483.478 √ τ + 67.7432 )\n√ π(32τ + 1)2 + 27116.5τ+( 1920 √ 2τ3/2 − 40 √ 2 √ τ + 20 )√ π(16τ + 1)2 + 27116.5τ √ π(32τ + 1)2 + 27116.5τ+ 117482.τ2 + 11013.9τ + 339.614 √ τ + 229.457 6\n− 3.16357× 106τ3/2 − 608588τ5/2 − 8.34635× 106τ7/2+( −1.86491× 106τ3/2 − 303446τ5/2 + 392.137 √ 1.25 + 1.252167.78 + 67.7432 ) √ π(16τ + 1)2 + 27116.5τ+( 940121τ3/2 + 221873τ5/2 − 483.478 √ 0.8 + 1.251083.89 + 67.7432\n) √ π(32τ + 1)2 + 27116.5τ+( 1920 √ 2τ3/2 − 40 √ 2 √ τ + 20 )√ π(16τ + 1)2 + 27116.5τ √ π(32τ + 1)2 + 27116.5τ+ 117482.τ2 + 339.614 √ 1.25 + 1.2511013.9 + 229.457 =\n− 3.16357× 106τ3/2 − 608588τ5/2 − 8.34635× 106τ7/2+( −1.86491× 106τ3/2 − 303446τ5/2 + 3215.89 )√ π(16τ + 1)2 + 27116.5τ+(\n940121τ3/2 + 221873τ5/2 + 990.171 )√\nπ(32τ + 1)2 + 27116.5τ+( 1920 √ 2τ3/2 − 40 √ 2 √ τ + 20 )√ π(16τ + 1)2 + 27116.5τ √ π(32τ + 1)2 + 27116.5τ+\n117482τ2 + 14376.6 =\n− 3.16357× 106τ3/2 − 608588τ5/2 − 8.34635× 106τ7/2+( 940121τ3/2 + 221873τ5/2 + 990.171 )√ 1024π(τ + 8.49155)(τ + 0.000115004)+(\n−1.86491× 106τ3/2 − 303446τ5/2 + 3215.89 )√\n256π(τ + 33.8415)(τ + 0.000115428)+( 1920 √ 2τ3/2 − 40 √ 2 √ τ + 20 )√ 1024π(τ + 8.49155)(τ + 0.000115004)√\n256π(τ + 33.8415)(τ + 0.000115428)+\n117482.τ2 + 14376.6 6\n− 3.16357× 106τ3/2 − 608588τ5/2 − 8.34635× 106τ7/2+( 940121τ3/2 + 221873τ5/2 + 990.171 )√ 1024π1.00014(τ + 8.49155)τ+(\n1920 √ 2τ3/2 − 40 √ 2 √ τ + 20 )√ 256π1.00014(τ + 33.8415)τ √ 1024π1.00014(τ + 8.49155)τ+(\n−1.86491× 106τ3/2 − 303446τ5/2 + 3215.89 )√ 256π(τ + 33.8415)τ+\n117482.τ2 + 14376.6 =\n− 3.16357× 106τ3/2 − 608588τ5/2 − 8.34635× 106τ7/2+( −91003τ3/2 + 4.36814× 106τ5/2 + 32174.4τ )√ τ + 8.49155\n√ τ + 33.8415 + 117482.τ2+(\n1.25852× 107τ3 + 5.33261× 107τ2 + 56165.1 √ τ )√\nτ + 8.49155+( −8.60549× 106τ3 − 5.28876× 107τ2 + 91200.4 √ τ )√\nτ + 33.8415 + 14376.6 6 √\n1.25 + 8.49155 √ 1.25 + 33.8415 ( −91003τ3/2 + 4.36814× 106τ5/2 + 32174.4τ ) +\n√ 1.25 + 8.49155 ( 1.25852× 107τ3 + 5.33261× 107τ2 + 56165.1 √ τ )\n+ √ 0.8 + 33.8415 ( −8.60549× 106τ3 − 5.28876× 107τ2 + 91200.4 √ τ ) −\n3.16357× 106τ3/2 − 608588τ5/2 − 8.34635× 106τ7/2 + 117482.τ2 + 14376.6 =\n− 4.84613× 106τ3/2 + 8.01543× 107τ5/2 − 8.34635× 106τ7/2− 1.13691× 107τ3 − 1.44725× 108τ2+ 594875.τ + 712078. √ τ + 14376.6 6\n14376.6τ3/2\n0.83/2 − 4.84613× 106τ3/2+\n8.01543× 107τ5/2 − 8.34635× 106τ7/2− 1.13691× 107τ3 − 1.44725× 108τ2 + 594875. √ ττ√\n0.8 +\n712078.τ √ τ\n0.8 =\n− 3.1311 · 106τ3/2 − 1.44725 · 108τ2 + 8.01543 · 107τ5/2 − 1.13691 · 107τ3− 8.34635 · 106τ7/2 6\n− 3.1311× 106τ3/2 + 8.01543× 10 7 √\n1.25τ5/2√ τ −\n8.34635× 106τ7/2 − 1.13691× 107τ3 − 1.44725× 108τ2 = − 3.1311× 106τ3/2 − 8.34635× 106τ7/2 − 1.13691× 107τ3 − 5.51094× 107τ22 < 0 .\nFirst we expanded the term (multiplied it out). The we put the terms multiplied by the same square root into brackets. The next inequality sign stems from inserting the maximal value of 1.25 for τ for some positive terms and value of 0.8 for negative terms. These terms are then expanded at the =-sign. The next equality factors the terms under the squared root. We decreased the negative term by setting τ = τ + 0.00011542 under the root. We increased positive terms by setting τ + 0.00011542 = 1.00014τ and τ + 0.000115004 = 1.00014τ under the root for positive terms. The positive terms are increase, since 0.8+0.000115420.8 < 1.000142, thus τ + 0.000115004 < τ + 0.00011542 6 1.00014τ . For the next inequality we decreased negative terms by inserting τ = 0.8 and increased positive terms by inserting τ = 1.25. The next equality expands the terms. We use upper bound of 1.25 and lower bound of 0.8 to obtain terms with corresponding exponents of τ .\nConsequently, the derivative of\nτ ( e ( µω+ντ√ 2 √ ντ )2 erfc ( µω + ντ√\n2 √ ντ\n) − 2e ( µω+2ντ√ 2 √ ντ )2 erfc ( µω + 2ντ√\n2 √ ντ\n)) (313)\nwith respect to τ is smaller than zero for maximal ν = 0.16.\nNext, we consider the function for the largest ν = 0.24 and the largest y = µω = 0.01 for determining the derivative with respect to τ . However we assume 0.9 6 τ , in order to restrict the domain of τ .\nThe expression becomes\nτ e ( 24τ 100 + 1 100 √ 2 √ 24τ 100 )2 erfc  24τ100 + 1100√ 2 √\n24τ 100\n− e ( 2 24τ 100 + 1 100 √ 2 √ 24τ 100 )2 erfc  2 24τ100 + 1100√ 2 √\n24τ 100\n  . (314)\nThe derivative with respect to τ is(√ π ( e (24τ+1)2 4800τ (192τ(3τ + 25)− 1) erfc ( 24τ + 1\n40 √ 3 √ τ\n) − (315)\n2e (48τ+1)2 4800τ (192τ(12τ + 25)− 1) erfc ( 48τ + 1\n40 √ 3 √ τ\n)) + 40 √ 3(72τ − 1) √ τ ) ( 4800 √ πτ )−1 .\nWe are considering only the numerator and use again the approximation of Ren and MacKenzie [30]. The error analysis on the whole numerator gives an approximation error 14 < E < 32. Therefore we add 32 to the numerator when we use the approximation of Ren and MacKenzie [30]. We obtain the inequalities: √ π ( e (24τ+1)2 4800τ (192τ(3τ + 25)− 1) erfc ( 24τ + 1\n40 √ 3 √ τ\n) − (316)\n2e (48τ+1)2 4800τ (192τ(12τ + 25)− 1) erfc ( 48τ + 1\n40 √ 3 √ τ\n)) + 40 √ 3(72τ − 1) √ τ 6\n√ π  2.911(192τ(3τ + 25)− 1)√ π(2.911−1)(24τ+1)\n40 √ 3 √ τ\n+ √ π (\n24τ+1 40 √ 3 √ τ\n)2 + 2.9112 −\n2 · 2.911(192τ(12τ + 25)− 1) √ π(2.911−1)(48τ+1)\n40 √ 3 √ τ\n+ √ π (\n48τ+1 40 √ 3 √ τ\n)2 + 2.9112 + 40 √ 3(72τ − 1) √ τ + 32 =\n√ π  (192τ(3τ + 25)− 1) (40√32.911√τ) √ π(2.911− 1)(24τ + 1) + √( 40 √ 32.911 √ τ )2 + π(24τ + 1)2 −\n2(192τ(12τ + 25)− 1) ( 40 √ 32.911 √ τ )\n√ π(2.911− 1)(48τ + 1) + √( 40 √ 32.911 √ τ )2 + π(48τ + 1)2 + 40 √\n3(72τ − 1) √ τ + 32 =((\n40 √ 3(72τ − 1) √ τ + 32 )(√ π(2.911− 1)(24τ + 1) + √( 40 √ 32.911 √ τ )2 + π(24τ + 1)2 ) ( √ π(2.911− 1)(48τ + 1) + √( 40 √ 32.911 √ τ )2 + π(48τ + 1)2 ) +\n2.911 · 40 √ 3 √ π(192τ(3τ + 25)− 1) √ τ(\n√ π(2.911− 1)(48τ + 1) + √( 40 √ 32.911 √ τ )2 + π(48τ + 1)2 ) −\n2 √ π40 √ 32.911(192τ(12τ + 25)− 1)\n√ τ ( √ π(2.911− 1)(24τ + 1) + √( 40 √ 32.911 √ τ )2 + π(24τ + 1)2 )) (( √ π(2.911− 1)(24τ + 1) + √( 40 √ 32.911 √ τ )2 + π(24τ + 1)2\n) ( √ π(2.911− 1)(48τ + 1) + √( 40 √ 32.911 √ τ )2 + π(48τ + 1)2 ))−1 .\nAfter applying the approximation of Ren and MacKenzie [30] and adding 200, we first factored out 40 √ 3 √ τ . Then we brought all terms to the same denominator.\nWe now consider the numerator:\n( 40 √ 3(72τ − 1) √ τ + 32 )(√ π(2.911− 1)(24τ + 1) + √( 40 √ 32.911 √ τ )2 + π(24τ + 1)2 ) (317)(\n√ π(2.911− 1)(48τ + 1) + √( 40 √ 32.911 √ τ )2 + π(48τ + 1)2 ) +\n2.911 · 40 √ 3 √ π(192τ(3τ + 25)− 1) √ τ\n( √ π(2.911− 1)(48τ + 1) + √( 40 √ 32.911 √ τ )2 + π(48τ + 1)2 ) −\n2 √ π40 √ 32.911(192τ(12τ + 25)− 1) √ τ(\n√ π(2.911− 1)(24τ + 1) + √( 40 √ 32.911 √ τ )2 + π(24τ + 1)2 ) =\n− 3.42607× 106 √ π(24τ + 1)2 + 40674.8ττ3/2+\n2880 √ 3 √ π(24τ + 1)2 + 40674.8τ √ π(48τ + 1)2 + 40674.8ττ3/2+\n1.72711× 106 √ π(48τ + 1)2 + 40674.8ττ3/2 − 5.81185× 106τ3/2 −\n836198 √ π(24τ + 1)2 + 40674.8ττ5/2 + 611410 √ π(48τ + 1)2 + 40674.8ττ5/2−\n1.67707× 106τ5/2 − 3.44998× 107τ7/2 + 422935.τ2 + 5202.68 √ π(24τ + 1)2 + 40674.8ττ+\n2601.34 √ π(48τ + 1)2 + 40674.8ττ + 26433.4τ + 415.94 √ τ + 480.268 √ τ √ π(24τ + 1)2 + 40674.8τ +\n108.389 √ π(24τ + 1)2 + 40674.8τ − 592.138 √ τ √ π(48τ + 1)2 + 40674.8τ− 40 √ 3 √ τ √ π(24τ + 1)2 + 40674.8τ √ π(48τ + 1)2 + 40674.8τ +\n32 √ π(24τ + 1)2 + 40674.8τ √ π(48τ + 1)2 + 40674.8τ +\n108.389 √ π(48τ + 1)2 + 40674.8τ + 367.131 =\n− 5.81185× 106τ3/2 − 1.67707× 106τ5/2 − 3.44998× 107τ7/2+( −3.42607× 106τ3/2 − 836198τ5/2 + 5202.68τ + 480.268 √ τ + 108.389 ) √ π(24τ + 1)2 + 40674.8τ+( 1.72711× 106τ3/2 + 611410τ5/2 + 2601.34τ − 592.138 √ τ + 108.389\n) √ π(48τ + 1)2 + 40674.8τ+( 2880 √ 3τ3/2 − 40 √ 3 √ τ + 32 )√ π(24τ + 1)2 + 40674.8τ √ π(48τ + 1)2 + 40674.8τ+ 422935.τ2 + 26433.4τ + 415.94 √ τ + 367.131 6\n− 5.81185× 106τ3/2 − 1.67707× 106τ5/2 − 3.44998× 107τ7/2+( −3.42607× 106τ3/2 − 836198τ5/2 + 480.268 √ 1.25 + 1.255202.68 + 108.389 ) √ π(24τ + 1)2 + 40674.8τ+( 1.72711× 106τ3/2 + 611410τ5/2 − 592.138 √ 0.9 + 1.252601.34 + 108.389\n) √ π(48τ + 1)2 + 40674.8τ+( 2880 √ 3τ3/2 − 40 √ 3 √ τ + 32 )√ π(24τ + 1)2 + 40674.8τ √ π(48τ + 1)2 + 40674.8τ+ 422935τ2 + 415.94 √ 1.25 + 1.2526433.4 + 367.131 =\n− 5.81185× 106τ3/2 − 1.67707× 106τ5/2 − 3.44998× 107τ7/2+( −3.42607× 106τ3/2 − 836198τ5/2 + 7148.69 )√ π(24τ + 1)2 + 40674.8τ+(\n1.72711× 106τ3/2 + 611410τ5/2 + 2798.31 )√\nπ(48τ + 1)2 + 40674.8τ+( 2880 √ 3τ3/2 − 40 √ 3 √ τ + 32 )√ π(24τ + 1)2 + 40674.8τ √ π(48τ + 1)2 + 40674.8τ+\n422935τ2 + 33874 =\n− 5.81185× 106τ3/2 − 1.67707× 106τ5/2 − 3.44998× 107τ7/2+( 1.72711× 106τ3/2 + 611410τ5/2 + 2798.31 )√ 2304π(τ + 5.66103)(τ + 0.0000766694)+(\n−3.42607× 106τ3/2 − 836198τ5/2 + 7148.69 )√\n576π(τ + 22.561)(τ + 0.0000769518)+( 2880 √ 3τ3/2 − 40 √ 3 √ τ + 32 )√ 2304π(τ + 5.66103)(τ + 0.0000766694)√\n576π(τ + 22.561)(τ + 0.0000769518)+\n422935τ2 + 33874 6\n− 5.81185106τ3/2 − 1.67707× 106τ5/2 − 3.44998× 107τ7/2+( 1.72711× 106τ3/2 + 611410τ5/2 + 2798.31 )√ 2304π1.0001(τ + 5.66103)τ+(\n2880 √ 3τ3/2 − 40 √ 3 √ τ + 32 )√ 2304π1.0001(τ + 5.66103)τ √ 576π1.0001(τ + 22.561)τ+(\n−3.42607× 106τ3/2 − 836198τ5/2 + 7148.69 )\n√ 576π(τ + 22.561)τ+\n422935τ2 + 33874. =\n− 5.81185106τ3/2 − 1.67707× 106τ5/2 − 3.44998× 107τ7/2+( −250764.τ3/2 + 1.8055× 107τ5/2 + 115823.τ ) √ τ + 5.66103\n√ τ + 22.561 + 422935.τ2+(\n5.20199× 107τ3 + 1.46946× 108τ2 + 238086. √ τ )√\nτ + 5.66103+( −3.55709× 107τ3 − 1.45741× 108τ2 + 304097. √ τ )√\nτ + 22.561 + 33874. 6 √\n1.25 + 5.66103 √ 1.25 + 22.561 ( −250764.τ3/2 + 1.8055× 107τ5/2 + 115823.τ ) +\n√ 1.25 + 5.66103 ( 5.20199× 107τ3 + 1.46946× 108τ2 + 238086. √ τ )\n+ √ 0.9 + 22.561 ( −3.55709× 107τ3 − 1.45741× 108τ2 + 304097. √ τ ) −\n5.81185106τ3/2 − 1.67707× 106τ5/2 − 3.44998× 107τ7/2 + 422935.τ2 + 33874. 6 33874.τ3/2\n0.93/2 − 9.02866× 106τ3/2 + 2.29933× 108τ5/2 − 3.44998× 107τ7/2−\n3.5539× 107τ3 − 3.19193× 108τ2 + 1.48578× 10 6 √ ττ√\n0.9 +\n2.09884× 106τ √ τ\n0.9 =\n− 5.09079× 106τ3/2 + 2.29933× 108τ5/2− 3.44998× 107τ7/2 − 3.5539× 107τ3 − 3.19193× 108τ2 6\n− 5.09079× 106τ3/2 + 2.29933× 10 8 √\n1.25τ5/2√ τ − 3.44998× 107τ7/2−\n3.5539× 107τ3 − 3.19193× 108τ2 = − 5.09079× 106τ3/2 − 3.44998× 107τ7/2 − 3.5539× 107τ3 − 6.21197× 107τ2 < 0 .\nFirst we expanded the term (multiplied it out). The we put the terms multiplied by the same square root into brackets. The next inequality sign stems from inserting the maximal value of 1.25 for τ for some positive terms and value of 0.9 for negative terms. These terms are then expanded at the =-sign. The next equality factors the terms under the squared root. We decreased the negative term by setting τ = τ + 0.0000769518 under the root. We increased positive terms by setting τ + 0.0000769518 = 1.0000962τ and τ + 0.0000766694 = 1.0000962τ under the root for positive terms. The positive terms are increase, since 0.8+0.00007695180.8 < 1.0000962, thus τ + 0.0000766694 < τ + 0.0000769518 6 1.0000962τ . For the next inequality we decreased negative terms by inserting τ = 0.9 and increased positive terms by inserting τ = 1.25. The next\nequality expands the terms. We use upper bound of 1.25 and lower bound of 0.9 to obtain terms with corresponding exponents of τ .\nConsequently, the derivative of\nτ ( e ( µω+ντ√ 2 √ ντ )2 erfc ( µω + ντ√\n2 √ ντ\n) − 2e ( µω+2ντ√ 2 √ ντ )2 erfc ( µω + 2ντ√\n2 √ ντ\n)) (318)\nwith respect to τ is smaller than zero for maximal ν = 0.24 and the domain 0.9 6 τ 6 1.25.\nLemma 47. In the domain −0.01 6 y 6 0.01 and 0.64 6 x 6 1.875, the function f(x, y) = e 1 2 (2y+x) erfc ( x+y√\n2x\n) has a global maximum at y = 0.64 and x = −0.01 and a global minimum at\ny = 1.875 and x = 0.01.\nProof. f(x, y) = e 1 2 (2y+x) erfc ( x+y√\n2x\n) is strictly monotonically decreasing in x, since its derivative\nwith respect to x is negative:\ne− y2 2x (√ πx3/2e (x+y)2 2x erfc ( x+y√ 2 √ x ) + √ 2(y − x) )\n2 √ πx3/2\n< 0\n⇐⇒ √ πx3/2e (x+y)2 2x erfc ( x+ y√\n2 √ x\n) + √ 2(y − x) < 0\n√ πx3/2e (x+y)2 2x erfc ( x+ y√\n2 √ x\n) + √ 2(y − x) 6\n2x3/2\nx+y√ 2 √ x\n+ √ (x+y)2\n2x + 4 π\n+ y √ 2− x √ 2 6\n2 · 0.643/2\n0.01+0.64√ 2 √ 0.64 + √ (0.01+0.64)2 2·0.64 + 4 π + 0.01 √ 2− 0.64 √ 2 = −0.334658 < 0. (319)\nThe two last inqualities come from applying Abramowitz bounds 22 and from the fact that the expression 2x 3/2\nx+y√ 2 √ x +\n√ (x+y)2\n2x + 4 π\n+ y √ 2− x √ 2 does not change monotonicity in the domain and hence\nthe maximum must be found at the border. For x = 0.64 that maximizes the function f(x, y) is monotonically in y, because its derivative w.r.t. y at x = 0.64 is\ney ( 1.37713 erfc(0.883883y + 0.565685)− 1.37349e−0.78125(y+0.64) 2 ) < 0\n⇐⇒ ( 1.37713 erfc(0.883883y + 0.565685)− 1.37349e−0.78125(y+0.64) 2 ) < 0(\n1.37713 erfc(0.883883y + 0.565685)− 1.37349e−0.78125(y+0.64) 2 ) 6(\n1.37713 erfc(0.883883 · −0.01 + 0.565685)− 1.37349e−0.78125(0.01+0.64) 2 ) =\n0.5935272325870631− 0.987354705867739 < 0. (320)\nTherefore, the values y = 0.64 and x = −0.01 give a global maximum of the function f(x, y) in the domain −0.01 6 y 6 0.01 and 0.64 6 x 6 1.875 and the values y = 1.875 and x = 0.01 give the global minimum.\nA4 Additional information on experiments\nIn this section, we report the hyperparameters that were considered for each method and data set and give details on the processing of the data sets.\nA4.1 121 UCI Machine Learning Repository data sets: Hyperparameters\nFor the UCI data sets, the best hyperparameter setting was determined by a grid-search over all hyperparameter combinations using 15% of the training data as validation set. The early stopping parameter was determined on the smoothed learning curves of 100 epochs of the validation set. Smoothing was done using moving averages of 10 consecutive values. We tested “rectangular” and “conic” layers – rectangular layers have constant number of hidden units in each layer, conic layers start with the given number of hidden units in the first layer and then decrease the number of hidden units to the size of the output layer according to the geometric progession. If multiple hyperparameters provided identical performance on the validation set, we preferred settings with a higher number of layers, lower learning rates and higher dropout rates. All methods had the chance to adjust their hyperparameters to the data set at hand.\nTable A4: Hyperparameters considered for self-normalizing networks in the UCI data sets.\nHyperparameter Considered values\nNumber of hidden units {1024, 512, 256} Number of hidden layers {2, 3, 4, 8, 16, 32} Learning rate {0.01, 0.1, 1} Dropout rate {0.05, 0} Layer form {rectangular, conic}\nTable A5: Hyperparameters considered for ReLU networks with MS initialization in the UCI data sets.\nHyperparameter Considered values\nNumber of hidden units {1024, 512, 256} Number of hidden layers {2,3,4,8,16,32} Learning rate {0.01, 0.1, 1} Dropout rate {0.5, 0} Layer form {rectangular, conic}\nTable A6: Hyperparameters considered for batch normalized networks in the UCI data sets.\nHyperparameter Considered values\nNumber of hidden units {1024, 512, 256} Number of hidden layers {2, 3, 4, 8, 16, 32} Learning rate {0.01, 0.1, 1} Normalization {Batchnorm} Layer form {rectangular, conic}\nTable A7: Hyperparameters considered for weight normalized networks in the UCI data sets.\nHyperparameter Considered values\nNumber of hidden units {1024, 512, 256} Number of hidden layers {2, 3, 4, 8, 16, 32} Learning rate {0.01, 0.1, 1} Normalization {Weightnorm} Layer form {rectangular, conic}\nTable A8: Hyperparameters considered for layer normalized networks in the UCI data sets.\nHyperparameter Considered values\nNumber of hidden units {1024, 512, 256} Number of hidden layers {2, 3, 4, 8, 16, 32} Learning rate {0.01, 0.1, 1} Normalization {Layernorm} Layer form {rectangular, conic}\nTable A9: Hyperparameters considered for Highway networks in the UCI data sets.\nHyperparameter Considered values\nNumber of hidden layers {2, 3, 4, 8, 16, 32} Learning rate {0.01, 0.1, 1} Dropout rate {0, 0.5}\nTable A10: Hyperparameters considered for Residual networks in the UCI data sets.\nHyperparameter Considered values\nNumber of blocks {2, 3, 4, 8, 16} Number of neurons per blocks {1024, 512, 256} Block form {rectangular, diavolo} Bottleneck {25%, 50%} Learning rate {0.01, 0.1, 1}\nA4.2 121 UCI Machine Learning Repository data sets: detailed results\nMethods compared. We used data sets and preprocessing scripts by Fernández-Delgado et al. [10] for data preparation and defining training and test sets. With several flaws in the method comparison[37] that we avoided, the authors compared 179 machine learning methods of 17 groups in their experiments. The method groups were defined by Fernández-Delgado et al. [10] as follows: Support Vector Machines, RandomForest, Multivariate adaptive regression splines (MARS), Boosting, Rule-based, logistic and multinomial regression, Discriminant Analysis (DA), Bagging, Nearest Neighbour, DecisionTree, other Ensembles, Neural Networks, Bayesian, Other Methods, generalized linear models (GLM), Partial least squares and principal component regression (PLSR), and Stacking. However, many of methods assigned to those groups were merely different implementations of the same method. Therefore, we selected one representative of each of the 17 groups for method comparison. The representative method was chosen as the group’s method with the median performance across all tasks. Finally, we included 17 other machine learning methods of Fernández-Delgado et al. [10], and 6 FNNs, BatchNorm, WeightNorm, LayerNorm, Highway, Residual and MSRAinit networks, and self-normalizing neural networks (SNNs) giving a total of 24 compared methods.\nResults of FNN methods for all 121 data sets. The results of the compared FNN methods can be found in Table A11.\nSmall and large data sets. We assigned each of the 121 UCI data sets into the group “large datasets” or “small datasets” if the had more than 1,000 data points or less, respectively. We expected that Deep Learning methods require large data sets to competitive to other machine learning methods. This resulted in 75 small and 46 large data sets.\nResults. The results of the method comparison are given in Tables A12 and A13 for small and large data sets, respectively. On small data sets, SVMs performed best followed by RandomForest and SNNs. On large data sets, SNNs are the best method followed by SVMs and Random Forest.\nimage-segmentation 2310 19 0.9114 0.9090 0.9024 0.8919 0.8481 0.8938 0.8838 ionosphere 351 34 0.8864 0.9091 0.9432 0.9545 0.9432 0.9318 0.9432 iris 150 5 0.9730 0.9189 0.8378 0.9730 0.9189 1.0000 0.9730 led-display 1000 8 0.7640 0.7200 0.7040 0.7160 0.6280 0.6920 0.6480 lenses 24 5 0.6667 1.0000 1.0000 0.6667 0.8333 0.8333 0.6667 letter 20000 17 0.9726 0.9712 0.8984 0.9762 0.9796 0.9580 0.9742 libras 360 91 0.7889 0.8667 0.8222 0.7111 0.7444 0.8000 0.8333 low-res-spect 531 101 0.8571 0.8496 0.9023 0.8647 0.8571 0.8872 0.8947 lung-cancer 32 57 0.6250 0.3750 0.1250 0.2500 0.5000 0.5000 0.2500 lymphography 148 19 0.9189 0.7297 0.7297 0.6757 0.7568 0.7568 0.7838 magic 19020 11 0.8692 0.8629 0.8673 0.8723 0.8713 0.8690 0.8620 mammographic 961 6 0.8250 0.8083 0.7917 0.7833 0.8167 0.8292 0.8208 miniboone 130064 51 0.9307 0.9250 0.9270 0.9254 0.9262 0.9272 0.9313 molec-biol-promoter 106 58 0.8462 0.7692 0.6923 0.7692 0.7692 0.6923 0.4615 molec-biol-splice 3190 61 0.9009 0.8482 0.8833 0.8557 0.8519 0.8494 0.8607 monks-1 556 7 0.7523 0.6551 0.5833 0.7546 0.9074 0.5000 0.7014 monks-2 601 7 0.5926 0.6343 0.6389 0.6273 0.3287 0.6644 0.5162 monks-3 554 7 0.6042 0.7454 0.5880 0.5833 0.5278 0.5231 0.6991 mushroom 8124 22 1.0000 1.0000 1.0000 1.0000 0.9990 0.9995 0.9995 musk-1 476 167 0.8739 0.8655 0.8992 0.8739 0.8235 0.8992 0.8992 musk-2 6598 167 0.9891 0.9945 0.9915 0.9964 0.9982 0.9927 0.9951 nursery 12960 9 0.9978 0.9988 1.0000 0.9994 0.9994 0.9966 0.9966 oocytes_merluccius_nucleus_4d 1022 42 0.8235 0.8196 0.7176 0.8000 0.8078 0.8078 0.7686 oocytes_merluccius_states_2f 1022 26 0.9529 0.9490 0.9490 0.9373 0.9333 0.9020 0.9412 oocytes_trisopterus_nucleus_2f 912 26 0.7982 0.8728 0.8289 0.7719 0.7456 0.7939 0.8202 oocytes_trisopterus_states_5b 912 33 0.9342 0.9430 0.9342 0.8947 0.8947 0.9254 0.8991 optical 5620 63 0.9711 0.9666 0.9644 0.9627 0.9716 0.9638 0.9755 ozone 2536 73 0.9700 0.9732 0.9716 0.9669 0.9669 0.9748 0.9716 page-blocks 5473 11 0.9583 0.9708 0.9656 0.9605 0.9613 0.9730 0.9708 parkinsons 195 23 0.8980 0.9184 0.8367 0.9184 0.8571 0.8163 0.8571 pendigits 10992 17 0.9706 0.9714 0.9671 0.9708 0.9734 0.9620 0.9657 pima 768 9 0.7552 0.7656 0.7188 0.7135 0.7188 0.6979 0.6927 pittsburg-bridges-MATERIAL 106 8 0.8846 0.8462 0.9231 0.9231 0.8846 0.8077 0.9231 pittsburg-bridges-REL-L 103 8 0.6923 0.7692 0.6923 0.8462 0.7692 0.6538 0.7308 pittsburg-bridges-SPAN 92 8 0.6957 0.5217 0.5652 0.5652 0.5652 0.6522 0.6087 pittsburg-bridges-T-OR-D 102 8 0.8400 0.8800 0.8800 0.8800 0.8800 0.8800 0.8800 pittsburg-bridges-TYPE 105 8 0.6538 0.6538 0.5385 0.6538 0.1154 0.4615 0.6538 planning 182 13 0.6889 0.6667 0.6000 0.7111 0.6222 0.6444 0.6889 plant-margin 1600 65 0.8125 0.8125 0.8375 0.7975 0.7600 0.8175 0.8425 plant-shape 1600 65 0.7275 0.6350 0.6325 0.5150 0.2850 0.6575 0.6775 plant-texture 1599 65 0.8125 0.7900 0.7900 0.8000 0.8200 0.8175 0.8350 post-operative 90 9 0.7273 0.7273 0.5909 0.7273 0.5909 0.5455 0.7727 primary-tumor 330 18 0.5244 0.5000 0.4512 0.3902 0.5122 0.5000 0.4512 ringnorm 7400 21 0.9751 0.9843 0.9692 0.9811 0.9843 0.9719 0.9827 seeds 210 8 0.8846 0.8654 0.9423 0.8654 0.8654 0.8846 0.8846 semeion 1593 257 0.9196 0.9296 0.9447 0.9146 0.9372 0.9322 0.9447 soybean 683 36 0.8511 0.8723 0.8617 0.8670 0.8883 0.8537 0.8484 spambase 4601 58 0.9409 0.9461 0.9435 0.9461 0.9426 0.9504 0.9513 spect 265 23 0.6398 0.6183 0.6022 0.6667 0.6344 0.6398 0.6720 spectf 267 45 0.4973 0.6043 0.8930 0.7005 0.2299 0.4545 0.5561 statlog-australian-credit 690 15 0.5988 0.6802 0.6802 0.6395 0.6802 0.6860 0.6279 statlog-german-credit 1000 25 0.7560 0.7280 0.7760 0.7720 0.7520 0.7400 0.7400\nmethodGroup method avg. rank p-value\nSVM LibSVM_weka 9.3 RandomForest RRFglobal_caret 9.6 2.5e-01 SNN SNN 9.6 3.8e-01 LMR SimpleLogistic_weka 9.9 1.5e-01 NeuralNetworks lvq_caret 10.1 1.0e-01 MARS gcvEarth_caret 10.7 3.6e-02 MSRAinit MSRAinit 11.0 4.0e-02 LayerNorm LayerNorm 11.3 7.2e-02 Highway Highway 11.5 8.9e-03 DiscriminantAnalysis mda_R 11.8 2.6e-03 Boosting LogitBoost_weka 11.9 2.4e-02 Bagging ctreeBag_R 12.1 1.8e-03 ResNet ResNet 12.3 3.5e-03 BatchNorm BatchNorm 12.6 4.9e-04 Rule-based JRip_caret 12.9 1.7e-04 WeightNorm WeightNorm 13.0 8.3e-05 DecisionTree rpart2_caret 13.6 7.0e-04 OtherEnsembles Dagging_weka 13.9 3.0e-05 Nearest Neighbour NNge_weka 14.0 7.7e-04 OtherMethods pam_caret 14.2 1.5e-04 PLSR simpls_R 14.3 4.6e-05 Bayesian NaiveBayes_weka 14.6 1.2e-04 GLM bayesglm_caret 15.0 1.6e-06 Stacking Stacking_weka 20.9 2.2e-12\nA4.3 Tox21 challenge data set: Hyperparameters\nFor the Tox21 data set, the best hyperparameter setting was determined by a grid-search over all hyperparameter combinations using the validation set defined by the challenge winners [28]. The hyperparameter space was chosen to be similar to the hyperparameters that were tested by Mayr et al. [28]. The early stopping parameter was determined on the smoothed learning curves of 100 epochs of the validation set. Smoothing was done using moving averages of 10 consecutive values. We tested “rectangular” and “conic” layers – rectangular layers have constant number of hidden units in each layer, conic layers start with the given number of hidden units in the first layer and then decrease the number of hidden units to the size of the output layer according to the geometric progession. All methods had the chance to adjust their hyperparameters to the data set at hand.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n−6 −3 0 3 6\nd e\nn s it y\nnetwork inputs\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n−6 −3 0 3 6\nd e\nn s it y\nnetwork inputs\nFigure A8: Distribution of network inputs of an SNN for the Tox21 data set. The plots show the distribution of network inputs z of the second layer of a typical Tox21 network. The red curves display a kernel density estimator of the network inputs and the black curve is the density of a standard normal distribution. Left panel: At initialization time before learning. The distribution of network inputs is close to a standard normal distribution. Right panel: After 40 epochs of learning. The distributions of network inputs is close to a normal distribution.\nDistribution of network inputs. We empirically checked the assumption that the distribution of network inputs can well be approximated by a normal distribution. To this end, we investigated the density of the network inputs before and during learning and found that these density are close to normal distributions (see Figure A8).\nA4.4 HTRU2 data set: Hyperparameters\nFor the HTRU2 data set, the best hyperparameter setting was determined by a grid-search over all hyperparameter combinations using one of the 9 non-testing folds as validation fold in a nested cross-validation procedure. Concretely, if M was the testing fold, we used M − 1 as validation fold, and for M = 1 we used fold 10 for validation. The early stopping parameter was determined on the smoothed learning curves of 100 epochs of the validation set. Smoothing was done using moving averages of 10 consecutive values. We tested “rectangular” and “conic” layers – rectangular layers have constant number of hidden units in each layer, conic layers start with the given number of hidden units in the first layer and then decrease the number of hidden units to the size of the output layer according to the geometric progession. All methods had the chance to adjust their hyperparameters to the data set at hand.\nA5 Other fixed points\nA similar analysis with corresponding function domains can be performed for other fixed points, for example for µ = µ̃ = 0 and ν = ν̃ = 2, which leads to a SELU activation function with parameters α02 = 1.97126 and λ02 = 1.06071.\nA6 Bounds determined by numerical methods\nIn this section we report bounds on previously discussed expressions as determined by numerical methods (min and max have been computed).\n0(µ=0.06,ω=0,ν=1.35,τ=1.12) < ∂J11 ∂µ < .00182415(µ=−0.1,ω=0.1,ν=1.47845,τ=0.883374)\n(321)\n0.905413(µ=0.1,ω=−0.1,ν=1.5,τ=1.25) < ∂J11 ∂ω < 1.04143(µ=0.1,ω=0.1,ν=0.8,τ=0.8)\n−0.0151177(µ=−0.1,ω=0.1,ν=0.8,τ=1.25) < ∂J11 ∂ν < 0.0151177(µ=0.1,ω=−0.1,ν=0.8,τ=1.25)\n−0.015194(µ=−0.1,ω=0.1,ν=0.8,τ=1.25) < ∂J11 ∂τ < 0.015194(µ=0.1,ω=−0.1,ν=0.8,τ=1.25)\n−0.0151177(µ=−0.1,ω=0.1,ν=0.8,τ=1.25) < ∂J12 ∂µ < 0.0151177(µ=0.1,ω=−0.1,ν=0.8,τ=1.25)\n−0.0151177(µ=0.1,ω=−0.1,ν=0.8,τ=1.25) < ∂J12 ∂ω < 0.0151177(µ=0.1,ω=−0.1,ν=0.8,τ=1.25)\n−0.00785613(µ=0.1,ω=−0.1,ν=1.5,τ=1.25) < ∂J12 ∂ν < 0.0315805(µ=0.1,ω=0.1,ν=0.8,τ=0.8)\n0.0799824(µ=0.1,ω=−0.1,ν=1.5,τ=1.25) < ∂J12 ∂τ < 0.110267(µ=−0.1,ω=0.1,ν=0.8,τ=0.8)\n0(µ=0.06,ω=0,ν=1.35,τ=1.12) < ∂J21 ∂µ < 0.0174802(µ=0.1,ω=0.1,ν=0.8,τ=0.8)\n0.0849308(µ=0.1,ω=−0.1,ν=0.8,τ=0.8) < ∂J21 ∂ω < 0.695766(µ=0.1,ω=0.1,ν=1.5,τ=1.25)\n−0.0600823(µ=0.1,ω=−0.1,ν=0.8,τ=1.25) < ∂J21 ∂ν < 0.0600823(µ=−0.1,ω=0.1,ν=0.8,τ=1.25)\n−0.0673083(µ=0.1,ω=−0.1,ν=1.5,τ=0.8) < ∂J21 ∂τ < 0.0673083(µ=−0.1,ω=0.1,ν=1.5,τ=0.8)\n−0.0600823(µ=0.1,ω=−0.1,ν=0.8,τ=1.25) < ∂J22 ∂µ < 0.0600823(µ=−0.1,ω=0.1,ν=0.8,τ=1.25)\n−0.0600823(µ=0.1,ω=−0.1,ν=0.8,τ=1.25) < ∂J22 ∂ω < 0.0600823(µ=−0.1,ω=0.1,ν=0.8,τ=1.25)\n−0.276862(µ=−0.01,ω=−0.01,ν=0.8,τ=1.25) < ∂J22 ∂ν < −0.084813(µ=−0.1,ω=0.1,ν=1.5,τ=0.8)\n0.562302(µ=0.1,ω=−0.1,ν=1.5,τ=1.25) < ∂J22 ∂τ < 0.664051(µ=0.1,ω=0.1,ν=0.8,τ=0.8)\n∣∣∣∣∂J11∂µ ∣∣∣∣ < 0.00182415(0.0031049101995398316) (322)∣∣∣∣∂J11∂ω ∣∣∣∣ < 1.04143(1.055872374194189)∣∣∣∣∂J11∂ν ∣∣∣∣ < 0.0151177(0.031242911235461816)\n∣∣∣∣∂J11∂τ ∣∣∣∣ < 0.015194(0.03749149348255419)∣∣∣∣∂J12∂µ ∣∣∣∣ < 0.0151177(0.031242911235461816)∣∣∣∣∂J12∂ω ∣∣∣∣ < 0.0151177(0.031242911235461816)∣∣∣∣∂J12∂ν ∣∣∣∣ < 0.0315805(0.21232788238624354)∣∣∣∣∂J12∂τ ∣∣∣∣ < 0.110267(0.2124377655377270)∣∣∣∣∂J21∂µ ∣∣∣∣ < 0.0174802(0.02220441024325437)∣∣∣∣∂J21∂ω ∣∣∣∣ < 0.695766(1.146955401845684)∣∣∣∣∂J21∂ν ∣∣∣∣ < 0.0600823(0.14983446469110305)∣∣∣∣∂J21∂τ ∣∣∣∣ < 0.0673083(0.17980135762932363)∣∣∣∣∂J22∂µ ∣∣∣∣ < 0.0600823(0.14983446469110305)∣∣∣∣∂J22∂ω ∣∣∣∣ < 0.0600823(0.14983446469110305)∣∣∣∣∂J22∂ν ∣∣∣∣ < 0.562302(1.805740052651535)∣∣∣∣∂J22∂τ ∣∣∣∣ < 0.664051(2.396685907216327)\nA7 References\n[1] Abramowitz, M. and Stegun, I. (1964). Handbook of Mathematical Functions, volume 55 of Applied Mathematics Series. National Bureau of Standards, 10th edition.\n[2] Ba, J. L., Kiros, J. R., and Hinton, G. (2016). Layer normalization. arXiv preprint arXiv:1607.06450.\n[3] Bengio, Y. (2013). Deep learning of representations: Looking forward. In Proceedings of the First International Conference on Statistical Language and Speech Processing, pages 1–37, Berlin, Heidelberg.\n[4] Blinn, J. (1996). Consider the lowly 2×2 matrix. IEEE Computer Graphics and Applications, pages 82–88.\n[5] Bradley, R. C. (1981). Central limit theorems under weak dependence. Journal of Multivariate Analysis, 11(1):1–16.\n[6] Cireşan, D. and Meier, U. (2015). Multi-column deep neural networks for offline handwritten chinese character classification. In 2015 International Joint Conference on Neural Networks (IJCNN), pages 1–6. IEEE.\n[7] Clevert, D.-A., Unterthiner, T., and Hochreiter, S. (2015). Fast and accurate deep network learning by exponential linear units (ELUs). 5th International Conference on Learning Representations, arXiv:1511.07289.\n[8] Dugan, P., Clark, C., LeCun, Y., and Van Parijs, S. (2016). Phase 4: Dcl system using deep learning approaches for land-based or ship-based real-time recognition and localization of marine mammals-distributed processing and big data applications. arXiv preprint arXiv:1605.00982.\n[9] Esteva, A., Kuprel, B., Novoa, R., Ko, J., Swetter, S., Blau, H., and Thrun, S. (2017). Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542(7639):115–118.\n[10] Fernández-Delgado, M., Cernadas, E., Barro, S., and Amorim, D. (2014). Do we need hundreds of classifiers to solve real world classification problems. Journal of Machine Learning Research, 15(1):3133–3181.\n[11] Goldberg, D. (1991). What every computer scientist should know about floating-point arithmetic. ACM Comput. Surv., 223(1):5–48.\n[12] Graves, A., Mohamed, A., and Hinton, G. (2013). Speech recognition with deep recurrent neural networks. In IEEE International conference on acoustics, speech and signal processing (ICASSP), pages 6645–6649.\n[13] Graves, A. and Schmidhuber, J. (2009). Offline handwriting recognition with multidimensional recurrent neural networks. In Advances in neural information processing systems, pages 545–552.\n[14] Gulshan, V., Peng, L., Coram, M., Stumpe, M. C., Wu, D., Narayanaswamy, A., Venugopalan, S., Widner, K., Madams, T., Cuadros, J., et al. (2016). Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. JAMA, 316(22):2402–2410.\n[15] Harrison, J. (1999). A machine-checked theory of floating point arithmetic. In Bertot, Y., Dowek, G., Hirschowitz, A., Paulin, C., and Théry, L., editors, Theorem Proving in Higher Order Logics: 12th International Conference, TPHOLs’99, volume 1690 of Lecture Notes in Computer Science, pages 113–130. Springer-Verlag.\n[16] He, K., Zhang, X., Ren, S., and Sun, J. (2015a). Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n[17] He, K., Zhang, X., Ren, S., and Sun, J. (2015b). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 1026–1034.\n[18] Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8):1735–1780.\n[19] Huval, B., Wang, T., Tandon, S., et al. (2015). An empirical evaluation of deep learning on highway driving. arXiv preprint arXiv:1504.01716.\n[20] Ioffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of The 32nd International Conference on Machine Learning, pages 448–456.\n[21] Kahan, W. (2004). A logarithm too clever by half. Technical report, University of California, Berkeley.\n[22] Korolev, V. and Shevtsova, I. (2012). An improvement of the Berry–Esseen inequality with applications to Poisson and mixed Poisson random sums. Scandinavian Actuarial Journal, 2012(2):81–105.\n[23] Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105.\n[24] LeCun, Y. and Bengio, Y. (1995). Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361(10):1995.\n[25] LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature, 521(7553):436–444.\n[26] Loosemore, S., Stallman, R. M., McGrath, R., Oram, A., and Drepper, U. (2016). The GNU C Library: Application Fundamentals. GNU Press, Free Software Foundation, 51 Franklin St, Fifth Floor, Boston, MA 02110-1301, USA, 2.24 edition.\n[27] Lyon, R., Stappers, B., Cooper, S., Brooke, J., and Knowles, J. (2016). Fifty years of pulsar candidate selection: From simple filters to a new principled real-time classification approach. Monthly Notices of the Royal Astronomical Society, 459(1):1104–1123.\n[28] Mayr, A., Klambauer, G., Unterthiner, T., and Hochreiter, S. (2016). DeepTox: Toxicity prediction using deep learning. Frontiers in Environmental Science, 3:80.\n[29] Muller, J.-M. (2005). On the definition of ulp(x). Technical Report Research report RR2005-09, Laboratoire de l’Informatique du Parallélisme.\n[30] Ren, C. and MacKenzie, A. R. (2007). Closed-form approximations to the error and complementary error functions and their applications in atmospheric science. Atmos. Sci. Let., pages 70–73.\n[31] Sak, H., Senior, A., Rao, K., and Beaufays, F. (2015). Fast and accurate recurrent neural network acoustic models for speech recognition. arXiv preprint arXiv:1507.06947.\n[32] Salimans, T. and Kingma, D. P. (2016). Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems, pages 901–909.\n[33] Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks, 61:85–117.\n[34] Silver, D., Huang, A., Maddison, C., et al. (2016). Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484–489.\n[35] Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Training very deep networks. In Advances in Neural Information Processing Systems, pages 2377–2385.\n[36] Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112.\n[37] Wainberg, M., Alipanahi, B., and Frey, B. J. (2016). Are random forests truly the best classifiers? Journal of Machine Learning Research, 17(110):1–5.\nList of Figures\n1 FNN and SNN trainin error curves . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2 Visualization of the mapping g . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\nA3 Graph of the main subfunction of the derivative of the second moment . . . . . . . 30\nA4 Graph of the Abramowitz bound for the complementary error function. . . . . . . . 37\nA5 Graphs of the functions ex 2 erfc(x) and xex 2 erfc(x). . . . . . . . . . . . . . . . . 38\nA6 The graph of function µ̃ for low variances . . . . . . . . . . . . . . . . . . . . . . 56\nA7 Graph of the function h(x) = µ̃2(0.1,−0.1, x, 1, λ01, α01) . . . . . . . . . . . . . 57 A8 Distribution of network inputs in Tox21 SNNs. . . . . . . . . . . . . . . . . . . . 94\nList of Tables\n1 Comparison of seven FNNs on 121 UCI tasks . . . . . . . . . . . . . . . . . . . . 8\n2 Comparison of FNNs at the Tox21 challenge dataset . . . . . . . . . . . . . . . . . 8\n3 Comparison of FNNs and reference methods at HTRU2 . . . . . . . . . . . . . . . 9\nA4 Hyperparameters considered for self-normalizing networks in the UCI data sets. . . 85\nA5 Hyperparameters considered for ReLU networks in the UCI data sets. . . . . . . . 85\nA6 Hyperparameters considered for batch normalized networks in the UCI data sets. . 85\nA7 Hyperparameters considered for weight normalized networks in the UCI data sets. . 86\nA8 Hyperparameters considered for layer normalized networks in the UCI data sets. . . 86\nA9 Hyperparameters considered for Highway networks in the UCI data sets. . . . . . . 86\nA10 Hyperparameters considered for Residual networks in the UCI data sets. . . . . . . 86\nA11 Comparison of FNN methods on all 121 UCI data sets. . . . . . . . . . . . . . . . 88\nA12 Method comparison on small UCI data sets . . . . . . . . . . . . . . . . . . . . . 90\nA13 Method comparison on large UCI data sets . . . . . . . . . . . . . . . . . . . . . . . 91\nA14 Hyperparameters considered for self-normalizing networks in the Tox21 data set. . 92\nA15 Hyperparameters considered for ReLU networks in the Tox21 data set. . . . . . . . 92\nA16 Hyperparameters considered for batch normalized networks in the Tox21 data set. . 92\nA17 Hyperparameters considered for weight normalized networks in the Tox21 data set. 93\nA18 Hyperparameters considered for layer normalized networks in the Tox21 data set. . 93\nA19 Hyperparameters considered for Highway networks in the Tox21 data set. . . . . . 93\nA20 Hyperparameters considered for Residual networks in the Tox21 data set. . . . . . 93\nA21 Hyperparameters considered for self-normalizing networks on the HTRU2 data set. 95\nA22 Hyperparameters considered for ReLU networks on the HTRU2 data set. . . . . . . 95\nA23 Hyperparameters considered for BatchNorm networks on the HTRU2 data set. . . . 95\nA24 Hyperparameters considered for WeightNorm networks on the HTRU2 data set. . . 96\nA25 Hyperparameters considered for LayerNorm networks on the HTRU2 data set. . . . 96\nA26 Hyperparameters considered for Highway networks on the HTRU2 data set. . . . . 96\nA27 Hyperparameters considered for Residual networks on the HTRU2 data set. . . . . 96\nBrief index\nAbramowitz bounds, 37\nBanach Fixed Point Theorem, 13 bounds\nderivatives of Jacobian entries, 21 Jacobian entries, 23 mean and variance, 24 singular value, 25, 27\ncentral limit theorem, 6 complementary error function\nbounds, 37 definition, 37\ncomputer-assisted proof, 33 contracting variance, 29\ndefinitions, 2 domain\nsingular value, 19 Theorem 1, 12 Theorem 2, 12 Theorem 3, 13\ndropout, 6\nerf, 37 erfc, 37 error function\nbounds, 37 definition, 37 properties, 39\nexpanding variance, 32 experiments, 7, 85\nastronomy, 8 HTRU2, 8, 95\nhyperparameters, 95 methods compared, 7 Tox21, 7, 92\nhyperparameters, 8, 92 UCI, 7, 85\ndetails, 85 hyperparameters, 85 results, 86\ninitialization, 6\nJacobian, 20 bounds, 23 definition, 20 derivatives, 21 entries, 20, 23 singular value, 21 singular value bound, 25\nlemmata, 19 Jacobian bound, 19\nmapping g, 2, 4\ndefinition, 11 mapping in domain, 29\nself-normalizing neural networks, 2 SELU\ndefinition, 3 parameters, 4, 11\nTheorem 1, 5, 12 proof, 13 proof sketch, 5 Theorem 2, 6, 12 proof, 14 Theorem 3, 6, 12 proof, 18'}]
