id: SP:a2e94b81126dc1b94111cc91c36d1f18a6bdeae2
title: Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms
authors: [{'affiliations': [], 'name': 'Tom Zahavy'}, {'affiliations': [], 'name': 'Bingyi Kang'}, {'affiliations': [], 'name': 'Alex Sivak'}, {'affiliations': [], 'name': 'Jiashi Feng'}, {'affiliations': [], 'name': 'Huan Xu'}, {'affiliations': [], 'name': 'Shie Mannor'}]
abstractText: The question why deep learning algorithms generalize so well has attracted increasing research interest. However, most of the well-established approaches, such as hypothesis capacity, stability or sparseness, have not provided complete explanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus on the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis will not change much due to perturbations of its training examples, then it will also generalize well. As most deep learning algorithms are stochastic (e.g., Stochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness arguments of Xu & Mannor, and introduce a new approach – ensemble robustness – that concerns the robustness of a population of hypotheses. Through the lens of ensemble robustness, we reveal that a stochastic learning algorithm can generalize well as long as its sensitiveness to adversarial perturbations is bounded in average over training examples. Moreover, an algorithm may be sensitive to some adversarial examples (Goodfellow et al., 2015) but still generalize well. To support our claims, we provide extensive simulations for different deep learning algorithms and different network architectures exhibiting a strong correlation between ensemble robustness and the ability to generalize.
references: [{'authors': ['Devansh Arpit', 'Stanislaw Jastrzkebski', 'Nicolas Ballas', 'David Krueger', 'Emmanuel Bengio', 'Maxinder S Kanwal', 'Tegan Maharaj', 'Asja Fischer', 'Aaron Courville', 'Yoshua Bengio'], 'title': 'A closer look at memorization in deep networks', 'venue': 'International Conference on Machine Learning,', 'year': 2017}, {'authors': ['Pierre Baldi', 'Peter J Sadowski'], 'title': 'Understanding dropout', 'venue': 'In Advances in Neural Information Processing Systems,', 'year': 2013}, {'authors': ['Charles Blundell', 'Julien Cornebise', 'Koray Kavukcuoglu', 'Daan Wierstra'], 'title': 'Weight uncertainty in neural networks', 'venue': 'International Conference on Machine Learning,', 'year': 2015}, {'authors': ['Yarin Gal', 'Zoubin Ghahramani'], 'title': 'Dropout as a bayesian approximation: Insights and applications', 'venue': 'In Deep Learning Workshop,', 'year': 2015}, {'authors': ['Ian J Goodfellow', 'Jonathon Shlens', 'Christian Szegedy'], 'title': 'Explaining and harnessing adversarial examples', 'venue': 'In International Conference on Learning Representations,', 'year': 2015}, {'authors': ['Shixiang Gu', 'Luca Rigazio'], 'title': 'Towards deep neural network architectures robust to adversarial examples', 'venue': 'arXiv preprint arXiv:1412.5068,', 'year': 2014}, {'authors': ['Moritz Hardt', 'Benjamin Recht', 'Yoram Singer'], 'title': 'Train faster, generalize better: Stability of stochastic gradient descent', 'venue': 'arXiv preprint arXiv:1509.01240,', 'year': 2015}, {'authors': ['Geoffrey Hinton', 'Oriol Vinyals', 'Jeff Dean'], 'title': 'Distilling the knowledge in a neural network', 'venue': 'In NIPS Deep Learning and Representation Learning Workshop,', 'year': 2014}, {'authors': ['Prateek Jain', 'Vivek Kulkarni', 'Abhradeep Thakurta', 'Oliver Williams'], 'title': 'To drop or not to drop: Robustness, consistency and differential privacy properties of dropout', 'venue': 'arXiv preprint arXiv:1503.02031,', 'year': 2015}, {'authors': ['Kenji Kawaguchi', 'Leslie Pack Kaelbling', 'Yoshua Bengio'], 'title': 'Generalization in deep learning', 'venue': 'arXiv preprint arXiv:1710.05468,', 'year': 2017}, {'authors': ['Yann LeCun', 'Corinna Cortes', 'Christopher JC Burges'], 'title': 'The mnist database of handwritten digits', 'year': 1998}, {'authors': ['Colin McDiarmid'], 'title': 'On the method of bounded differences', 'venue': 'Surveys in combinatorics,', 'year': 1989}, {'authors': ['Nicolas Papernot', 'Patrick McDaniel', 'Xi Wu', 'Somesh Jha', 'Ananthram Swami'], 'title': 'Distillation as a defense to adversarial perturbations against deep neural networks', 'venue': 'In Security and Privacy (SP),', 'year': 2016}, {'authors': ['Uri Shaham', 'Yutaro Yamada', 'Sahand Negahban'], 'title': 'Understanding adversarial training: Increasing local stability of neural nets through robust optimization', 'venue': 'arXiv preprint arXiv:1511.05432,', 'year': 2015}, {'authors': ['Nitish Srivastava', 'Geoffrey Hinton', 'Alex Krizhevsky', 'Ilya Sutskever', 'Ruslan Salakhutdinov'], 'title': 'Dropout: A simple way to prevent neural networks from overfitting', 'venue': 'The Journal of Machine Learning Research,', 'year': 1929}, {'authors': ['Thilo Strauss', 'Markus Hanselmann', 'Andrej Junginger', 'Holger Ulmer'], 'title': 'Ensemble methods as a defense to adversarial perturbations against deep neural networks', 'venue': 'arXiv preprint arXiv:1709.03423,', 'year': 2017}, {'authors': ['Christian Szegedy', 'Wojciech Zaremba', 'Ilya Sutskever', 'Joan Bruna', 'Dumitru Erhan', 'Ian Goodfellow', 'Rob Fergus'], 'title': 'Intriguing properties of neural networks', 'venue': 'In International Conference on Learning Representations,', 'year': 2014}, {'authors': ['Stefan Wager', 'Sida Wang', 'Percy S Liang'], 'title': 'Dropout training as adaptive regularization', 'venue': 'In Advances in Neural Information Processing Systems,', 'year': 2013}, {'authors': ['Huan Xu', 'Shie Mannor'], 'title': 'Robustness and generalization', 'venue': 'Machine learning,', 'year': 2012}, {'authors': ['Huan Xu', 'Constantine Caramanis', 'Shie Mannor'], 'title': 'Robust regression and lasso', 'venue': 'In Advances in Neural Information Processing Systems,', 'year': 2009}, {'authors': ['Huan Xu', 'Constantine Caramanis', 'Shie Mannor'], 'title': 'Robustness and regularization of support vector machines', 'venue': 'The Journal of Machine Learning Research,', 'year': 2009}, {'authors': ['Chiyuan Zhang', 'Samy Bengio', 'Moritz Hardt', 'Benjamin Recht', 'Oriol Vinyals'], 'title': 'Understanding deep learning requires rethinking generalization', 'venue': 'International Conference on Learning Representations,', 'year': 2016}]
sections: [{'heading': '1 Introduction', 'text': 'Deep Neural Networks (DNNs) have been successfully applied in many artificial intelligence tasks, providing state-of-the-art performance and a remarkably small generalization error. On the other hand, DNNs often have far more trainable model parameters than the number of samples they are trained on and were shown to have a large enough capacity to memorize the training data (Zhang et al., 2016). Thus, most statistical learning theory that explains generalization via hypothesis capacity struggle to explain the generalization ability of large artificial neural networks.\nIn this work, we focus on a different approach to study generalization of DNNs, i.e., the connection between the robustness of a deep learning algorithm and its ability to generalize. Xu & Mannor have shown that if an algorithm is robust (i.e., its empirical loss does not change dramatically for perturbed samples), its generalization performance can also be guaranteed. However, in the context of DNNs, practitioners observe contradicting evidence between these two attributes. On the one hand, DNNs generalize well, and on the other, they are fragile to adversarial perturbation on the inputs (Szegedy et al., 2014; Goodfellow et al., 2015). Nevertheless, adversarial training (methods based on generating adversarial examples to training examples and using them during training) have been shown to improve the generalization of deep neural network models (Szegedy et al., 2014; Goodfellow et al., 2015; Shaham et al., 2015), indicating an implicit connection between the robustness of a neural net and its ability to generalize. Moreover, it was observed that dropout, coupled with adversarial training, is best at hindering memorization without reducing the model’s ability to learn (Arpit et al., 2017).\nIn order to solve this contradiction, we revisit the robustness argument in (Xu & Mannor, 2012) and present ensemble robustness, to characterize the generalization performance of deep learning algorithms. Our proposed approach is not intended to give tight performance guarantees for general deep learning algorithms, but rather to pave a way for addressing the question: how can deep learning perform so well while being fragile to adversarial examples? Answering this question is difficult, yet we present evidence\nar X\niv :1\n60 2.\n02 38\n9v 4\n[ cs\n.L G\n] 5\nN ov\n2 01\n7\nin both theory and simulation strongly suggesting that ensemble robustness is crucial to the generalization performance of deep learning algorithms.\nEnsemble robustness concerns the fact that a randomized algorithm (e.g., Stochastic Gradient Descent (SGD), Dropout (Srivastava et al., 2014), Bayes-by-backprop (Blundell et al., 2015), etc.) produces a distribution of hypotheses instead of a deterministic one. Therefore, ensemble robustness takes into consideration robustness of the population of the hypotheses: even though some hypotheses may be sensitive to perturbation on inputs, an algorithm can still generalize well as long as most of the hypotheses sampled from the distribution are robust on average. Kawaguchi et al. (2017) took a different approach and claimed that deep neural networks can generalize well despite nonrobustness. However, our definition of ensemble robustness together with our empirical findings suggest that deep learning methods are typically robust although being fragile to adversarial examples.\nThrough ensemble robustness, we prove that the following holds with a high probability: randomized learning algorithms can generalize well as long as its output hypothesis has bounded sensitiveness to perturbation in average (see Theorem 1). Specified for deep learning algorithms, we reveal that if hypotheses from different runs of a deep learning method perform consistently well in terms of robustness, the performance of such deep learning method can be confidently expected. Moreover, each hypothesis may be sensitive to some adversarial examples as long as it is robust on average.\nAlthough ensemble robustness may be difficult to compute analytically, we demonstrate an empirical estimate of ensemble robustness and investigate the role of ensemble robustness via extensive simulations. The results provide supporting evidence for our claim: ensemble robustness consistently explains the generalization performance of deep neural networks. Furthermore, ensemble robustness is measured solely on training data, potentially allowing one to use the testing examples for training and selecting the best model based on its ensemble robustness.'}, {'heading': '2 Related Works', 'text': 'Xu et al.( 2012) proposed to consider model robustness for estimating generalization performance for deterministic algorithms, such as for SVM (Xu et al., 2009b) and Lasso (Xu et al., 2009a). They suggest using robust optimization to construct learning algorithms, i.e., minimizing the empirical loss with respect to the adversarial perturbed training examples.\nIntroducing stochasticity to deep learning algorithms has achieved great success in practice and also receives theoretical investigation. Hardt et al. (2015) analyzed the stability property of SGD methods, and Dropout (Srivastava et al., 2014) was introduced as a way to control over-fitting by randomly omitting subsets of features at each iteration of a training procedure. Different explanations for the empirical success of dropout have been proposed, including, avoiding over-fitting as a regularization method (Baldi & Sadowski, 2013; Wager et al., 2013; Jain et al., 2015) and explaining dropout as a Bayesian approximation for a Gaussian process (Gal & Ghahramani, 2015). Different from those works, this work will extend the results in (Xu & Mannor, 2012) to randomized algorithms, in order to analyze them from an ensemble robustness perspective.\nAdversarial examples for deep neural networks were first introduced in (Szegedy et al., 2014), while some recent works propose to utilize them as a regularization technique for training deep models (Goodfellow et al., 2015; Gu & Rigazio, 2014; Shaham et al., 2015). However, all of those works attempt to find the “worst case” examples in a local neighborhood of the original training data and are not focused on measuring the global robustness of an algorithm nor on studying the connection between robustness and generalization.'}, {'heading': '3 Preliminaries', 'text': 'In this work, we investigate the generalization property of stochastic learning algorithms in deep neural networks, by establishing their PAC bounds. In this section, we provide some preliminary facts that are necessary for developing the approach of ensemble robustness. After introducing the problem setup we are interested in, we in particular highlight the inherent randomness of deep learning algorithms and give\na formal description of randomized learning algorithms. Then, we briefly review the relationship between robustness and generalization performance established in (Xu & Mannor, 2012).\nProblem setup We now introduce the learning setup for deep neural networks, which follows a standard one for supervised learning. More concretely, we have Z and H as the sample set and the hypothesis set respectively. The training sample set s = {s1, . . . , sn} consists of n i.i.d. samples generated by an unknown distribution µ, and the target of learning is to obtain a neural network that minimizes expected classification error over the i.i.d. samples from µ. Throughout the paper, we consider the training set s with a fixed size of n.\nWe denote the learning algorithm as A, which is a mapping from Zn to H. We use A : s → hs to denote the learned hypothesis given the training set s. We consider the loss function `(h, z) whose value is nonnegative and upper bounded by M . Let L(·) and `emp(·) denote the expected error and the training error for a learned hypothesis hs, i.e.,\nL(hs) , Ez∼µ`(hs, z), and `emp(hs) , 1\nn ∑ si∈s `(hs, si). (1)\nWe are going to characterize the generalization error |L(hs)− `emp(hs)| of deep learning algorithms in the following section.\nRandomized algorithms Most of modern deep learning algorithms are in essence randomized ones, which map a training set s to a distribution of hypotheses ∆(H) instead of a single hypothesis. For example, running a deep learning algorithm A with dropout for multiple times will produce different hypotheses which can be deemed as samples from the distribution ∆(H). This is an important observation we make for deep learning analysis in this work, and we will point out such randomness actually plays an important role for deep learning algorithms to perform well. Therefore, before proceeding to analyze the performance of deep learning, we provide a formal definition of randomized learning algorithms here.\nDefinition 1 (Randomized Algorithms). A randomized learning algorithm A is a function from Zn to a set of distributions of hypotheses ∆(H), which outputs a hypothesis hs ∼ ∆(H) with a probability πs(h).\nWhen learning with a randomized algorithm, the target is to minimize the expected empirical loss for a specific output hypothesis hs, similar to the ones in (1). Here ` is the loss incurred by a specific output hypothesis by one instantiation of the randomized algorithm A.\nExamples of the internal randomness of a deep learning algorithm A include dropout rate (the parameter for a Bernoulli distribution for randomly masking certain neurons), random shuffle among training samples in SGD, the initialization of weights for different layers, to name a few.\nRobustness and generalization Xu & Mannor (2012) established the relation between algorithmic robustness and generalization for the first time. An algorithm is robust if the following holds: if two samples are close to each other, their associated losses are also close. For being self-contained, we here briefly review the algorithmic robustness and its induced generalization guarantee.\nDefinition 2 (Robustness, Xu & Mannor (2012)). Algorithm A is (K, (·)) robust, for K ∈ N and (·) : Zn → R, if Z can be partitioned into K disjoint sets, denoted by {Ci}Ki=1, such that the following holds for all s ∈ Zn:\n∀s ∈ s,∀z ∈ Z,∀i = 1, . . . ,K : if s, z ∈ Ci, then |`(As, s)− `(As, z)| ≤ (n).\nBased on the above robustness property of algorithms, Xu et al. (Xu & Mannor, 2012) prove that a robust algorithm also generalizes well. Motivated by their results, Shaham et al. (Shaham et al., 2015) proposed adversarial training algorithm to minimize the empirical loss over synthesized adversarial examples. However, those results cannot be applied for characterizing the performance of modern deep learning models well.'}, {'heading': '4 Ensemble Robustness', 'text': 'In order to explain the good performance of deep learning, one needs to understand the internal randomness of deep learning algorithms and the population performance of the multiple possible hypotheses. Intuitively, a single output hypothesis cannot be robust to adversarial perturbation on training samples and the deterministic robustness argument in (Xu & Mannor, 2012) cannot be applied here. Fortunately, deep learning algorithms generally output the hypothesis sampled from a distribution of hypotheses. Therefore, even if some samples are not ”nice” for one specific hypothesis, they aren’t likely to fail most of the hypothesis from the produced distribution. Thus, deep learning algorithms are able to generalize well. Such intuition motivates us to introduce the concept of ensemble robustness that is defined over the distribution of output hypotheses of a deep learning algorithm.\nDefinition 3 (Ensemble Robustness). A randomized algorithm A is (K, ̄(n)) ensemble robust, for K ∈ N and (n), if Z can be partitioned into K disjoint sets, denoted by {Ci}Ki=1, such that the following holds for all s ∈ Zn:\n∀s ∈ s,∀i = 1, . . . ,K : if s ∈ Ci, then EAmax\nz∈Ci |`(As, s)− `(As, z)| ≤ ̄(n).\nHere the expectation is taken w.r.t. the internal randomness of the algorithm A.\nEnsemble robustness is a “weaker” requirement for the model compared with the robustness proposed in (Xu & Mannor, 2012) and it fits better for explaining deep learning. In the following section, we demonstrate through simulations that a deep learning model is not robust but it is indeed ensemble robust. So in practice the deep model can still achieve good generalization performance.\nAn algorithm with strong ensemble robustness can provide good generalization performance in expectation w.r.t. the generated hypothesis, as stated in the following theorem. We note that the proofs for all the theorems that we present in this section can be found supplementary material. In addition, the supplementary material holds an additional proof for the special case of Dropout.\nTheorem 1. Let A be a randomized algorithm with (K, ̄(n)) ensemble robustness over the training set s, with |s| = n. Let ∆(H)← A : s denote the output hypothesis distribution of A. Then for any δ > 0, with probability at least 1− δ with respect to the random draw of the s and h ∼ ∆(H), the following holds:\n|L(h)− `emp(h)| ≤ √ nM̄(n) + 2M2\nδn .\nNote that in the above theorem, we hide the dependency of the generalization bound on K in ensemble robustness measure (n). Due to space limitations, all the technical lemmas and details of the proofs throughout the paper are deferred to supplementary material. Theorem 1 leads to following corollary which gives a way to minimize expected loss directly.\nCorollary 1. Let A be a randomized algorithm with (K, ̄(n)) ensemble robustness. Let C1, . . . , CK be a partition of Z, and write z1 ∼ z2 if z1, z2 fall into the same Ck. If the training sample s is generated by i.i.d. draws from µ, then with probability at least 1− δ, the following holds over h ∈ H\nL(h) ≤ 1 n n∑ i=1 max zi∼si `(h, zi) +\n√ nM̄(n) + 2M2\nδn .\nCorollary 1 suggests that one can minimize the expected error of a deep learning algorithm effectively through minimizing the empirical error over the training samples si perturbed in an adversarial way. In fact, such an adversarial training strategy has been exploited in (Goodfellow et al., 2015; Shaham et al., 2015).\nTheorem 2. Let A be a randomized algorithm with (K, ̄(n)) ensemble robustness over the training set s, where |s| = n. Let ∆(H) denote the output hypothesis distribution of the algorithm A on the training set s. Suppose following variance bound holds:\nvarA [ max z∼si |`(As, si)− `(As, z)| ] ≤ α\nThen for any δ > 0, with probability at least 1− δ with respect to the random draw of the s and h ∼ ∆(H), we have\n|L(As)− `emp(As)| ≤ ̄(n) + 1√ 2δ α+M\n√ 2K ln 2 + 2 ln(1/δ)\nn\nTheorem 2 suggests that controlling the variance of the deep learning model can substantially improve the generalization performance. Note that here we need to consider the trade-off between the expectation and variance of ensemble robustness. To see this, consider following two extreme examples. When α = 0, we do not allow any variance in the output of the algorithm A. Thus, A reduces to a deterministic one. To achieve the above upper bound, it is required that the output hypothesis satisfies maxz∈Ci |`(h, si) − `(h, z)| ≤ (n). However, due to the intriguing property of deep neural networks (Szegedy et al., 2014), the deterministic model robustness measure (n) (ref. Definition 2) is usually large. In contrast, when the hypotheses variance α can be large enough, there are multiple possible output hypotheses from the distribution ∆(H). We fix the partition of Z as C1, . . . , CK . Then,\nEA[ max z∼s∈s∩Ci |`(h, s)− `(h, z)|] = ∑\nj∈∆(H)\nP{h = hj} max z∼s∈s∩Ci |`(hj , s)− `(hj , z)|\n≤ ∑\nj∈∆(H)\nP{h = hj} max z∼s∈s∩Ci max h∈∆H |`(h, s)− `(h, z)|\n≤ max z∼s∈s∩Ci max h∈∆(H) |`(h, s)− `(h, z)|.\nTherefore, allowing certain variance on produced hypotheses, a randomized algorithm can tolerate the non-robustness of some hypotheses to certain samples. As long as the ensemble robustness is small, the algorithm can still perform well.'}, {'heading': '5 Simulations', 'text': 'This section is devoted to simulations for quantitatively and qualitatively demonstrating how ensemble robustness of a deep learning method explains its performance. We first introduce our experiment settings and implementation details.'}, {'heading': '5.1 Experiment Settings', 'text': 'Data sets We conduct simulations on two benchmarks. MNIST, a dataset of handwritten digit images (28x28) with 50,000 training samples and 10,000 test samples (LeCun et al., 1998). NotMNIST1, a ”mnist like database” containing font glyphs for the letters A through J (10 classes). The training set contains 367,440 samples and 18,724 testing examples. The images (for both data sets) were scaled such that each pixel is in the range [0, 1]. We note that we did not use the cross-validation data.\nNetwork architecture and parameter setting Without explicit explanation, we use multi-layer perceptrons throughout the simulations. All networks we examined are composed of three fully connected layers, each of which is followed by a rectified linear unit on top. The output of the last fully-connected layer is fed to a 10-way softmax. In order to avoid the bias brought by specific network architecture on our observations, we sample at random the number of units in each layer (uniformly over {400, 800, 1200} units) and the learning rate (uniformly over [0.005, 0.05]). Finally, we used a mini-batch of 128 training examples at a time.\nCompared algorithms We evaluate and compare ensemble robustness as well as the generalization performance for following 4 deep learning algorithms. (1) Explicit ensembles, i.e., using a stochastic algorithm to train different members in the ensemble by running the algorithm multiple times with different seeds. In practice, this was implemented using SGD as the stochastic algorithm, trained to minimize the cross-entropy loss. (2) Implicit ensembles, i.e., learning a probability distribution on the weights of a neural network and sampling ensemble members from it. This was implemented with the Bayes-by-backprop (Blundell et al., 2015) algorithm, a recent approach for training Bayesian Neural\n1http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html\nNetworks. It uses backpropagation to learn a probability distribution on the weights of a neural network by minimising the expected lower bound on the marginal likelihood (or the variational free energy). Methods 3 and 4 correspond for adding adversarial training (Szegedy et al., 2014; Goodfellow et al., 2015; Shaham et al., 2015) to the ensemble methods, where the magnitude of perturbation is measured by its `2 norm and is sampled uniformly over {0.1, 0.3, 0.5} to avoid sampling bias. From now on, a specific configuration will refer to a unique set of these parameters (algorithm type, network width, learning rate and perturbation norm).'}, {'heading': '5.2 Empirical Ensemble Robustness and Generalization', 'text': 'We now present simulations that empirically validate Theorem 1, i.e., that the ensemble robustness of a DNN (measured on the training set) is highly correlated with its generalization performance. As ensemble robustness involves taking an expectation over all the possible output hypothesis, it is computationally intractable to exactly measure ensemble robustness for different deep learning algorithms. In this simulation, we take the empirical average of robustness to adversarial perturbation from 5 different hypotheses of the same learning algorithm as its ensemble robustness. In the case of the SGD variants, for each configuration, we collect an ensemble of output hypotheses by repeating the training procedures using the same configuration while using different random seeds. In the case of the Bayes-by-backprop methods, the algorithm explicitly outputs a distribution over output hypothesis, so we simply sample the networks from the learned weight distribution.\nIn particular, we aim to empirically demonstrate that a deep learning algorithm with stronger ensemble robustness presents better generalization performance (Theorem 1). Recall the definition of ensemble robustness in Definition 3, another obstacle in calculating ensemble robustness is to find the most adversarial perturbation ∆s (or equivalently the most adversarial example z = s + ∆s) for a specific training sample s ∈ s within a partition set Ci. We therefore employ an approximate search strategy for finding the adversarial examples. More concretely, we optimize the following first-order Taylor expansion of the loss function as a surrogate for finding the adversarial example:\n∆si ∈ arg max ‖∆si‖≤r `(si) + 〈∇`si(s),∆si〉, (2)\nwith a pre-defined magnitude constraint r on the perturbation ∆si. In the simulations, we vary the magnitude r in order to calculate the empirical ensemble robustness at different perturbation levels.\nWe then calculate the empirical ensemble robustness by averaging the difference between the loss of the algorithm on the training samples and the adversarial samples output by the method in (2):\n̄emp = 1\nT T∑ t=1 max i∈{1,...,n} |`(A(t)s , si)− `(A(t)s , si + ∆si)|, (3)\nwith T = 5 denoting the size of the ensemble.\nWe emphasize that ̄(n) (Theorem 1) and the empirical approximation ̄(n)emp measure the non robustness of an algorithm, i.e., an algorithm is more robust if ̄(n) is smaller.'}, {'heading': '5.3 Results', 'text': 'The generalization performance of different learning algorithms and different networks compared with the empirical ensemble robustness on MNIST is given in Figure 1. Notice that the x-axis corresponds to the empirical ensemble robustness (Equation 3), and the y-axis corresponds to the test error. Examining Figure 1 we observe a high correlation between ensemble robustness and generalization for all learning algorithms, i.e., algorithms that are more robust (have lower ̄(n)) generalize better on this data set.\nIn addition, adversarial training methods consistently present stronger ensemble robustness (smaller ̄) than pure SGD or Bayes-by-backprop. Figure 2 presents similar results on the notMNIST dataset, although we observe lower (yet positive) correlation for the Bayes-by-backprop algorithm in this case. These observations support our claim on the relation between ensemble robustness and algorithm generalization performance in Theorem 1.\nWe also compare ensemble robustness with robustness on MNIST in Table 1, where robustness is measured similarly to ensemble robustness using Equation 3 but with T = 1 (while T = 5 for ensemble robustness). Indeed, we observe that averaging over instances of the same algorithm, exhibits a higher correlation between generalization and robustness, i.e., ensemble robustness is a better estimation for the generalization performance than standard robustness.'}, {'heading': '6 Conclusions', 'text': 'In this paper, we investigated the generalization ability of stochastic deep learning algorithm based on their ensemble robustness; i.e., the property that if a testing sample is “similar” to a training sample, then its loss is close to the training error. We established both theoretically and experimentally evidence that ensemble robustness of an algorithm, measured on the training set, indicates its generalization performance well. Moreover, our theory and experiments suggest that DNNs may be robust (and generalize) while being fragile to specific adversarial examples. Measuring ensemble robustness of stochastic deep learning algorithms may be computationally prohibitive as one needs to sample several output hypotheses of the algorithm. Thus, we demonstrated that by learning the probability distribution of the weights of a neural network explicitly, e.g., via variational methods such as Bayes-by-backprop, we can still observe a positive correlation between robustness and generalization while using fewer computations, making ensemble robustness feasible to measure.\nAs a direct consequence, one can potentially measure the generalization error of an algorithm without using testing examples. In future work, we plan to further investigate if ensemble robustness can be used for model selection instead of cross-validation (and hence, increasing the training set size), in particular in problems that have a small training set. A different direction is to study the resilience of deep learning methods to adversarial attacks (Papernot et al., 2016). Strauss et al. (2017) recently showed that ensemble methods are useful as a mean to defense against adversarial attacks. However, they only\nconsidered implicit ensemble methods which are computationally prohibitive. As our simulations show that explicit ensembles are robust as well, we believe that they are likely to be a useful defense strategy while reducing computational cost. Finally, Theorem 2 suggests that a randomized algorithm can tolerate the non-robustness of some hypotheses to certain samples; this may help to explain Proposition 1 in Kawaguchi et al. (2017): ”For any dataset, there exist arbitrarily unstable non-robust algorithms such that has a small generalization gap”. We leave this intuition for future work.'}, {'heading': '7 Understanding Dropout via Ensemble Robustness', 'text': 'In this section, we illustrate how ensemble robustness can well characterize the performance of various training strategies of deep learning. In particular, we take the dropout as a concrete example.\nDropout is a widely used technique for optimizing deep neural network models. We demonstrate that dropout is a random scheme to perturb the algorithm. During dropout, at each step, a random fraction of the units are masked out in a round of parameter updating.\nAssumption 1. We assume the randomness of the algorithm A is parametrized by r = (r1, . . . , rL) ∈ R where rl, l = 1, . . . , L are random elements drawn independently.\nFor a deep neural network consisting of L layers, the random variable rl is the dropout randomness for the l-th layer. The next theorem establishes the generalization performance for the neural network with dropout training.\nTheorem 3 (Generalization of Dropout Training). Consider an L-layer neural network trained by dropout. Let A be an algorithm with (K, ̄(n)) ensemble robustness. Let ∆(H) denote the output hypothesis distribution of the randomized algorithm A on a training set s. Assume there exists a β > 0 such that,\nsup r,t sup z∈Z |`(As,r, z)− `(As,t, z)| ≤ β ≤ L−3/4,\nwith r and t only differing in one element. Then for any δ > 0, with probability at least 1− δ with respect to the random draw of the s and h ∼ ∆(H),\nL(hs,r)− `emp(hs,r) ≤ ̄(n) + √ 2 log(1/δ)/L+\n√ 2K ln 2 + 2 ln(2/δ)\nn .\nTheorem 3 also establishes the relation between the depth of a neural network model and the generalization performance. It suggests that when using dropout training, controlling the variance β of the empirical performance over different runs is important: when β converges at the rate of L−3/4, increasing the layer number L will improve the performance of a deep neural network model. However, simply making L larger without controlling β does not help. Therefore, in practice, we usually use voting from multiple models to reduce the variance and thus decrease the generalization error (Hinton et al., 2014). Also, when dropout training is applied for more layers in a neural network model, smaller variance of the model performance is preferred. This can be compensated by increasing the size of training examples or ensemble of multiple models.'}, {'heading': '8 Technical Lemmas', 'text': 'Lemma 1. For a randomized learning algorithm A with (K, ̄(n)) uniform ensemble robustness, and loss function ` such that 0 ≤ `(h, z) ≤M , we have,\nPs { EA|L(h)− `emp(h)| ≤ ̄(n) +M √ 2K ln 2 + 2 ln(1/δ)\nn\n} ≥ 1− δ,\nwhere we use Ps to denote the probability w.r.t. the choice of s, and |s| = n.\nProof. Given a random choice of training set s with cardinality of n, let Ni be the set of index of points of s that fall into the Ci. Note that (|N1|, . . . , |NK |) is an i.i.d. multinomial random variable with parameters n and (µ(C1), . . . , µ(CK)). The following holds by the Breteganolle-Huber-Carol inequality:\nPs { K∑ i=1 ∣∣∣∣ |Ni|n − µ(Ci) ∣∣∣∣ ≥ λ } ≤ 2K exp ( −nλ2 2 ) .\nWe have\nEA|L(As)− `emp(As)|\n= EA ∣∣∣∣∣ K∑ i=1 Ez∼µ(`(As, z)|z ∈ Ci)µ(Ci)− 1 n n∑ i=1 `(As, si) ∣∣∣∣∣ (a) ≤ EA\n∣∣∣∣∣ K∑ i=1 Ez∼µ(`(As, z)|z ∈ Ci) |Ni| n − 1 n n∑ i=1 `(As, si) ∣∣∣∣∣ + EA\n∣∣∣∣∣ K∑ i=1 Ez∼µ(`(As, z)|z ∈ Ci)µ(Ci)− K∑ i=1 Ez∼µ(`(As, z)|z ∈ Ci) |Ni| n ∣∣∣∣∣ (b)\n≤ K∑ i=1 EA ∣∣∣∣∣∣Ez∼µ(`(As, z)|z ∈ Ci) |Ni|n − 1n ∑ j∈Ni `(As, sj) ∣∣∣∣∣∣ + EA\n∣∣∣∣∣ K∑ i=1 Ez∼µ(`(As, z)|z ∈ Ci)µ(Ci)− K∑ i=1 Ez∼µ(`(As, z)|z ∈ Ci) |Ni| n ∣∣∣∣∣ ≤ 1 n K∑ i=1 ∑ j∈Ni EA ( max z∈Ci |`(As, sj)− `(As, z)| ) + max z∈Z |`(As, z)| K∑ i=1 ∣∣∣∣ |Ni|n − µ(Ci) ∣∣∣∣ (4)\n(c) ≤ ̄(n) +M K∑ i=1 ∣∣∣∣ |Ni|n − µ(Ci) ∣∣∣∣\n(d) ≤ ̄(n) +M √ 2K ln 2 + 2 ln(1/δ)\nn (5)\nHere the inequalities (a) and (b) are due to triangle inequality, (c) is from the definition of ensemble robustness and the fact that the loss function is upper bounded by M , and (d) holds with a probability greater than 1− δ.\nLemma 2. For a randomized learning algorithm A with (K, ̄(n)) uniform ensemble robustness, and loss function ` such that 0 ≤ `(h, z) ≤M , we have,\nEs|L(h)− `emp(h)|2 ≤M̄(n) + 2M2\nn .\nProof. Let Ni be the set of index of points of s that fall into the Ci. Note that (|N1|, . . . , |NK |) is an i.i.d. multinomial random variable with parameters n and (µ(C1), . . . , µ(CK). Then Es|Nk| = n · µ(Ck)\nfor k = 1, . . . ,K.\nEs|L(h)− `emp(h)|2\n= Es ∣∣∣∣∣Ez∈Z`(h, z)− 1n n∑ i=1 `(h, si) ∣∣∣∣∣ 2\n= Es ∣∣∣∣∣ K∑ k=1 Ez∈Z`(h, z|z ∈ Ck)µ(Ck)− 1 n n∑ i=1 `(h, si) ∣∣∣∣∣ 2\n= ( K∑ k=1 Ez∈Z`(h, z|z ∈ Ck)µ(Ck) )2 + 1 n2 Es ( n∑ i=1 `(h, si) )2\n− 2 ( K∑ k=1 Ez∈Z`(h, z|z ∈ Ck)µ(Ck) ) Es ( 1 n n∑ i=1 `(h, si) )\n≤ ( K∑ k=1 Ez∈Z`(h, z|z ∈ Ck)µ(Ck) )∣∣∣∣∣ K∑ k=1 Ez∈Z`(h, z|z ∈ Ck)µ(Ck)− Es 1 n n∑ i=1 `(h, si) ∣∣∣∣∣ + 1\nn2 Es ( n∑ i=1 `(h, si) )2 − ( K∑ k=1 Ez∈Z`(h, z|z ∈ Ck)µ(Ck) ) Es ( 1 n n∑ i=1 `(h, si) )\n≤M ∣∣∣∣∣ K∑ k=1 Ez∈Z`(h, z|z ∈ Ck)µ(Ck)− Es 1 n n∑ i=1 `(h, si) ∣∣∣∣∣︸ ︷︷ ︸ H + 2M2 n\nWe then bound the term H as follows.\nH = ∣∣∣∣∣ K∑ k=1 Ez∈Z`(h, z|z ∈ Ck)Es Nk n − Es 1 n n∑ i=1 `(h, si) ∣∣∣∣∣ ≤ 1 n Es ∣∣∣∣∣∣ K∑ k=1 Ez∈Z`(h, z|z ∈ Ck)Nk − ∑ j∈Ck `(h, sj)\n∣∣∣∣∣∣ ≤ 1 n Es K∑ k=1 Nk max sj∈Ck,z∈Ck |`(h, z)− `(h, sj)|\n= 1\nn K∑ k=1 NkEs max sj∈Ck,z∈Ck |`(h, z)− `(h, sj)|\n≤ ̄(n).\nThen we have,\nEs|L(h)− `emp(h)|2 ≤M̄(n) + 2M2\nn .\nTo analyze the generalization performance of deep learning with dropout, following lemma is central.\nLemma 3 (Bounded difference inequality (McDiarmid, 1989)). Let r = (r1, . . . , rL) ∈ R be L independent random variables (rl can be vectors or scalars) with rl ∈ {0, 1}ml . Assume that the function f : RL → R satisfies:\nsup r(l) ,̃r(l) ∣∣∣f(r(l))− f(r̃(l))∣∣∣ ≤ cl,∀l = 1, . . . , L, whenever r(l) and r̃(l) differ only in the l-th element. Here, cl is a nonnegative function of l. Then, for every > 0,\nPr {f(r1, . . . , rL)− Erf(r1, . . . , rL) ≥ }\n≤ exp ( −2 2/\nL∑ l=1 c2l\n) .'}, {'heading': '9 Proof of Theorem 1', 'text': 'Proof of Theorem 1. Now we proceed to prove Theorem 1. Using Chebyshev’s inequality, Lemma 2 leads to the following inequality:\nPrs {|L(h)− `emp(h)| ≥ |h} ≤ nMEs maxs∈s,z∼s |`(h, s)− `(h, z)|+ 2M2\nn 2 .\nBy integrating with respect to h, we can derive the following bound on the generalization error:\nPrs,A {|L(h)− `emp(h)| ≥ } ≤ nMEA,s maxs∈s,z∼s |`(h, s)− `(h, z)|+ 2M2\nn 2 .\nThis is equivalent to:\n|L(h)− `emp(h)| ≤ √ nM̄(n) + 2M2\nδn\nholds with a probability greater than 1− δ.'}, {'heading': '10 Proof of Theorem 2', 'text': 'Proof. To simplify the notations, we use X(h) to denote the random variable maxz∼s |`(h, s)− `(h, z)|. According to the definition of ensemble robustness, we have EAX(h) ≤ (n). Also, the assumption gives var[X(h)] ≤ α. According to Chebyshev’s inequality, we have,\nP { X(h) ≤ (n) + α√\nδ\n} ≥ 1− δ.\nNow, we proceed to bound |L(h)− `emp(h)| for any h ∼ ∆(H) output by As. Following the proof of Lemma 2, we also divide the set Z into K disjoint set C1, . . . , CK and let Ni be the set of index of points in ∫ that fall into Ci. Then we have,\n|L(h)− `emp(h)|\n≤ 1 n K∑ i=1 ∑ j∈Ni max z∈Ci |`(h, sj)− `(h, z)|+ √ 2K ln 2 + 2 ln(1/δ) n\n≤ (n) + α√ δ +\n√ 2K ln 2 + 2 ln(1/δ)\nn\nholds with probability at least 1− 2δ. Let δ be 2δ, we have,\n|L(h)− `emp(h)| ≤ (n) + α√ 2δ +\n√ 2K ln 2 + 2 ln(1/2δ)\nn\nholds with probability at least 1− δ. This gives the first inequality in the theorem. The second inequality can be straightforwardly derived from the fact that var(X) = E[X2]− (E[X])2 ≤ME[X]− (E[X])2.'}, {'heading': '11 Proof of Theorem 3', 'text': 'Proof. Let R(s, r) = L(As,r)− `emp(As,r) denote the random variable that we are going to bound. For every r, t ∈ RL, and L ∈ N, we have\n|R(s, r)−R(s, t)|\n= ∣∣∣∣∣Ez∈Z [`(As,r, z)− `(As,t, z)]− 1n n∑ i=1 (`(As,r, zi)− `(As,t, zi)) ∣∣∣∣∣ ≤ Ez∈Z |`(As,r, z)− `(As,t, z)|+ 1\nn n∑ i=1 |`(As,r, zi)− `(As,t, zi)| .\nAccording to the definition of β: sup r,t |R(s, r)−R(s, t)| ≤ 2β,\nand applying Lemma 3 we obtain (note that s is independent of r) Pr {R(s, r)− ErR(s, r) ≥ |s} ≤ exp ( − 2\n2Lβ2\n) .\nWe also have EsPr {R(s, r)− ErR(s, r) ≥ } = EsPr {R(s, r)− ErR(s, r) ≥ |s} ≤ exp ( − 2\n2Lβ2\n) .\nSetting the r.h.s. equal to δ and writing as a function of δ, we have that with probability at least 1− δ w.r.t. the random sampling of s and r:\nR(s, r)− ErR(s, r) ≤ β √ 2L log(1/δ).\nThen according to Lemma 1: ErR(s, r) ≤ ̄(n) + √ 2K ln 2 + 2 ln(1/δ)\nn\nholds with probability greater than 1− δ. Observe that the above two inequalities hold simultaneously with probability at least 1− 2δ. Combining those inequalities and setting δ = δ/2 gives\nR(s, r) ≤ β √ 2L log(1/δ) + ̄(n) +\n√ 2K ln 2 + 2 ln(2/δ)\nn .'}]
