id: SP:339df4c0aceaad36eb15d03f1f116086485257bd
title: Texture Synthesis Through Convolutional Neural Networks and Spectrum Constraints
authors: [{'affiliations': [], 'name': 'Gang Liu'}, {'affiliations': [], 'name': 'Yann Gousseau'}, {'affiliations': [], 'name': 'Gui-Song Xia'}]
abstractText: This paper presents a significant improvement for the synthesis of texture images using convolutional neural networks (CNNs), making use of constraints on the Fourier spectrum of the results. More precisely, the texture synthesis is regarded as a constrained optimization problem, with constraints conditioning both the Fourier spectrum and statistical features learned by CNNs. In contrast with existing methods, the presented method inherits from previous CNN approaches the ability to depict local structures and fine scale details, and at the same time yields coherent large scale structures, even in the case of quasi-periodic images. This is done at no extra computational cost. Synthesis experiments on various images show a clear improvement compared to a recent state-of-the art method relying on CNN constraints only.
references: [{'authors': ['G.R. Cross', 'A.K. Jain'], 'title': 'Markov random field texture models', 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence, no. 1, pp. 25–39, 1983.', 'year': 1983}, {'authors': ['B. Julesz'], 'title': 'Visual pattern discrimination', 'venue': 'IRE Transactions on Information Theory, vol. 8, no. 2, pp. 84–92, 1962.', 'year': 1962}, {'authors': ['D.J. Heeger', 'J.R. Bergen'], 'title': 'Pyramid-based texture analysis/synthesis', 'venue': 'Proceedings of the 22nd annual conference on Computer graphics and interactive techniques, pp. 229–238, ACM, 1995.', 'year': 1995}, {'authors': ['J. Portilla', 'E.P. Simoncelli'], 'title': 'Aparametric texture model based on joint statistics of complex wavelet coefficients', 'venue': 'International Journal of Computer Vision, vol. 40, no. 1, pp. 49–71, 2000.', 'year': 2000}, {'authors': ['B. Galerne', 'Y. Gousseau', 'J.-M. Morel'], 'title': 'Random phase textures: Theory and synthesis', 'venue': 'IEEE Transactions on Image Processing, vol. 20, no. 1, pp. 257–267, 2011.', 'year': 2011}, {'authors': ['G.-S. Xia', 'S. Ferradans', 'G. Peyré', 'J. Aujol'], 'title': 'Synthesizing and mixing stationary gaussian texture models', 'venue': 'SIAM J. Imaging Sciences, vol. 7, no. 1, pp. 476–508, 2014.', 'year': 2014}, {'authors': ['G. Tartavel', 'Y. Gousseau', 'G. Peyré'], 'title': 'Variational Texture Synthesis with Sparsity and Spectrum Constraints', 'venue': 'Journal of Mathematical Imaging and Vision, vol. 52, pp. 124–144, may 2015.', 'year': 2015}, {'authors': ['A.A. Efros', 'T.K. Leung'], 'title': 'Texture synthesis by non-parametric sampling', 'venue': 'The Proceedings of the Seventh IEEE International Conference on Computer Vision, vol. 2, no. September, pp. 1033–1038, 1999.', 'year': 1999}, {'authors': ['L.-Y. Wei', 'M. Levoy'], 'title': 'Fast texture synthesis using tree-structured vector quantization', 'venue': 'Proceedings of the 27th annual conference on Computer graphics and interactive techniques, pp. 479–488, 2000.', 'year': 2000}, {'authors': ['S. Lefebvre', 'H. Hoppe'], 'title': 'Parallel controllable texture synthesis', 'venue': 'ACM Transactions on Graphics, vol. 24, no. 3, p. 777, 2005.', 'year': 2005}, {'authors': ['C. Aguerrebere', 'Y. Gousseau', 'G. Tartavel'], 'title': 'Exemplar-based texture synthesis: the efros-leung algorithm', 'venue': 'Image Processing On Line, vol. 2013, pp. 223–241, 2013.', 'year': 2013}, {'authors': ['L. Raad', 'A. Desolneux', 'J.-M. Morel'], 'title': 'Conditional gaussian models for texture synthesis', 'venue': 'Scale Space and Variational Methods in Computer Vision, pp. 474–485, Springer, 2015.', 'year': 2015}, {'authors': ['J. Dai', 'Y. Lu', 'Y.-N. Wu'], 'title': 'Generative Modeling of Convolutional Neural Networks', 'venue': 'arxiv, dec 2014.', 'year': 2014}, {'authors': ['L. Gatys', 'A.S. Ecker', 'M. Bethge'], 'title': 'Texture synthesis using convolutional neural networks', 'venue': 'Advances in Neural Information Processing Systems, pp. 262–270, 2015.', 'year': 2015}, {'authors': ['Y. Lu', 'S.-c. Zhu', 'Y.N. Wu'], 'title': 'Exploring Generative Perspective of Convolutional Neural Networks by Learning Random Field Models.', 'year': 2016}, {'authors': ['D. Ulyanov', 'V. Lebedev', 'A. Vedaldi', 'V. Lempitsky'], 'title': 'Texture Networks: Feed-forward Synthesis of Textures and Stylized Images', 'venue': 'tech. rep., mar 2016.', 'year': 2016}, {'authors': ['B. Galerne', 'Y. Gousseau', 'J.-M. Morel'], 'title': 'Micro-texture synthesis by phase randomization', 'venue': 'Image Processing On Line, vol. 1, 2011.', 'year': 2011}, {'authors': ['K. Simonyan', 'A. Zisserman'], 'title': 'Very deep convolutional networks for large-scale image recognition', 'venue': 'arXiv preprint arXiv:1409.1556, 2014.', 'year': 2014}, {'authors': ['M. Cimpoi', 'S. Maji', 'A. Vedaldi'], 'title': 'Deep filter banks for texture recognition and segmentation', 'venue': 'Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3828–3836, 2015.', 'year': 2015}, {'authors': ['C. Zhu', 'R.H. Byrd', 'P. Lu', 'J. Nocedal'], 'title': 'L-bfgs-b: Fortran subroutines for large scale bound constrained optimization', 'venue': 'Report NAM-11, EECS Department, Northwestern University, 1994.', 'year': 1994}]
sections: [{'text': 'I. INTRODUCTION\nA large body of works has been dedicated over the last 30 years to the task of texture synthesis and more precisely to the task of generating new but perceptually similar images from a given exemplar. Parametric Markov models were among the first to be investigated for such tasks [1]. Then, many methods were developed relying on the idea, inspired by the works of Julesz [2], that realistic textures could be obtained by constraining a well chosen set of image statistics: wavelet marginals [3], joint statistics of wavelet coefficients [4], Fourier spectrum [5], [6] or the distribution of sparse representation coefficients [7]. A completely different approach is provided by patch-based, non-parametric methods that have been largely studied after the initial works of [8] and [9]. Here, the basic idea is to generate a new texture by iteratively sampling patches from the exemplar, see e.g. [10] for a review. These methods enable realistic results even on highly structured textures, at the price of very limited innovation capacity, see e.g. [11]. Recently, the work from [12] renewed this type of methods by promoting the use of parametric distributions for patches.\nRecently, several synthesis methods based on convolutional neural networks (CNNs) have been proposed and shown to yield state-of-the art results [13]–[16]. Indeed, these methods are able to generate perceptually satisfying results on complex textures without producing piecewise recopy of the exemplar. The idea, in order to generate new textures, is to constrain some statistics of convolutional networks initially introduced for image classification. In [14], [16], a non-parametric approach is proposed through the correlation matrices of filter responses (named feature maps), as we will see in more details later. In [13], [15], a MRF model is learned from the feature\nmaps. One limitation of these approaches, however, is the difficulty to efficiently account for large scale regularity, see e.g. Fig. 1 for an illustration.\nIn this work, we propose to incorporate some low frequency constraints into the CNN approach, in order to allow the synthesis of textures having large scale regularity. In particular, this will allow the synthesis of quasi-periodic textures such as brick wall or object alignments. In order to do this, we draw on previous works on texture synthesis [5], [17] that have shown the interest and easiness of using spectrum constraints for synthesizing textures. We therefore combine spectrum and feature maps constraint through the definition of a new loss function, allowing the reproduction of both fine scale details and large scale regular structures. The combination of these two types of constraints is illustrated in Fig. 1.\nThe remainder of the paper is organized as follows. In Sec.II we briefly recall how to use CNNs to synthesize textures. We then explain how to incorporate frequency control\nar X\niv :1\n60 5.\n01 14\n1v 3\n[ cs\n.C V\n] 1\n9 M\nay 2\n01 6\ninto this framework in Sec.III. Eventually, we experimentally show the interest of the approach in Sec.IV.\nII. TEXTURE SYNTHESIS BY CNNS'}, {'heading': 'A. Convolution neural network', 'text': 'A convolution neural network (CNN) is a feed-forward artificial neural network whose neurons process overlapping regions of the input, generally an image. Parameters of such networks are usually learned using a back-propagated algorithm from a large number of annotated inputs. The network VGG-19, proposed in [18], is one of the CNN trained with the ImageNet dataset and has proven to be efficient in describing texture images with only convolution layers [19]. Neglecting the latter fully connected layers, the network contains 16 linearly rectified convolution layers and 5 pooling layers. Notice that for the task of texture synthesis, according to [14], the max-pooling strategy may be replaced by average-pooling for improving the gradient flow and obtaining slightly cleaner results.'}, {'heading': 'B. Texture model', 'text': 'In order to use CNNs for texture synthesis, it was first proposed in [14] to constrain the statistics of feature maps. Inspired by Portilla and Simoncelli [4], important statistics are provided by correlations between the feature maps corresponding to different filters.\nGiven a texture exemplar I ∈ RN , where N is the number of pixels in the image, we first input I to the CNN to calculate the feature maps at each convolution layer. Suppose there are ml neurons at l-th convolution layer, the extracted feature map is denoted by f l ∈ Rml×Nl , with Nl as the number of stimuli. Note that at the first convolution layer, Nl = N .\nAfter obtaining the feature map f l at the l-th layer, the correlation matrix Gl ∈ Rml×ml [14] is computed as\nGlp,q = Nl∑ i=1 f lp(i) · f lq(i), (1)\nwhere p, q denote the index of feature map corresponding to the filter p, q ∈ {1, · · · ,ml}.\nThe set of correlation matrices {G1, G2, · · · , GL} from the different layers are used to model the texture [14] and to constrain the synthesis of new texture images.'}, {'heading': 'C. Texture generation', 'text': 'As proposed in [14], new texture samples are generated by starting from a white noise image and by iteratively imposing the previously defined correlation matrices, using gradient descent. Fig. 2 shows the flowchart of this algorithm.\nGiven an image Î initialized by white noise, its feature maps f̂ l are computed at each layer of the CNN, as well as the correlation matrix Ĝl. A loss function at layer l is then defined as the difference between the correlation matrices of the generated image and the reference one:\nEl = 1\n4N2l m 2 l ml∑ p=1 ml∑ q=1 ‖Glp,q − Ĝlp,q‖2. (2)\nThe derivative of this loss function is computed as\n∂El\n∂f̂ lp(i) =\n1\nN2l m 2 l ml∑ q=1 f lq(i) · (Glp,q − Ĝlp,q). (3)\nNote that this equation does not include a positive part as in [14], because the feature maps are always positive after the rectified linear transform. Combining these layers, a total loss function is defined as\nLcnn(I, Î) = L∑\nl=0\nwlEl, (4)\nwhere wl denotes the weight of loss at layer l. With the derivative given by Eq.(3), a back-propagation (BP) algorithm can be applied to propagate the error from layer l to layer l − 1, which is illustrated in red line in Fig.2. Then, at each layer, the loss (or error) includes two aspects: the derivative of the current layer by Eq. 3 and the propagated error from the later layer. At any given layer in the network, we can obtain the propagated data error, denoted here by ∆cnn. A solution is then computed using the L-BFGS algorithm [20], from the initialized image Î and the back-propagated error ∆cnn.\nIII. TEXTURE SYNTHESIS BY CNNS WITH SPECTRUM CONSTRAINT\nEven though texture synthesis using CNN yields impressive results in many cases, it still suffers from several shortcomings. One of them is the difficulty to accurately reproduce regularity at large scales, typically low-frequency structures. This is illustrated in Fig.3. In this figure, the left column shows the original images and the right column displays the synthesized images, where large scales are not correctly handled. One possible reason for this is that the size of the convolutional filters in VGG-19 are 3×3×Nl, which are too small to depict the large structures. Another possible reason is that the number of layers of this network does not allow the handling of very large scale. Nevertheless in our experiments, neither increasing the filter size (this was tested using VGG-m [21] and 7 × 7 filters) or increasing the number of layers did solve this issue. In this contribution, we show that enriching the loss function of the CNN with a term constraining the Fourier spectrum of the image solves this issue in many cases.'}, {'heading': 'A. Spectrum constraint', 'text': 'It was shows in [5], [6], [17] that a large set of textures (the so-called micro-textures) may be reproduced simply by imposing the Fourier spectrum of the outputs and letting their Fourier phases be chosen at random. In a different direction, this constraint was used for enabling low frequency structures in sparsity-based texture synthesis [7], in a variational framework. We here follow a similar path by constraining the loss function in the CNN-based approach. In what follows, we write F(I) for the Discrete Fourier Transform (DFT) of an image I and F−1 for the inverse DFT.\nIn order to illustrate the Fourier representation of low frequency, quasi-periodic structures, Fig.4 shows the amplitude of the DFT of two images, from which strong peaks can be observed.\nGiven an image I , we write EI for the set of images having the same spectrum (modulus of the Fourier transform) as I . Simple computations show that the projection of any given image Î on EI (that is, the image from EI being closest to Î) is given by\nĨ = F−1 ( F(Î) · F(I)∗\n|F(Î) · F(I)∗| · F(I)\n) , (5)\nwhere F(I)∗ denotes the conjugate of F(I). For color images, the phase of the gray level image is first computed, and then imposed to each color channel.'}, {'heading': 'B. Texture synthesis by CNNs with spectrum constraint', 'text': 'In order to constrain both the statistics of the feature maps and the spectrum of the results, we add in the loss function of the CNN an additional term, obtained from the distance of the current image Î to the space EI corresponding to some reference texture I . We write d(Î , EI) for this term, then its gradient is simply computed as Î − Ĩ , where Ĩ is computed\nfollowing Eq.(5). Therefore, the gradient term of the CNN is written as\n∆spe = Î − Ĩ , (6)\ncorresponding to the loss function\nLspe = 1\n2 d(Î , EI)2, (7)\nwhere as before I is the reference texture.\nThe final loss function and gradient of the CNN are simply obtained as\nL = LCNN + βLspe, (8) ∆ = ∆CNN + β∆spe. (9)\nThe L-BFGS algorithm is then applied to synthesize a new texture image.\nIV. RESULTS AND ANALYSIS'}, {'heading': 'A. Experimental setup', 'text': 'Following [14], the following layers of the convolution neural network are used: ’Conv1 1’, ’Pooling1’, ’Pooling2’, ’Pooling3’, ’Pooling4’. The corresponding weights are set to be w1 = w2 = w3 = w4 = w5 = 109. For the spectrum constraint, we use β = 105. For the experiments of this paper, we use CG texture samples1 as input. All the images are rescaled first, as in [14].'}, {'heading': 'B. Texture synthesis results', 'text': 'We illustrate the proposed approach by comparing it with the original CNN-based approach from [14]. This last approach is one of the state-of-the art approaches among methods that produce truly new images. In particular, and contrarily to most patch-based methods derived from [8] (see e.g. [11]), these methods (the one from [14], as well as the proposed one) do not produce results containing parts that are verbatim copied from the exemplars.\nIn Fig. 5, 6, 7, the left column displays the given texture exemplars, the middle shows the synthesized results using the CNN-based algorithm [14] and the right illustrates our results by combining the CNN model with a spectrum constraint.\nIn Fig. 5, from top to bottom are images of a checker board, two brick wall and a building with windows. All these have the particularity to present quasi-periodic structures. In the three first examples, we can see that the algorithm from [14] fail to preserve the large scale organization, which is correctly reconstructed when adding the spectrum constraint. The last example is more difficult. Although not totally satisfying, the proposed method increases the regularity of the result.\nFig. 6 shows synthesis results from Zellige tiles, made of complex decorative patterns. Although some defaults may appear in the global organization of the synthesized images, the combined use of Fourier spectrum constraints and CNN permits the reproduction of both the global patterns and the small scale details. Fig. 7 shows an example of failure, where the global structures are too complex to be reproduced by the proposed approach.\nAlthough we do not illustrate it here, it is also worth mentioning that in the cases where the original approach from [14] is sufficient, results are not degraded by adding the spectrum constraint.\nThe proposed approach has roughly the same complexity as the one from [14], the computation of the FFT being neglectable compared to CNN computations. Each experiment displayed in this paper took approximately 15 minutes using a 4 kernel CPU.\nV. CONCLUSION\nThis paper presents an effective improvement for the synthesis of textures using convolutional neural networks (CNNs), by incorporating a spectrum constraint in the loss function of the CNN. The generated texture images not only preserve local structures and fine scale details, but also preserve large\n1http://www.textures.com\nquasi-periodic structures. The experimental results prove that the spectrum constraint is a necessary complement for texture synthesis, especially for structured textures, at no additional computational cost.\nREFERENCES [1] G. R. Cross and A. K. Jain, “Markov random field texture models,”\nIEEE Transactions on Pattern Analysis and Machine Intelligence, no. 1, pp. 25–39, 1983.\n[2] B. Julesz, “Visual pattern discrimination,” IRE Transactions on Information Theory, vol. 8, no. 2, pp. 84–92, 1962.\n[3] D. J. Heeger and J. R. Bergen, “Pyramid-based texture analysis/synthesis,” in Proceedings of the 22nd annual conference on Computer graphics and interactive techniques, pp. 229–238, ACM, 1995.\n[4] J. Portilla and E. P. Simoncelli, “Aparametric texture model based on joint statistics of complex wavelet coefficients,” International Journal of Computer Vision, vol. 40, no. 1, pp. 49–71, 2000.\n[5] B. Galerne, Y. Gousseau, and J.-M. Morel, “Random phase textures: Theory and synthesis,” IEEE Transactions on Image Processing, vol. 20, no. 1, pp. 257–267, 2011.\n[6] G.-S. Xia, S. Ferradans, G. Peyré, and J. Aujol, “Synthesizing and mixing stationary gaussian texture models,” SIAM J. Imaging Sciences, vol. 7, no. 1, pp. 476–508, 2014.\n[7] G. Tartavel, Y. Gousseau, and G. Peyré, “Variational Texture Synthesis with Sparsity and Spectrum Constraints,” Journal of Mathematical Imaging and Vision, vol. 52, pp. 124–144, may 2015.\n[8] A. A. Efros and T. K. Leung, “Texture synthesis by non-parametric sampling,” The Proceedings of the Seventh IEEE International Conference on Computer Vision, vol. 2, no. September, pp. 1033–1038, 1999.\n[9] L.-Y. Wei and M. Levoy, “Fast texture synthesis using tree-structured vector quantization,” Proceedings of the 27th annual conference on Computer graphics and interactive techniques, pp. 479–488, 2000.\n[10] S. Lefebvre and H. Hoppe, “Parallel controllable texture synthesis,” ACM Transactions on Graphics, vol. 24, no. 3, p. 777, 2005.\n[11] C. Aguerrebere, Y. Gousseau, and G. Tartavel, “Exemplar-based texture synthesis: the efros-leung algorithm,” Image Processing On Line, vol. 2013, pp. 223–241, 2013.\n[12] L. Raad, A. Desolneux, and J.-M. Morel, “Conditional gaussian models for texture synthesis,” in Scale Space and Variational Methods in Computer Vision, pp. 474–485, Springer, 2015.\n[13] J. Dai, Y. Lu, and Y.-N. Wu, “Generative Modeling of Convolutional Neural Networks,” arxiv, dec 2014.\n[14] L. Gatys, A. S. Ecker, and M. Bethge, “Texture synthesis using convolutional neural networks,” in Advances in Neural Information Processing Systems, pp. 262–270, 2015.\n[15] Y. Lu, S.-c. Zhu, and Y. N. Wu, “Exploring Generative Perspective of Convolutional Neural Networks by Learning Random Field Models.” 2016.\n[16] D. Ulyanov, V. Lebedev, A. Vedaldi, and V. Lempitsky, “Texture Networks: Feed-forward Synthesis of Textures and Stylized Images,” tech. rep., mar 2016.\n[17] B. Galerne, Y. Gousseau, and J.-M. Morel, “Micro-texture synthesis by phase randomization,” Image Processing On Line, vol. 1, 2011.\n[18] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.\n[19] M. Cimpoi, S. Maji, and A. Vedaldi, “Deep filter banks for texture recognition and segmentation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3828–3836, 2015.\n[20] C. Zhu, R. H. Byrd, P. Lu, and J. Nocedal, “L-bfgs-b: Fortran subroutines for large scale bound constrained optimization,” Report NAM-11, EECS Department, Northwestern University, 1994.\n[21] K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman, “Return of the devil in the details: Delving deep into convolutional nets,” in British Machine Vision Conference, 2014.'}]
