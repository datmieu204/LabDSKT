id: SP:e4de78f22da9da1763b8728f9730d062a9e5e11e
title: STTR: A System for Tracking All Vehicles All the Time At the Edge of the Network
authors: [{'affiliations': [], 'name': 'Zhuangdi Xu'}, {'affiliations': [], 'name': 'Harshit Gupta'}, {'affiliations': [], 'name': 'Umakishore Ramachandran'}]
abstractText: To fully exploit the capabilities of sensors in real life, especially cameras, smart camera surveillance requires the cooperation from both domain experts in computer vision and systems. Existing alert-based smart surveillance is only capable of tracking a limited number of suspicious objects, while in most real-life applications, we often do not know the perpetrator ahead of time for tracking their activities in advance. In this work, we propose a radically different approach to smart surveillance for vehicle tracking. Specifically, we explore a smart camera surveillance system aimed at tracking all vehicles in real time. The insight is not to store the raw videos, but to store the space-time trajectories of the vehicles. Since vehicle tracking is a continuous and geo-distributed task, we assume a geo-distributed Fog computing infrastructure as the execution platform for our system. To bound the storage space for storing the trajectories on each Fog node (serving the computational needs of a camera), we focus on the activities of vehicles in the vicinity of a given camera in a specific geographic region instead of the time dimension, and the fact that every vehicle has a “finite” lifetime. To bound the computational and network communication requirements for detection, re-identification, and inter-node communication, we propose novel techniques, namely, forward and backward propagation that reduces the latency for the operations and the communication overhead. STTR is a system for smart surveillance that we have built embodying these ideas. For evaluation, we develop a toolkit upon SUMO to emulate camera detections from traffic flow and adopt MaxiNet to emulate the fog computing infrastructure on Microsoft Azure.
references: [{'authors': ['Mohammad AAlsmirat', 'Yaser Jararweh', 'Islam Obaidat', 'Brij B Gupta'], 'title': 'Internet of surveillance: a cloud supported large-scale wireless surveillance system', 'venue': 'The Journal of Supercomputing 73,', 'year': 2017}, {'authors': ['Clemens Arth', 'Christian Leistner', 'Horst Bischof'], 'title': 'Object reacquisition and tracking in large-scale smart camera networks', 'venue': 'In Distributed Smart Cameras,', 'year': 2007}, {'authors': ['Flavio Bonomi', 'Rodolfo Milito', 'Jiang Zhu', 'Sateesh Addepalli'], 'title': 'Fog computing and its role in the internet of things', 'venue': 'In Proceedings of the first edition of the MCC workshop on Mobile cloud computing', 'year': 2012}, {'authors': ['Hu Cao', 'OuriWolfson', 'Goce Trajcevski'], 'title': 'Spatio-temporal data reduction with deterministic error bounds', 'venue': 'The VLDB Journal-The International Journal on Very Large Data Bases 15,', 'year': 2006}, {'authors': ['Ching-Tang Fan', 'Yuan-Kai Wang', 'Cai-Ren Huang'], 'title': 'Heterogeneous information fusion and visualization for a large-scale intelligent video surveillance system', 'venue': 'IEEE Transactions on Systems, Man, and Cybernetics: Systems 47,', 'year': 2017}, {'authors': ['Mengran Gou', 'Srikrishna Karanam', 'Wenqian Liu', 'Octavia Camps', 'Richard J Radke'], 'title': 'Dukemtmc4reid: A large-scale multi-camera person re-identification dataset', 'venue': 'In IEEE Conference on Computer Vision and Pattern RecognitionWorkshops', 'year': 2017}, {'authors': ['Marios Hadjieleftheriou', 'George Kollios', 'Vassilis J Tsotras', 'Dimitrios Gunopulos'], 'title': 'Indexing spatiotemporal archives', 'venue': 'The VLDB Journal 15,', 'year': 2006}, {'authors': ['Arun Hampapur', 'Lisa Brown', 'Jonathan Connell', 'Ahmet Ekin', 'Norman Haas', 'Max Lu', 'Hans Merkl', 'Sharath Pankanti'], 'title': 'Smart video surveillance: exploring the concept of multiscale spatiotemporal tracking', 'venue': 'IEEE signal processing magazine 22,', 'year': 2005}, {'authors': ['Arun Hampapur', 'Lisa Brown', 'Jonathan Connell', 'Sharat Pankanti', 'Andrew Senior', 'Yingli Tian'], 'title': 'Smart surveillance: applications, technologies and implications', 'venue': 'In Information, Communications and Signal Processing, 2003 and Fourth Pacific Rim Conference on Multimedia. Proceedings of the 2003 Joint Conference of the Fourth International Conference on,', 'year': 2003}, {'authors': ['Kirak Hong', 'David Lillethun', 'Umakishore Ramachandran', 'Beate Ottenwälder', 'Boris Koldehofe'], 'title': 'Mobile fog: A programming model for large-scale applications on the internet of things', 'venue': 'In Proceedings of the second ACM SIGCOMM workshop on Mobile cloud computing', 'year': 2013}, {'authors': ['Yasutomo Kawanishi', 'Daisuke Deguchi', 'Ichiro Ide', 'Hiroshi Murase'], 'title': 'Trajectory Ensemble: Multiple Persons Consensus Tracking across Non-overlapping Multiple Cameras over Randomly Dropped Camera Networks', 'venue': 'In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops', 'year': 2017}, {'authors': ['Daniel Krajzewicz', 'Jakob Erdmann', 'Michael Behrisch', 'Laura Bieker'], 'title': 'Recent Development and Applications of SUMO - Simulation of Urban MObility', 'venue': 'International Journal On Advances in Systems andMeasurements', 'year': 2012}, {'authors': ['Ralph Lange', 'Frank Dürr', 'Kurt Rothermel'], 'title': 'Scalable processing of trajectory-based queries in space-partitioned moving objects databases', 'venue': 'In Proceedings of the 16th ACM SIGSPATIAL international conference on Advances in geographic information systems', 'year': 2008}, {'authors': ['Ralph Lange', 'Frank Dürr', 'Kurt Rothermel'], 'title': 'Efficient real-time trajectory tracking', 'venue': 'The VLDB Journal-The International Journal on Very Large Data Bases 20,', 'year': 2011}, {'authors': ['Xinchen Liu', 'Wu Liu', 'Huadong Ma', 'Huiyuan Fu'], 'title': 'Large-scale vehicle re-identification in urban surveillance videos', 'venue': 'In Multimedia and Expo (ICME),', 'year': 2016}, {'authors': ['Nirvana Meratnia', 'A Rolf'], 'title': 'Spatiotemporal compression techniques for moving point objects', 'venue': 'In International Conference on Extending Database', 'year': 2004}, {'authors': ['James CMiller', 'Matthew L Smith', 'Michael E McCauley'], 'title': 'CREW FATIGUE AND PERFORMANCE ON US COAST GUARD CUTTERS', 'year': 1998}, {'authors': ['Jinfeng Ni', 'Chinya V Ravishankar'], 'title': 'Indexing spatio-temporal trajectories with efficient polynomial approximations', 'venue': 'IEEE Transactions on Knowledge and Data Engineering 19,', 'year': 2007}, {'authors': ['Chiao-Fe Shu', 'Arun Hampapur', 'Max Lu', 'Lisa Brown', 'Jonathan Connell', 'Andrew Senior', 'Yingli Tian'], 'title': 'Ibm smart surveillance system (s3): a open and extensible framework for event based surveillance', 'venue': 'In Advanced Video and Signal Based Surveillance,', 'year': 2005}, {'authors': ['Xiaogang Wang'], 'title': 'Intelligent multi-camera video surveillance: A review', 'venue': 'Pattern recognition letters 34,', 'year': 2013}, {'authors': ['Philip Wette', 'Martin Draxler', 'Arne Schwabe', 'Felix Wallaschek', 'Mohammad Hassan Zahraee', 'Holger Karl'], 'title': 'Maxinet: Distributed emulation of softwaredefined networks', 'venue': 'In Networking Conference,', 'year': 2014}, {'authors': ['Chih-WeiWu', 'Meng-Ting Zhong', 'Yu Tsao', 'Shao-Wen Yang', 'Yen-Kuang Chen', 'Shao-Yi Chien'], 'title': 'Track-clustering Error Evaluation for Track-based Multi- Camera Tracking System Employing Human Re-identification', 'venue': 'In Computer Vision and Pattern Recognition Workshops (CVPRW),', 'year': 2017}]
sections: [{'text': 'CCS CONCEPTS • Computer systems organization → Distributed architectures; Sensor networks; Real-time systems; • Information systems → Storage management; • Networks→ Network types;\nKEYWORDS multi-target multi-camera tracking, smart camera surveillance, fog computing, trajectory management\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. DEBS ’18, June 25–29, 2018, Hamilton, New Zealand © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5782-1/18/06. . . $15.00 https://doi.org/10.1145/3210284.3210291\nACM Reference Format: Zhuangdi Xu, Harshit Gupta, and Umakishore Ramachandran. 2018. STTR: A System for Tracking All Vehicles All the Time At the Edge of the Network. In DEBS ’18: The 12th ACM International Conference on Distributed and Eventbased Systems, June 25–29, 2018, Hamilton, New Zealand. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3210284.3210291'}, {'heading': '1 INTRODUCTION', 'text': 'A smart camera surveillance system [8] has the potential for simultaneously reducing manual labor (which could be error-prone) and achieving better efficiency and effectiveness for surveillance tasks. Specifically, due to increase in urban terrorism there is a crying need to bring more automation to bear on the task of suspicious vehicle tracking. Without such automation help, security personnel have to watch tons of archived videos in order to track a suspicious vehicle, which is extremely demanding and error-prone due to lapses in attention [18]. Existing smart camera surveillance systems, such as IBM S3 [20], are largely alert-based. In other words, users need to register the suspicious vehicles in advance and then once the system detects them, it will start tracking and sending alerts. However, in reality, it is not always the case that we know which vehicle we want to track in advance. For example, after a traffic accident, the police would want to find where the perpetrators escaped, which there is no way to know in advance. Alert-based smart surveillance cannot help a lot in these situations. The state-of-the-art is to store raw video streams from cameras and if a situation warrants it, to analyze the recorded videos to extract the track of a suspicious vehicle postmortem.\nIn this work, we propose a radically different approach to smart surveillance for vehicle tracking. Specifically, we explore a smart camera surveillance system aimed at tracking all vehicles in real time. The insight is not to store the raw videos, but store the spacetime trajectories of the vehicles. Queries could then be answered directly from such recorded space-time tracks of all the vehicles. In this work, we do not focus on the querying side of the problem but on demonstrating that it is feasible to construct a system that would store the trajectories of all vehicles for their lifetime.\nWe adopt Fog/edge computing [3] as the platform for smart camera-based vehicle tracking. Processing sensor streams (especially cameras) at the edge of the network is advantageous for three reasons: (a) reducing the latency for processing the streams, (b) reducing the backhaul bandwidth needed to send raw sensor streams to the Cloud; and (c) preserving privacy concerns for the collected sensor data. Specifically, for our work in building a smart surveillance system for 24x7 vehicle tracking, Fog computing offers the following advantages:\n• For a large-scale smart camera surveillance system, sending thousands of camera streams to the Cloud is not practical and a waste of network resources. • Vehicle tracking is a continuous and local task, and geodistributed Fog infrastructure is an ideal fit for such a task. • Storing the vehicle trajectories at the edge of the network would enable fast query processing.\nThe biggest challenge for our approach is the limited storage on each fog node, since the vision in this work is to store the space-time track of all vehicles for their lifetime. Considering the infinite time dimension, it appears very hard (even impossible) to construct such a system, since cameras are continuously generating new vehicle detections. Based on this impression, the solution usually lies in discarding old trajectories or pushing the heavy lifting to the Cloud (under the assumption of unlimited resources in the Cloud) and reducing the role of the Fog to simply serve as a cache for recent trajectories. In our work, we look at the problem of trajectory generation differently. Instead of the time dimension, we focus on activity of a vehicle in a geographical region and formulate an upper bound for the storage space needed per vehicle for recording its activity in its entire lifetime. While the time dimension is infinite, the number of simultaneous activities (number of vehicles) of a finite geographical region is finite and the activity (lifetime) of given vehicle is also finite, which implies the storage required for a given finite geographical region to store all its activities is also finite. The assumption is that after the useful life of a vehicle (i.e., it is no longer being used in the roadways either because it has been totaled and/or has been sent to a junkyard), the tracks will no longer be hosted on the Fog nodes. Most likely it will be archived in the Cloud for legal reasons should such information be needed at some future date for law enforcement purposes. Considering the tag/title registrations for our vehicles every year, the task of tracking all vehicles is like an advanced vehicle registration system, and instead of on a yearly basis, the granularity is finer (say every second). And each camera (and its associated Fog node) is like a tag office, which is responsible of recording all activities occurring in its region.\nThe second challenge for large-scale camera-based tracking is the size of computation, where the system should make its best effort to detect and re-identify the vehicles as they are moving from cameras to cameras in real time. In other words, we want to reduce the size of computation or search search space such that we can minimize the latency of processing each vehicle detection. The smart camera system that we propose in this paper exploits the locality of vehicle tracking problem, and restricts the space to nearby cameras that vehicles must pass through. Specifically we propose two strategies to bound the computation that each camera has to do: forward propagation which progressively multicasts the signature of a detected vehicle to the downstream cameras in the forward path of the vehicle (using the topology of camera deployment in a given neighborhood) so that vehicle re-identification can be immediately triggered at a camera when a vehicle is sighted; and backward propagation to reach back to the upstream cameras upon detecting a new vehicle that was not informed by the forward propagation.\nBased on the above two strategies, we propose our smart camera surveillance system dubbed, “STTR” (short for Space Time Trajectory Registration). STTR is aimed at solving the system side challenge, i.e., distributed computation, communication, and storage, for real-time vehicle tracking using camera networks. We rely on domain expertise from computer vision for multi-camera tracking algorithms including vehicle detection and re-identification. Also, in this work, we only focus on demonstrating the generation and storage of the space-time tracks of vehicles in real time. Efficient indexing structure of the space-time tracks for fast query processing is outside the scope of this work and will be explored in the future.\nWe summarize the contribution of this work as follows: • We present details of the activity-based tracking of all the vehicles all the time in a given geographical area that is at the intellectual core of STTR, which enables bounding the storage space requirements at each Fog node. • We present the details of forward and backward propagation that enable bounding the computation and communication requirements of STTR. • We implement STTR using ZeroMQ for inter-camera communication and Redis persistent key-value store for recording the space-time trajectories. • We build a toolkit on top of SUMO [13], with which we are able to import OpenStreetMap [21], generate traffic flows and detectors, simulate vehicle movements, and the corresponding camera streams. • We evaluate STTR with the above tool kit and MaxiNet [24] on Microsoft Azure to experimentally verify the theoretical assertions about finite storage space requirements for activity-based space-time tracking of vehicles.\nThe rest of the paper is organized as follows: Section 2 covers related work, Section 3 presents the problem definition and notations, Section 4 summarizes the theoretical upper bounds of computation and storage for the problem, Section 5 presents the details of the forward and backward strategies, Section 6 gives the architecture of STTR, Section 7 gives the implementation details of STTR, Section 8 presents the experimental setup for emulating the Fog computing infrastructure and the camera streams on Microsoft Azure using MaxiNet, Section 9 summarizes the evaluation of STTR, and Section 10 concludes with directions for future work.'}, {'heading': '2 RELATEDWORK', 'text': 'Multi-target multi-camera tracking (MTMC). Wu, et al. [26] present a new evaluation measure to isolate the trackbased multi-camera tracking (T-MCT) errors from single camera tracking (SCT) errors. Gou, et al. [6] introduce DukeMTMC4ReID, a new large-scale real-world person re-identification dataset, which uses 8 disjoint surveillance camera views covering parts of the Duke University campus. Similarly, VeRi [16] is a dataset for vehicle re-identification in urban surveillance scenario, which contains over 40,000 bounding boxes of 619 vehicles captured by 20 cameras. These datasets are more specialized for computer vision algorithms, while in our work, traffic flow and the road network are more influential. That’s why we adopt SUMO [13] for simulation and evaluation. Wang [22] discusses topics related to intelligent multi-camera video surveillance such as multi-camera calibration, and computing\nthe topology of camera networks. We believe that our work can be integrated with these ideas as long as they are local feature or detector based. Particularly we leave the detection and matching modules to domain experts, to allow integration of such algorithms into our system. Kawanishi, et al. [12] study failures caused by appearance change or environmental illumination if matching only happens across adjacent cameras and introduce “random camera drop" and “trajectory ensemble" to integrate incomplete trajectory result. This work is a very good complement for extending our system when detections are missed due to occlusion (e.g., a truck hiding a vehicle from the camera’s field of view). Smart video surveillance. Arth’s [2] work optimizes the object re-acquisition and tracking when there are limited resources. Alsmirat [1] utilizes the cloud and mobile edge computing (MEC) to optimize the network bandwidth for wireless surveillance system. Fan [5] presents an event-driven visualizationmechanism fusingmulti-modal information for a largescale intelligent video surveillance system. Mobile fog [11] proposes a programming model for developing large-scale distributed situation awareness applications, launching the application components on Fog nodes at the edge of the network. Vehicle tracking by cameras is used as an example application for evaluation in mobile fog, as the intent is to showcase the features of the programming model. Arun [9] proposes the use of smart surveillance to shift from “investigation of incidents" to “prevention of potentially catastrophic incidents". Their system architecture involves object detection, multi-object tracking, object classification and real time alert. Their work is a pioneering effort in smart surveillance. IBM s3 [20] smart surveillance system involves a smart engine for video/image analysis and middleware for large scale surveillance management. Their work focuses more on openness and extensibility of the framework, and cross-indexed data model for correlation across multiple sensors and event types. Trajectory management. There exists many research efforts on trajectory compression or simplification [15][4][17]. While our work does not involve any trajectory simplification, these technologies can be very helpful if used with our system to further reduce actual storage usage. Another major topic on trajectory management is efficient query support [14][7][19], which usually balances the trade-off between spatio-temporal range query and retrieval by distance or time interval. At the current stage of our work, our focus is on building the large-scale smart camera surveillance system and storing all the trajectories. Efficient query support (e.g., spatial and temporal indexing into the stored trajectories) is in our future work.'}, {'heading': '3 PROBLEM DEFINITION AND NOTATIONS', 'text': 'This section defines the problem addressed in this work and all the notations used in the rest of the paper which we summarize in Table 1.\nFigure 11 shows a pictorial representation of the road network and the computational infrastructure assumed in this work. We assume that vehicles are continuously moving on the road network, and there are no shortcuts hidden from the cameras for the vehicles to “disappear” from camera observation. We expect that most\n1Icons made by Smashicons, Freepik from www.flaticon.com is licensed by CC 3.0 BY\ncameras are at road intersections and a few along the road. We assume a non-overlapping camera network in this paper, although there is nothing inherent in the system that we have built that will preclude it being used to a situation where the field of view (FOV) of the cameras overlap. We do not assume that cameras are at each road intersection, and in Sections 5 and 9, we will show how our system reacts to different density of camera distributions. Cameras are connected to nearby fog nodes, so camera streams can be processed there. For simplicity of illustration, we will assume\neach camera is connected to a unique fog node and refer to ci and fi interchangeably. In other words, in this paper we will use the word “camera” generally to include not only the capability of videoing but also computation, storage and network. In reality, it is more common that multiple cameras are connected to one nearby fog node, which can be done by running the application instances in the docker containers. We assume nearby cameras are connected by cables/fibers for network communication such that a low-latency local area network is set up among cameras.\nWith this setup, the smart camera surveillance systemwill launch application instances running on each camera, which cooperate with each other to track all passing vehicles in real time and store the trajectories such that the system is able to answer track-related queries immediately such as where did the red vehicle at c2 come from? or where does the blue vehicle at c9 head to? In the rest of the paper, we assume a perfect vehicle detection and re-identification algorithm and our work focuses on designing distributed computation, communication, and storage in this system.'}, {'heading': '4 ACTIVITY AND STORAGE UPPER BOUND', 'text': 'In this section, we will elaborate howwe use physical restrictions to find the upper bound of storage required on each camera to store vehicles’ trajectories. To start with, we first define two terminologies, vehicle activity and camera’s activity region.\nDefinition 4.1. For vehiclex , if we have∃d (vx , ci , t )∧(∀ci′∄t ′(t ′ > t ) ∧ d (vx , ci′ , t ′)), then we call vehicle x is active under camera i from time t , shortened as vx ∈ ci |[t ,now]. Similarly, if we have ∃d (vx , ci , tr )∧ (∃c j tp (tp > tr )∧d (vx , c j , tp ))∧ (∀ci′∄tq (tp > tq > tr ) ∧ d (vx , ci′ , tq )), then we call vehicle x was active under camera i between tr and tp , shortened as vx ∈ ci |[tr , tp ].\nIn other words, at any given time point, each vehicle is active under the camera that last detected it. As the last camera detecting the vehicle changes with the movement of the vehicle, so we have a chain of vehicle activities. For example, in figure 1, we have vr ∈ c2 → vr ∈ c3 for the red vehicle and vb ∈ c9 → vb ∈ c5 → vb ∈ c6 → vb ∈ c7 for the blue vehicle2. Let’s consider a specific stage of chain vx ∈ ci |[tr , tp ]→ vx ∈ c j |[tp , tq ]. We know vehicle x is at camera i at tr and at camera j at tp , but we have no information where vx is exactly between tr and tp . For example, it 2Time interval is omitted for simplicity.\ncould be moving on the road, or it could have come to a stop (say in a parking garage, or on the roadside). But we do know it must be at some place that cannot be detected by any other cameras during [tr , tp ]. The collection of these places are called camera i’s activity region.\nDefinition 4.2. For camera i , its activity region is defined as:⋃ ∀vx ,c j vx ∈ci |[tr ,tp ]→vx ∈c j |[tp,tq ] location(vx )\nFor example, in Figure 1, the activity region of c5 is the 4 roads towards c2, c4, c6 and c9. In reality, camera’s activity region can be more diverse including roads, garage, plaza, estate. But what matters is the size of activity region and whether it is finite. With reference to Figure 1, there is a practical upper bound for the size of the activity regions owned by cameras c2, c3, c5, c6, c9, c10; while it is unbounded for the cameras c1, c4, c8, c7, c11, since these cameras are at the periphery of a geographical region of interest.\nDefinition 4.3. For a given geographical region and camera surveillance system, cameras that have a finite activity region are regarded as interior cameras, while cameras that have an infinite activity region are regarded as boundary cameras, and camera i’s activity region is shortened as size (ci ).\nIn the rest of the paper, we only focus on interior cameras unless stated otherwise. For a closed geographical region, almost all cameras are interior cameras, and for a large-scale camera surveillance system, majority of the cameras are interior. In Section 7, we will revisit boundary cameras and discuss options therein for bounding the storage requirement.\nThe finite size of the camera’s activity region gives us a very good property, because at any given time, the number of vehicles that are active under this camera has to be finite, for vehicles need to occupy space. Meanwhile, each vehicle’s life is also finite which indicates there exists trajectory upper bound for a given camera. Putting these facts together, we have the following simple idea —At any time point, each camera stores the trajectory of vehicles that are active under its region. And we have the storage space required on a camera i at time t is:\ndisksize (ci , t ) = ∑\n∀vx ,vx ∈ci |[t,t ] sizeo f (traj (vx ))\nwhere traj is the trajectory of a given vehicle and sizeof is the number of bytes to store them. By applying the activity region and the trajectory upper bound, we have the following theorem:\nTheorem 4.4. For each camera i , its storage space over timedisksize (ci , t ) has the following upper bound:\n∀t ,disksize (ci , t ) ≤ ρisize (ci )#maximum size of trajectory\nwhere ρisize (ci ) is the maximum number of vehicles that can be simultaneously active under ci .\nSince this upper bound is conditioned by ρisize (ci ), which is governed by the physical environment near the camera and is unlikely to change frequently, this also gives a good reference for system administrators to know the storage capacity needed for each camera.\nWe show the storage required on each camera is finite, but is it small enough to be practical? We make the discussion concrete with a hypothetical but realistic example. Considering that the average life of a vehicle is 150000 miles[23], with a deployment of 5 cameras every mile, using two 64-bit longitude- latitude to represent each car detection, and assuming 100 (maximum number of simultaneous vehicles) activities for a given camera, the upper bound for storage space needed on each camera can be calculated as 150000 ∗ 5 ∗ 2 ∗ 64 ∗ 100 which is around 1.12GB3. In reality, we will have to store more meta data besides longitude-latitude, but at the same time it is also unlikely that ALL the vehicles in one camera’s activity region are close to the end of their active life.'}, {'heading': '5 PROPAGATING VEHICLE DETECTIONS', 'text': 'In this section, we are looking into the detection propagation models to limit the storage and computational needs for vehicle reidentification. Upon vehicle detection by a camera, the detections have to be propagated for re-identification by neighboring cameras. While domain experts provide detection and re-identification algorithms, our system will handle the underlying detection propagation to reduce the resource requirements. Broadcasting to all the cameras in the region of interest is not only wasteful of networking resources but also burdening ALL the cameras with unnecessary additional computational work (leading to latency for re-identification). Instead, we propose two detection propagation models, detection forward and backward, which exploit the topology of camera deployment in the roadways.'}, {'heading': '5.1 Forward Propagation', 'text': 'Upon detection of a vehicle, the camera propagates the detected vehicle’s signature to the set of cameras that are likely candidates for this vehicle to pass through next. We call this forward propagation. This way the downstream cameras are already primed to run the re-identification procedure when the vehicle is sighted without any additional communication. For example, in Figure 1, when the blue vehicle is detected by c5, c6 is the only candidate camera downstream in the direction in which the blue car is headed, so we can safely forwardd (vb , c5, t ) to c6. For a more complicated case, let us remove c6 from the camera surveillance system. In this case, since the direction of travel of the vehicle cannot be predicted, we have to forward d (vb , c5, t ) to a set of cameras (c3, c7, c10). Eventually one camera will detect the vehicle again, which is c7 in this case, and c7 will send a confirmation to (c3, c10), so they can safely discard d (vb , c5, t ). In general, until such confirmation is received from one of the downstream cameras that are in the plausible set for the next sighting of the vehicle, all cameras have to hold on to the objects received by this forward propagation.\nTheorem 5.1. The resource for each camera to store all the detection objects received by forward propagation has the following upper bound:\n∀ci , sizeo f ( f orward (ci )) ≤ ρisize (ci ) ∗ sizeo f (dx )\n3This upper bound is predicated on clearing out the trajectories of “destroyed vehicles” from the fog nodes either by archiving them in the Cloud or deleting them permanently as discussed in Section 7.2.3.\nwhere f orward (ci ) is the candidate pool for detection objects received by forward propagation and we assume the format of detected object is fixed, so sizeo f (dx ) is constant.\nThe rationale behind theorem 5.1 has similarity with how we discuss the activity and storage upper bound. When the detected object d (vx , ci , tr ) stays in the f orward (c j ) at time tp , it means vx ∈ ci |[tr , tp ]. In other words, vx must be somewhere between the ci and c j , which we call the shared activity region between camera i and j, shortened as ci ∩ c j . By summing up all cameras that have shared activity region with c j , we have size ( ∑ ∀ci,c j ci ∩ c j ) ≤ size (c j ). And we also have ∀dx ∈ f orward (c j ) ⇒ V (dx ) ∈⋃ ∀ci,c j ci ∩ c j . Combining these terms together, if a detection object is in the camera’s forward propagation candidate pool, then the corresponding vehicle must occupy the space in the camera’s activity region. So the storage resource of the camera’s candidate pool is capped by the product of maximum number of vehicles in the camera’s activity region and size of each detected object.'}, {'heading': '5.2 Backward Propagation', 'text': 'Backward Propagationwould be warranted if a vehicle is detected by a camera for which no forward detection notification was received from some upstream camera in the direction of travel of the vehicle4. In this case, the camera has to guess whence from (i.e., which upstream camera) the vehicle came. Therefore, it sends the detected object “backwards” to a candidate set of upstream cameras once again taking into account the deployment topology for possible re-identification by one of those cameras. For example, in Figure 1, considering removing c6 from the camera surveillance system, if the blue vehicle is detected by c7, (without any prior forwarding information), wewill backwardd (vb , c7, t ) to a set of cameras (c3, c5, c10). One camera should confirm it saw vb before, which is c5 in this case. Different from forward propagation, cameras do not need to keep the detected objects received from backward propagation. Instead, if the camera cannot re-identify the detected object in the backward propagation, it simply discards the object. On the other hand, the candidate pool for backward propagation is composed of detected objects that have not been re-identified by any other upstream cameras, which is ∀dx ∈ backward (ci ) ⇒ V (dx ) ∈ ci where backward (ci ) is the backward candidate pool of camera i .\nTheorem 5.2. The resource for each camera to store all the detected objects for backward propagation has the following upper bound:\n∀ci , sizeo f (backward (ci )) ≤ ρisize (ci ) ∗ sizeo f (dx )\nAgain the storage space is capped by the product of the maximum number of vehicles in the camera’s activity region and size of each detected object, as backward (ci ) is actually the collection of detected objects of vehicles that are active under camera i .'}, {'heading': '6 STTR SYSTEM ARCHITECTURE', 'text': 'STTR (short for Space Time Trajectories Registration) is the system that embodies the ideas presented thus far for registering the spacetime tracks of all the vehicles all the time. Figure 2 shows the STTR\n4In the current implementation of STTR, failure of forward propagation is caused by the optimizations in section 7.1, while in reality it could also be due to the fact that an upstream camera failed to detect the vehicle.\nsystem architecture. Each application instance running on the camera contains 7 modules: Detection, Matching, Forward, Backward, TrajectoryStore, CameraMap and Policy. Among them, Detection and Matching provide interfaces for domain experts to plug in unique algorithms. Specifically, we expect the input of Detection will be the raw camera stream (frames) and the output of Detection will be the stream of vehicle’s detected objects. Each detected object is a JSON object that at least includes time stamp, unique signature and direction of travel information. Matching implements the reidentification algorithm reid (dx ,p), which inputs a detected object and a candidate pool and outputs the object from the candidate pool. For the remaining 5 modules, CameraMap maintains the geographical relationship between cameras, which is mainly used with the direction of travel to form the set of cameras for forward and backward propagation; Forward/Backward implements the vehicle detection propagation in Section 5, and real-time requirement is also taken into account by dynamically monitoring the traffic flow and truncating/flushing the forward candidate pool; TrajectoryStore distributes the trajectory according to vehicle activities as discussed in Section 4 and optimizes network consumption; Policy module allows each camera to configure parameters and disable specific modules, for example only a subset of cameras are permitted to archive the trajectories at the cloud. There could be services running in the cloud to reach into the space-time trajectories stored in the camera nodes. For example, we show the HistoryTrajectory service in the cloud in Figure 2. This service may provide the ability to archive some of the trajectories (e.g., vehicles no longer in active service, trajectories that are older than some delta, etc.) from the camera nodes to the cloud. Similarly, we envision other services, e.g., QueryEngine that may embody our future work with supporting query processing of the stored trajectories in the cameras.\nThe flow of processing each detected objects is presented as green lines in Figure 2. After getting detected objects dx from Detection, dx is given to the Forward to multicast it to candidate cameras, and Matching to reid (dx , f orward (ci )). If a re-identification is successfully found, dx and the previous detected object is given to TrajectoryStore for writing. Otherwise, dx will be given to the Backward to multicast to backward candidate cameras. Meanwhile,\nBackward will re-identify the detected objects it received with backward (ci ) through Matching, and similarly if a re-identification is found, TrajectoryStore will be used for recording.'}, {'heading': '7 IMPLEMENTATION DETAILS', 'text': 'STTR is written in Python, communication between cameras is based on ZeroMQ, and persistent key-value store is based on Redis. In this section, we go into some implementation details.'}, {'heading': '7.1 Matching', 'text': 'The Matching module brings in the re-identification algorithm provided by domain experts, which accepts a detected object and a candidate pool. In this section, we show some tricks in STTR to help further reducing the latency of re-identification.\n7.1.1 Forward candidate pool flush. Consider a vehicle stream v1v2 . . .v2n from camera ca to cb , where the odd sequencev1v3 . . .v2n−1 stops in a plaza in the middle and the even sequence v2v4 . . .v2n directly heads to cb . However, by forward propagation, the detected objects of the odd sequence, which is d1d3 . . .d2n−1, will stay in the f orward (cb ). So for vehicle v2k from the even sequence, the re-identification on cb has to go through first k detected objects in f orward (cb ), and this phenomenon will accumulate over time. In other words, long-term stopped vehicle will produce an unwelcome detected object in the top of following cameras’ forward candidate pool, and we want to discard it (when the corresponding vehicle moves again later, its re-identification can be found through backward propagation) to reduce the latency for majority moving vehicles.\nAlgorithm 1 Forward_candidate_pool_flush 1: k ← reid (dx , f orward (ci )) 2: if k > α ∗ len( f orward (ci )) then 3: f orward (ci ) ← f orward (ci )[k + 1 : end] 4: else 5: f orward (ci ) ← f orward (ci )[0 : k − 1,k + 1 : end] 6: end if\nAlgorithm 1 illustrates the idea of flush operation of camera’s forward candidate pool. After we get the re-identification result for every detected object, we check the position of the result in the candidate pool, where α is a parameter between 0-1 for tolerance of short-term stopped vehicles and overtaking between vehicles. If we find the result is after first α ∗ len( f orward (ci )) objects, we flush the candidate pool by removing all detected objects before the re-identification result. Otherwise, we only remove the result itself.\n7.1.2 Adaptive truncated candidate pool. Considering a longterm stopped vehicle moves again, when it gets detected by the following camera, unfortunately its forwarded detected object has been discarded. However, there is no way for Matching module to know this, so it will try to re-identify the vehicle with the whole forward candidate pool, fail and fall back to Backward module. When the re-identification algorithm is efficient and rate of traffic flow is low, we can afford doing this, but this is not often the case. Instead we want to consider the rate of traffic flow, efficiency\nof re-identification algorithm and then decide whether to give a truncated candidate pool to the Matching module. Intuitively, the slower the algorithm is, the more we need to truncate the candidate pool. Similarly, if the traffic flow is high, we have to spend less time on each detected object, so the candidate pool needs to include few objects. By these observations, Algorithm 2 shows how we monitor the rate of traffic flow, time spent on re-identifying the last detected object, and then update the size of the candidate pool given to the re-identification function.\nAlgorithm 2 Adaptive_truncated_candidate_pool 1: rate ← Detection(ci ) 2: L ← Lrate∗t ime 3: (k, time ) ← reid (dx , f orward (ci )[0 : L])'}, {'heading': '7.2 TrajectoryStore', 'text': 'In this subsection, we go into the detail of trajectory implementation in STTR and how to bring the activity and storage upper bound in Section 4 into life and optimize it. We implement the trajectories as vertices and edges linking two vertices on the key-value store. Each trajectory vertex is a list of trajectory records of when and which camera detected the corresponding vehicle. Each edge links trajectory vertices stored on different fog nodes and heads to the vertex with large time stamp. For simplicity, we will use the following notation u (д1 : t1,д2 : t2) |c2 → u (д3 : t3) |c3 to represent one trajectory vertex on camera 2 storing two trajectory records (д1, t1) and (д2, t2), while another trajectory vertex on camera 3 stores the trajectory record (д3, t3) and a directed edge links them together.\n7.2.1 Greedy trajectory aggregation. The most intuitive way to implement the idea — At any time point, each camera stores the trajectory of vehicles that are active under its region – is to aggregate the trajectories as vehicle moves from one camera to another. Take the red vehicle in Figure 35 as an example: • When the red vehicle is detected by C0, we generate the trajectory vertex u (G0 : T0) |F0. • When the red vehicle is then detected by C1, we get and remove the previous trajectory vertex u (G0 : T0) |F0 from F0, and aggregate the new trajectory vertex at F1, so we have u (G0 : T0,G1 : T1) |F1.\n5Icons made by Smashicons, Freepik from www.flaticon.com is licensed by CC 3.0 BY\n• Similarly, when the red vehicle is finally detected by C2, we get and remove the previous trajectory vertex u (G0 : T0,G1 : T1) |F1 from F1, and aggregate the new trajectory vertex at F2, so we have u (G0 : T0,G1 : T1,G2 : T2) |F2.\nAlgorithm 3 Greedy_trajectory_aggregation 1: dx ← Detection(ci ) 2: d ′x ← Matchinд(dx , ci ) 3: u ′x ← pullAndRemoveTrajectory (d ′x ) 4: store (append (u ′x , ci : T (di )) |ci )\nAlgorithm 3 summarizes the procedure of greedy trajectory aggregation. Upon receiving the new detected object dx and its reidentification result d ′x , we pull and remove the trajectory vertexu ′x from the cameraC (d ′x ), which is the previous camera that detected the vehicle. Then we store and append the new trajectory record to u ′x . The greedy trajectory aggregation is a direct translation from the activity upper bound idea, but it suffers from the increasing network consumption over time. Because the trajectories of vehicle will become larger and larger over the life of the vehicle, and it will be not practical to keep pulling the whole trajectories from one camera to another.\n7.2.2 Lazy trajectory aggregation. The idea is that each camera only aggregates the trajectory if its storage is under pressure and a new trajectory vertex is coming. We again use the example in Figure 3 and assume no camera’s storage is under pressure at the beginning. • When the red vehicle is detected by C0, we generate the trajectory vertex u (G0 : T0) |F0. • When the red vehicle is then detected byC1, we generate the new trajectory vertex and create an edge from the previous trajectory vertex, which is u (G0 : T0) |F0 → u (G1 : T1) |F1. • Similarly, when the red vehicle is finally detected by C2, we will have u (G0 : T0) |F0 → u (G1 : T1) |F1 → u (G2 : T2) |F2. • After a period time, F1’s storage is under pressure and a new trajectory vertex is coming, so u (G1 : T1) |F1 is chosen to be aggregated. • u (G1 : T1) |F1 will be aggregated to F2, so we have u (G0 : T0) |F0 → u (G1 : T1,G2 : T2) |F2 as the trajectory of the red vehicle.\nAlgorithm 4 Lazy_trajectory_aggregation 1: dx ← Detection(ci ) 2: d ′x ← Matchinд(dx , ci ) 3: uy ← randomPick (ci ) 4: aддreдate (uy ) 5: store (trajectoryVertex (d ′x )− > u (ci : T (dx )) |ci )\nAlgorithm 4 summarizes the procedure of lazy trajectory aggregation. Upon receiving the new detected object dx and its reidentification result d ′x , if the storage of the camera is under pressure6, a random victim trajectory vertex with outgoing edge is 6Idea of lazy trajectory aggregation also works without setting a threshold for storage pressure, and actually in evaluation, in order to see the lazy trajectory aggregation in\npicked uy . We aggregate uy to the trajectory vertex which the outgoing edge points to. Then we create an edge from the trajectory vertex of d ′x to the new trajectory vertex generated from dx . Under lazy trajectory aggregation, each camera also owns the activity upper bound and corresponding storage upper bound. It is because whenever a new trajectory vertex is coming, either the activity region of the camera hasn’t been full of vehicles or one old trajectory vertex is available to be aggregated to another camera. In the worst case, for example, all cameras are under pressure, and then lazy trajectory aggregation will be eventually consistent with greedy aggregation as it will keep aggregating the trajectory vertex with outgoing edges such that trajectory data leaving on one camera will only belong to the vehicles that are still active under this camera. Moreover, lazy trajectory aggregation is network friendly for two reasons. Firstly, the rate of the trigger of lazy aggregation is the rate of incoming vehicles detected by the camera which is slow compared to the network speed7. Secondly, the minimum aggregation only needs to transmit one trajectory record as there is only one trajectory record written in. Yet, for network efficiency, it is usually better to pack multiple trajectory records into one transmission. On the other hand, the downside of lazy trajectory aggregation is the potential increased in query processing time, e.g., for a range search. Exploring indexing techniques for efficient query processing is part of our future work.\n7.2.3 General discussion. Does the infinite time line contradict with the activity upper bound we give for each camera? The answer is no. Let’s consider a closed region first. In a closed region, every camera is the interior camera except for those at the sink and the source. Source is where the vehicle is created, such as the vehicle factory, the entrance of the region, while the sink is where the vehicle is “destroyed” (e.g, vehicle is “totaled”, or sent to the junkyard), the exit of the region. In other words, when the vehicle is created from the source, we start tracking the vehicle, and when the vehicle is destroyed at the sink, we stop tracking the vehicle. So more precisely, we are able to track all alive vehicles in the region over their lifetime with each camera owning limited resources. With time elapsing, there will be new vehicles born from the source and we start to allocate resource for them, old vehicles gone at the sink and we “free” their resources (either by deletion or archiving them permanently in the cloud). Notice we do not need explicit garbage collection, because the trajectory aggregation will eventually aggregate the whole trajectory to the sink, as the sink is the last camera detecting those retired vehicles. On the other hand, for a general non-closed region, boundary cameras are different from sink and source, because vehicles can leave from one boundary camera and enter the region from another. One interesting option we can try is to create virtual cameras that connect all boundary cameras together to manually force a closed region. Although all cameras turn into interior, these virtual cameras cover the infinite activity region in terms of storing activities. Theoretically it is possible any vehicle from around the world could frequently leave and enter a given region. However, in reality, we believe vehicles active in a specified geographic region are largely “return customers” to the STTR\nthe three-hour simulation, we omit the threshold and start the aggregation from the beginning. 7Physical world is slow compared to the cyber world!\nsystem every day. The example cloud service (HistoryTrajectory) shown in Figure 2 would come in handy to archive trajectories from the boundary cameras (or the virtual cameras) into the cloud.'}, {'heading': '7.3 Other implementation details', 'text': 'For performance and our best effort to guarantee the real-time property, we adopt a lock-free system design and manually handle the collisions between threads and cameras. For example, considering the trajectory uxt |ci → uxt+1 |c j , in lazy trajectory aggregation, it is possible that Ci and Cj will independently make the decision to aggregate uxt |ci and uxt+1 |c j at the same time. When the uxt |ci aggregation request reaches Cj , uxt+1 |c j may not exist anymore. A “fail-wait-retry" protocol is used, as c j is responsible for updating the trajectory edge information correctly. The trajectory vertex will be chosen as the victim for lazy trajectory aggregation only if it has both incoming and outgoing edges, which indicates its role in re-identification has completed. Otherwise the trajectory vertex and corresponding detected object have to be kept on the birth camera for the re-identification purpose. Backward propagation is multi-thread supported. Instead of blocking and processing one vehicle detected object each time, multi-threading can be used to improve the overall throughput, which is very helpful when the re-identification is slow and we have redundant computational resources. A simple example cloud service is implemented, which can be used to achieve the trajectory based on LIVEness factor. For example, LIVEness = 3600which means for each vehicle, the recent one-hour trajectory will be kept at the Fogwhile the trajectory older than one hour will be pushed to the Cloud. This is implemented by checking the trajectory’s timestamp upon each trajectory aggregation, and establishing a trajectory edge from Cloud to the Fog to guarantee the integrity of each vehicle’s trajectory.'}, {'heading': '8 EXPERIMENTAL SETUP', 'text': 'In this section, we present the experimental setup for performance evaluation of STTR.'}, {'heading': '8.1 Traffic flow and camera stream of detections', 'text': 'SUMO is an open source, highly portable, microscopic and continuous road traffic simulation package designed to handle large road\nnetworks [13]. We build a tool kit upon SUMO to generate camera stream of detections given the simulated traffic flow. Specifically, we import and simplify the campus map from OpenStreetMap [21] as shown in Figure 4. Vehicles are generated on each road following the distribution shown in the Table 2, where sigma is the driver’s imperfectness. Number of vehicles started on each road is proportional to the length of the roads. Rerouters which keep vehicles moving are generated at the end of each lane. And detectors which record the entering and leaving of vehicles are generated at beginning and end of each lane. Finally, we cluster the output of detectors by road intersections, extract the vehicle identifier, time stamp and direction of travel and publish them to cameras’ application instances as stream of detected objects.'}, {'heading': '8.2 Fog computing topology', 'text': 'We emulate the fog computing topology using MaxiNet [24] on a Microsoft Azure virtual machine with 32 cores, 128 GB memory.\nFigure 58 shows the example network topology. Each fog node owns two hosts (docker containers), one for STTR’s application instance and the other for the redis key-value store, and one network switch which connects these two components. Nearby fog nodes’ switches are connected such that the network topology is isomorphic with the road network in Figure 4.'}, {'heading': '9 EVALUATION', 'text': 'All experiments are based on a 10000-second traffic flow and camera stream emulation. Following metrics are collected during the emulation: storage usage from the Redis’s info command, network usage using Linux utilities tcpdump and tcpstat [10], computation latency for each detected object. Following variables are changed between experiments: set of cameras enabled to see how density of cameras\n8Icons made by Smashicons, Freepik from www.flaticon.com is licensed by CC 3.0 BY\naffects the system, and synthetically inject latency to emulate varying efficiency of re-identification9 to see how efficiency of matching affects the system. Specifically, we have three camera sets: Ch = {all cameras},Cm = {c1, c2, c3, c4, c5, c6, c14, c15, c16, c17, c18, c19, c21, c22, c23, c24, c25} and cl = {c1, c3, c6, c15, c18, c21, c25}. And we have three latency settings: Eh = 0ms , Em = 100ms and El = 250ms , used when comparing the object signatures of the detected vehicle with the candidate pool during the forward and backward processing. The performance will be best for the experiment that uses all the cameras with negligible cost for re-identification (Ch ,Eh ) to use as the gold standard for all the other experimental settings.'}, {'heading': '9.1 Storage', 'text': 'In this subsection, we quantify the storage space usage on each camera over time. There are three factors that influence the storage usage: the road network and camera density, the traffic flow, and the trajectory aggregation. As shown in Section 4, the road network and camera density determine the maximum activity region and therefore the upper bound of storage usage on a particular camera. Meanwhile the actual traffic flow determines the distribution of routing storage usage across cameras. Finally, the trajectory aggregation will migrate data around cameras to guarantee that no camera is overloaded.\n9.1.1 Activity vs. storage usage. Figure 6 shows the storage usage and number of activities accumulated on each camera at the end of the experiment. As mentioned earlier, an activity denotes that a particular vehicle is active under the camera. Hence the number of accumulated activities on a camera implies the number of vehicles whose journey ended in the camera’s activity region. Firstly, we can see that in general, the storage usage increases with the increase in the number of activities, because each camera needs to store all activities ending in its activity region. However, the storage usage on a camera does not increase linearly with the number of accumulated activities because of the size of each activity (trajectory) is not\n9We consider the optimal case is comparing two license plate numbers for which the overhead is negligible.\nnecessarily the same. Secondly, we observe that the number of accumulated activities on each camera increases with the decrease in the density of cameras. This is because a lower density of cameras leads to each camera being responsible for a large activity region, making them responsible for a higher number of activities. However, the storage usage does not necessarily increase, because with fewer cameras, we also have fewer detections and correspondingly the size of a trajectory is smaller. This demonstrates that the storage usage on each camera is determined by the number of accumulated vehicle activities and the size of vehicle’s trajectory together.\n9.1.2 Camera density vs. storage usage. To better see how density affects the storage usage, we pick camera1 and plot its storage usage and the number of activities over time for the three experimental settings (Ch , Cm and Cl ) in Figure 7. Firstly, we can see the storage usage is slowly increasing, which is expected because each trajectory becomes longer over time. Meanwhile, there exists dramatic fluctuations on storage usage, which is caused by the greedy trajectory aggregation. When a large trajectory gets migrated and aggregated to another camera, the storage usage declines rapidly, and when a large trajectory gets migrated and aggregated in, the storage usage roars up. Secondly, we observe that the storage usage pattern with a medium density of cameras (green line) is lower than that of high camera density (red line). The storage usage pattern with low camera density (blue line) is further lesser than the medium density pattern. This means the storage usage is lower with fewer cameras, and the reason is that each trajectory is shorter due to fewer number of detection points. On the other hand, from the right parts of the figure, we can clearly see the general increase in the number of activities during the whole simulation when we choose fewer cameras.\n9.1.3 Greedy/lazy aggregation vs. storage usage. Figure 8 compares the storage usage between greedy and lazy trajectory aggregation. We plot the cameras with the maximum and minimum average storage usage, so we can expect that other cameras’ storage variation would fall between them. We also plot a baseline (yellow\nand purple line in the figure), which represent cases when no aggregation is triggered and every camera simply stores the detections in its activity region. The baseline, though not practical, serves as the worst case upper bound for storage need at each camera.\nWe can see that both greedy and lazy trajectory aggregation techniques incur much less storage usage over time than the baseline approach, which is because by aggregating a vehicle’s trajectory and storing them on one camera, we save on the duplication of the vehicle meta data. Moreover, both greedy and lazy trajectory aggregation also balance the storage usage better across cameras than the baseline. With the baseline approach, where trajectory aggregation is not enabled, we see that the storage gap (between maximum and minimum storage use) keeps increasing over time because some cameras see more activities than others, which would eventually result in storage hot spots on some cameras. Meanwhile, with greedy or lazy trajectory aggregation, the storage usage will uniformly increase on each camera over time.\nNext, we focus on the behavior of storage usage for greedy and lazy aggregation. Although the storage usage fluctuates a lot due to continuous data movement caused by trajectory aggregation, we still see that the distribution of storage usage under lazy and greedy trajectory do not differ significantly, which follows our claim that the lazy trajectory can also satisfy the storage upper bound in Section 4. If we compare the red line and green line, which are respectively camera13’s storage usage under lazy and greedy aggregation, we do see that the lazy aggregation leads to more storage use. It is because lazy trajectory aggregation does need to maintain more meta data for the trajectory that is not fully aggregated on one camera, such as edges linking distributed trajectory vertices. Finally, we find that lazy trajectory aggregation’s slow data movement can better balance the storage across cameras, as the storage usage pattern of least and most loaded cameras are quite close.'}, {'heading': '9.2 Network', 'text': 'In this set of evaluations, we look into the network usage of each camera over time. Similar to storage usage variation, the network\nusage will be affected by the road network, the traffic flow and the trajectory aggregation. However, the road network and actual traffic flow are accountable for network usage mainly due to the dependence of message flow patterns between cameras (forward propagation/backward) which is negligible over time compared to the data movement caused by trajectory aggregation. So we are going to focus on the difference in network usage between greedy and lazy trajectory aggregation.\n9.2.1 Greedy/lazy aggregation vs. network usage. In Figure 9, we pick two cameras with maximum and minimums total network usage during the experiment from greedy and lazy trajectory aggregation, and we plot their network usage in kbytes/second over the whole simulation. As we can see the greedy aggregation will cause the network usage to increase continuously over time because of the growing size of the trajectory, while on the other hand, the lazy aggregation almost has a constant network usage over time. In addition, we observe that the maximum and minimum network usage for cameras under lazy aggregation are roughly the same. This result shows that all cameras place roughly the same load on the network infrastructure with lazy aggregation, which is because each lazy aggregation is composed of fixed number of trajectory records10 due to the increased activity in the camera’s region.'}, {'heading': '9.3 Latency', 'text': 'In this subsection, we are exploring the factors that affect the latency of processing each detected vehicle. This latency is defined as the time interval between receiving a “detection notification” and completion of the re-identification and writing the trajectory into the key-value store. Since this latency is unaffected by the choice of lazy Vs. greedy aggregation strategy, we show the results only for the lazy aggregation.\n9.3.1 Forward/Backward vs. latency. A detected object can be processed either through forward propagation or backward propagation. Figure 10 shows the latency difference between these two strategies. We can see forward propagation always outperforms\n10We aggregate 5 trajectory records in the implementation, although 1 also works!\nbackward propagation and the difference increases with the increasing latency in re-identification algorithm which we inject synthetically. This is because the system is optimized towards forward propagation as a fast path trying to get an immediate re-identification result and backward propagation as a slow path to make sure not losing the track of vehicles.\n9.3.2 Density of camera and efficiency of re-identification vs. latency. The optimal case for computing the similarity between two detected vehicles is comparing their license plates for which the overhead is negligible and represented by the Eh experimental setting. However realistically, the re-identification algorithm may use some appearance-base similarity as its basis for comparison (e.g., the L2 distance between two large signature vectors), We use Em and El to simulate this extra latency. Similarly, we might not be able to afford cameras at each road intersection, and we use Cm and Cl to simulate the sparse camera distribution. From Figure 11, we can see that the extra latency in comparing the signature dominates the latency of processing each detected object. There is a clear latency\nincrease between experiments under Eh to Em and Em to El . On the other hand, the density of cameras has much less influence, which is further declining when we have less extra latency. For example, we can see under Em = 100ms , Cm takes 2.15x time to process a vehicle detection thanCh . However, under Eh = 0ms , the camera density does not affect the result. All these experimental results demonstrate that an efficient re-identification algorithm is critical for the smart camera surveillance’s real-time property.'}, {'heading': '10 CONCLUSION', 'text': 'STTR is a distributed smart camera-based system for registering the trajectories of all the vehicles all the time built assuming a geo-distributed Fog infrastructure. The key insight is to focus on the activities of vehicles in the vicinity of a camera rather than the time dimension to bound the storage requirement for storing the aggregated trajectories of the vehicles. The other insight is to judiciously communicate the vehicle detections at a given camera (forwards and backwards) exploiting the deployment knowledge of the cameras on the roadways, reducing the latency for detection and re-identification tasks, and bounding the inter-node communication requirements. We have built a toolkit on top of SUMO to generate traffic flows and detectors, simulate vehicle movements, and the corresponding camera streams. We implement and evaluate STTR with the above tool kit and Maxinet [24] on Microsoft Azure to experimentally verify the theoretical assertions about finite storage space requirement for activity-based space-time tracking of vehicles. There are two areas of future work. Our current system focuses purely on generating the space-time tracks with low latency and storing them with finite storage space on the Fog nodes. An immediate future work is creating efficient spatial and temporal index structures for answering queries on the stored trajectories. Another avenue for future work is increasing the confidence in the accuracy of the space-time tracks from the systems side. While the detection and re-identification algorithms are the forte of domain experts (computer vision), there are opportunities from the system side to bound errors in detection and re-identification by adopting a probabilistic approach to trajectory generation and maintenance. We are also planning to work with the campus police department to perform in situ studies using STTR.'}, {'heading': 'ACKNOWLEDGEMENTS', 'text': 'This work was funded in part by an NSF Award (NSF-CPS- 1446801) and a grant from Microsoft Corp. We thank members of Georgia Tech’s Embedded Pervasive Lab and the anonymous reviews for helping to improve the presentation.'}]
