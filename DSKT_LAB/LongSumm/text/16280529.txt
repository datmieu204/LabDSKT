id: SP:6ab8cc00d544695ac9fb7fb006dc368da9fa3761
title: Stochastic Backpropagation through Mixture Density Distributions
authors: [{'affiliations': [], 'name': 'Alex Graves'}]
abstractText: The ability to backpropagate stochastic gradients through continuous latent distributions has been crucial to the emergence of variational autoencoders [4, 6, 7, 3] and stochastic gradient variational Bayes [2, 5, 1]. The key ingredient is an unbiased and low-variance way of estimating gradients with respect to distribution parameters from gradients evaluated at distribution samples. The “reparameterization trick” [6] provides a class of transforms yielding such estimators for many continuous distributions, including the Gaussian and other members of the location-scale family. However the trick does not readily extend to mixture density models, due to the difficulty of reparameterizing the discrete distribution over mixture weights. This report describes an alternative transform, applicable to any continuous multivariate distribution with a differentiable density function from which samples can be drawn, and uses it to derive an unbiased estimator for mixture density weight derivatives. Combined with the reparameterization trick applied to the individual mixture components, this estimator makes it straightforward to train variational autoencoders with mixture-distributed latent variables, or to perform stochastic variational inference with a mixture density variational posterior. General Result Let f(x) be a probability density function (PDF) over x ∈ R and cumulative density function (CDF) F (x). f can be rewritten as
references: [{'authors': ['C. Blundell', 'J. Cornebise', 'K. Kavukcuoglu', 'D. Wierstra'], 'title': 'Weight Uncertainty in Neural Networks', 'venue': 'ArXiv e-prints, May', 'year': 2015}, {'authors': ['A. Graves'], 'title': 'Practical variational inference for neural networks', 'venue': 'Advances in Neural Information Processing Systems, volume 24, pages 2348–2356.', 'year': 2011}, {'authors': ['K. Gregor', 'I. Danihelka', 'A. Graves', 'D. Wierstra'], 'title': 'DRAW: A recurrent neural network for image generation', 'venue': 'ArXiv e-prints, March', 'year': 2015}, {'authors': ['K. Gregor', 'I. Danihelka', 'A. Mnih', 'C. Blundell', 'D. Wierstra'], 'title': 'Deep autoregressive networks', 'venue': 'Proceedings of the 31st International Conference on Machine Learning,', 'year': 2014}, {'authors': ['D.P. Kingma', 'T. Salimans', 'M. Welling'], 'title': 'Variational dropout and the local reparameterization trick', 'venue': 'ArXiv e-prints, June', 'year': 2015}, {'authors': ['D.P. Kingma', 'M. Welling'], 'title': 'Auto-encoding variational bayes', 'venue': 'Proceedings of the International Conference on Learning Representations,', 'year': 2014}, {'authors': ['D.J. Rezende', 'S. Mohamed', 'D. Wierstra'], 'title': 'Stochastic backpropagation and approximate inference in deep generative models', 'venue': 'Proceedings of the 31st International Conference on Machine Learning, pages 1278–1286,', 'year': 2014}]
sections: [{'text': 'ar X\niv :1\n60 7.\n05 69\n0v 1\n[ cs\n.N E\n] 1'}, {'heading': 'General Result', 'text': 'Let f(x) be a probability density function (PDF) over x ∈ RD and cumulative density function (CDF) F (x). f can be rewritten as\nf(x) =\nD ∏\nd=1\nfd(xd|x<d) (1)\nwhere x<d = x1, . . . , xd−1, f1(x1|x<1) = f1(x1) and fd is the marginal PDF of xd conditioned on x<d. A sample x̂ can be drawn from f using the multivariate quantile transform: first draw a vector of D independent samples\nu = (u1, . . . , uD) from U(0, 1), then recursively define x̂ as\nx̂1 = F −1 1 (u1) (2) x̂d = F −1 d (ud|x̂<d) (3)\nwhere F−1d is the quantile function (inverse CDF) corresponding to the PDF fd. Inverting Eq. 3 and applying the definition of a univariate CDF yields\nFd(x̂d|x̂<d) =\n∫ x̂d\nt=−∞\nfd(t|x̂<d)dt = ud (4)\nAssume that f depends on some parameter θ. The general form of Leibniz integral rule tells us that\n∂Fd(x̂d|x̂<d)\n∂θ = fd(x̂d|x̂<d)\n∂x̂d\n∂θ +\n∫ x̂d\nt=−∞\n∂fd(t|x̂<d)\n∂θ dt =\n∂ud\n∂θ = 0 (5)\nand therefore ∂x̂d\n∂θ = −\n1\nfd(x̂d|x̂<d)\n∫ x̂d\nt=−∞\n∂fd(t|x̂<d)\n∂θ dt (6)\nIf the above integral is intractable it can be estimated with Monte-Carlo sampling, as long as fd(t|x̂<d) can be sampled and Fd(x̂d|x̂<d) is tractable:\n∫ x̂d\nt=−∞\n∂fd(t|x̂<d)\n∂θ dt =\n∫ x̂d\nt=−∞\nfd(t|x̂<d) ∂ log fd(t|x̂<d)\n∂θ dt (7)\n= Fd(x̂d|x̂<d)\n∫ ∞\nt=−∞\nfd(t ≤ x̂d|x̂<d) ∂ log fd(t|x̂<d)\n∂θ dt (8)\n≈ Fd(x̂d|x̂<d)\nN\nN ∑\nn=1\n∂ log fd(t n|x̂<d)\n∂θ ; tn ∼ fd(t ≤ x̂d|x̂<d) (9)\nwhere\nfd(t ≤ x̂d|x̂<d) =\n{\nfd(t|x̂<d) Fd(x̂d|x̂<d) if t ≤ x̂d 0 otherwise (10)\nwhich can be sampled by drawing from fd(t|x̂<d) and rejecting the result if it is greater than x̂d.\nLet h be the expectation over f of an arbitrary differentiable function g of x (e.g. a loss function) and denote by Q(u) the sample from f returned by the quantile transform applied to u. Then\nh =\n∫\nu∈[0,1]D g(Q(u))du (11)\nand hence\n∂h ∂θ =\n∫\nu∈[0,1]D\n∂g(Q(u))\n∂θ du (12)\n=\n∫\nu∈[0,1]D\nD ∑\nd=1\n∂g(Q(u))\n∂Qd(u)\n∂Qd(u)\n∂θ du (13)\nwhich can be estimated with Monte-Carlo sampling:\n∂h ∂θ ≈ 1 N\nN ∑\nn=1\nD ∑\nd=1\n∂g(xn)\n∂xnd\n∂xnd ∂θ\n(14)\nwhere xn ∼ f(x). Note that the above estimator does not require Q to be known, as long as f can be sampled.'}, {'heading': 'Application to Mixture Density Weights', 'text': 'If f is a mixture density distribution with K components then\nf(x) = K ∑\nk=1\nπkf k(x) (15)\nand\nfd(xd|x<d) =\nK ∑\nk=1\nPr(k|x<d)f k d (xd|x<d) (16)\nwhere Pr(k|x<d) is the posterior responsibility of the component k, given the prior mixture density weight πk and the observation sequence x<d.\nIn what follows we will assume that the mixture components have diagonal covariance, so that fkd (xd|x<d) = f k d (xd). It should be possible to extend the analysis to non-diagonal components, but that is left for future work. Abbreviating Pr(k|x<d) to p k d and applying the diagonal covariance of the components, Eq. 16 becomes\nfd(xd|x<d) = ∑\nk\npkdf k d (xd) (17)\nwhere pkd is defined by the following recurrence relation:\npk1 = πk (18) pkd = pkd−1f k d−1(xd−1)\nfd−1(xd−1|x<d−1) (19)\nWe seek the derivatives of h with respect to the mixture weights πj , after the weights have been normalised (e.g. by a softmax function). Setting xd = t and differentiating Eq. 17 gives\n∂fd(t|x<d)\n∂πj =\n∑\nk\n[\n∂pkd ∂πj fkd (t) + ∂fkd (t) ∂t ∂t ∂πj pkd\n]\n(20)\nSetting x = x̂ where x̂ is a sample drawn from f , and observing that ∂t ∂πj = 0 if\nt does not depend on f , we can substitute the above into Eq. 6 to get\n∂x̂d ∂πj = −\n1\nfd(x̂d|x̂<d)\n∑\nk\n∂pkd ∂πj ∫ x̂d\nt=−∞\nfkd (t)dt (21)\n= − 1\nfd(x̂d|x̂<d)\n∑\nk\n∂ log pkd ∂πj pkdF k d (x̂d) (22)\nDifferentiating Eq. 19 yields (after some rearrangement)\n∂ log pkd ∂πj = ∂ log pkd−1 ∂πj − ∑\nl\npld ∂ log pld−1\n∂πj (23)\n+\n[\n∂ log fkd−1(x̂d−1)\n∂x̂d−1 − ∑\nl\npld ∂ log f ld−1(x̂d−1)\n∂x̂d−1\n]\n∂x̂d−1\n∂πj (24)\n∂ log pkd ∂πj and ∂x̂d ∂πj can then be obtained with a joint recursion, starting from the initial conditions\n∂ log pk1 ∂πj = δjk πj (25)\n∂x̂1 ∂πj = −\nF j 1 (x̂1) f1(x̂1) (26)\nWe are now ready to approximate ∂h ∂πj by substituting into Eq. 14:\n∂h\n∂πj ≈\n1\nN\nN ∑\nn=1\nD ∑\nd=1\n∂g(xn)\n∂xnd ∂xnd ∂πj ; xn ∼ f(x) (27)\nPseudocode for the complete computation is provided in Algorithm 1.\ninitialise ∂h ∂πj ← 0 for n = 1 to N do draw x ∼ f(x) pk1 ← πk ∂ log pk\n1 ∂πj ← δjk πj\n∂x1 ∂πj ← −\nF j 1 (x1) f1(x1)\nf1(x1)← ∑ k πkf k 1 (x1) for d = 2 to D do fd(xd|x<d)← ∑ k p k df k d (xd)\npkd ← pkd−1f k d−1(xd−1)\nfd−1(xd−1|x<d−1)\n∂ log pkd ∂πj ← ∂ log pkd−1 ∂πj − ∑ l p l d ∂ log pld−1 ∂πj +\n∂xd−1 ∂πj\n[\n∂ log fkd−1(xd−1)\n∂xd−1 − ∑ l p l d\n∂ log f ld−1(xd−1)\n∂xd−1\n]\n∂xd ∂πj ← − 1 fd(xd|x<d)\n∑\nk ∂ log pkd ∂πj pkdF k d (xd)\nend for ∂h ∂πj ← ∂h ∂πj + ∑ d ∂g(x) ∂xd ∂xd ∂πj\nend for ∂h ∂πj ← 1 N ∂h ∂πj\nAlgorithm 1: Stochastic Backpropagation through Mixture Density Weights'}, {'heading': 'Acknowledgements', 'text': 'Useful discussions and comments were provided by Ivo Danihelka, Danilo Rezende, Remi Munos, Diederik Kingma, Charles Blundell, Mevlana Gemici, Nando de Freitas, and Andriy Mnih.'}]
