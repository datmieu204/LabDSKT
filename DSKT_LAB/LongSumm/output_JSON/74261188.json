{
    "abstractText": "Adam Santoro,1,2 Paul W. Frankland,1,2,3,4 and X Blake A. Richards5,6 1Institute of Medical Sciences, University of Toronto, Toronto, Ontario M5S 1AB, Canada, 2Program in Neurosciences and Mental Health, Hospital for Sick Children, Toronto, Ontario M5G 1X8, Canada, 3Department of Psychology, University of Toronto, Toronto, Ontario M5S 3G3, Canada, 4Department of Physiology, University of Toronto, Toronto, Ontario M5S 1A8, Canada, 5Department of Biological Sciences, University of Toronto Scarborough, Toronto, Ontario M1C 1A4, Canada, and 6Department of Cell and Systems Biology, University of Toronto, Toronto, Ontario M5S 3G5, Canada",
    "authors": [
        {
            "affiliations": [],
            "name": "Adam Santoro"
        },
        {
            "affiliations": [],
            "name": "Paul W. Frankland"
        },
        {
            "affiliations": [],
            "name": "Blake A. Richards"
        }
    ],
    "id": "SP:4e75b52af3629fa0e53dc7b2cad088bd0a634bcb",
    "references": [
        {
            "authors": [
                "I Bethus",
                "D Tse",
                "RG Morris"
            ],
            "title": "Dopamine and memory: modulation",
            "year": 2010
        },
        {
            "authors": [
                "ND CrossRef Medline Daw",
                "Y Niv",
                "P Dayan"
            ],
            "title": "Uncertainty-based competition",
            "year": 2005
        },
        {
            "authors": [
                "P Dayan",
                "Y Niv"
            ],
            "title": "Reinforcement learning: the good, the bad and the ugly",
            "venue": "Curr Opin Neurobiol 18:185–196",
            "year": 2008
        },
        {
            "authors": [
                "RJ Dolan",
                "P Dayan"
            ],
            "title": "Goals and habits in the brain. Neuron",
            "year": 2013
        },
        {
            "authors": [
                "BB Doll",
                "DA Simon",
                "ND Daw"
            ],
            "title": "The ubiquity of model-based reinforcement learning",
            "venue": "Curr Opin Neurobiol 22:1075–1081",
            "year": 2012
        },
        {
            "authors": [
                "BB Doll",
                "D Shohamy",
                "ND Daw"
            ],
            "title": "Multiple memory systems as substrates for multiple decision systems",
            "venue": "Neurobiol Learn Mem",
            "year": 2015
        },
        {
            "authors": [
                "SJ Durrant",
                "C Taylor",
                "S Cairney",
                "PA Lewis"
            ],
            "title": "Sleep-dependent consolidation of statistical learning. Neuropsychologia 49:1322–1331",
            "venue": "CrossRef Medline",
            "year": 2011
        },
        {
            "authors": [
                "DR Euston",
                "M Tatsuno",
                "BL McNaughton"
            ],
            "title": "Fast-forward playback of recent memory sequences in prefrontal cortex",
            "year": 2007
        },
        {
            "authors": [
                "DJ Foster",
                "RG Morris",
                "P Dayan"
            ],
            "title": "A model of hippocampally dependent navigation, using the temporal difference learning rule. Hippocampus 10:1–16",
            "venue": "CrossRef Medline",
            "year": 2000
        },
        {
            "authors": [
                "PW Frankland",
                "B Bontempi"
            ],
            "title": "The organization of recent and remote memories",
            "venue": "Nat Rev Neurosci",
            "year": 2005
        },
        {
            "authors": [
                "VE Ghosh",
                "A Gilboa"
            ],
            "title": "What is a memory schema? A historical perspective on current neuroscience literature",
            "year": 2014
        },
        {
            "authors": [
                "D Hassabis",
                "EA Maguire"
            ],
            "title": "Deconstructing episodic memory with construction",
            "venue": "Trends Cogn Sci",
            "year": 2007
        },
        {
            "authors": [
                "G Hinton"
            ],
            "title": "A practical guide to training restricted Boltzmann machines. Momentum 9:926",
            "year": 2010
        },
        {
            "authors": [
                "A Hupbach",
                "R Gomez",
                "O Hardt",
                "L Nadel"
            ],
            "title": "Reconsolidation of episodic memories: a subtle reminder triggers integration of new information",
            "venue": "Learn Mem 14:47–53. CrossRef Medline",
            "year": 2007
        },
        {
            "authors": [
                "S Káli",
                "P Dayan"
            ],
            "title": "Hippocampally-dependent consolidation in a hierarchical model of neocortex",
            "venue": "Neural information processing systems,",
            "year": 2000
        },
        {
            "authors": [
                "S Káli",
                "P Dayan"
            ],
            "title": "Off-line replay maintains declarative memories in a model of hippocampal-neocortical interactions",
            "venue": "Nat Neurosci",
            "year": 2004
        },
        {
            "authors": [
                "JJ Kim",
                "MS Fanselow"
            ],
            "title": "Modality-specific retrograde amnesia of fear",
            "venue": "Science",
            "year": 1992
        },
        {
            "authors": [
                "SB Klein",
                "L Cosmides",
                "J Tooby",
                "S Chance"
            ],
            "title": "Decisions and the evolution of memory: multiple systems, multiple functions",
            "venue": "Psychol Rev 109:",
            "year": 2002
        },
        {
            "authors": [
                "HS Kudrimoti",
                "CA Barnes",
                "BL McNaughton"
            ],
            "title": "Reactivation of hippocampal cell assemblies: effects of behavioral state, experience, and EEG dynamics",
            "venue": "J Neurosci",
            "year": 1999
        },
        {
            "authors": [
                "D Kumaran",
                "JL McClelland"
            ],
            "title": "Generalization through the recurrent interaction of episodic memories: a model of the hippocampal system",
            "venue": "Psychol Rev",
            "year": 2012
        },
        {
            "authors": [
                "D Kumaran",
                "D Hassabis",
                "JL McClelland"
            ],
            "title": "What learning systems do intelligent agents need? Complementary learning systems theory updated",
            "venue": "Trends Cogn Sci 20:512–534",
            "year": 2016
        },
        {
            "authors": [
                "M Lengyel",
                "P Dayan"
            ],
            "title": "Hippocampal contributions to control: the third way",
            "venue": "Neural information processing systems,",
            "year": 2007
        },
        {
            "authors": [
                "D Marr"
            ],
            "title": "A theory for cerebral neocortex",
            "venue": "Proc R Soc Lond B Biol Sci 176:161–234",
            "year": 1970
        },
        {
            "authors": [
                "D Marr"
            ],
            "title": "Simple memory: a theory for archicortex",
            "venue": "Proc R Soc Lond B Biol Sci",
            "year": 1971
        },
        {
            "authors": [
                "D Marr"
            ],
            "title": "A computational investigation into the human representation and processing of visual information",
            "year": 1982
        },
        {
            "authors": [
                "JL McClelland"
            ],
            "title": "Incorporating rapid neocortical learning of new schema-consistent information into complementary learning systems theory",
            "venue": "J Exp Psychol Gen",
            "year": 2013
        },
        {
            "authors": [
                "JL McClelland",
                "BL McNaughton",
                "RC O’Reilly"
            ],
            "title": "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory",
            "venue": "Psychol Rev",
            "year": 1995
        },
        {
            "authors": [
                "V Mnih",
                "K Kavukcuoglu",
                "D Silver",
                "AA Rusu",
                "J Veness",
                "MG Bellemare",
                "A Graves",
                "M Riedmiller",
                "AK Fidjeland",
                "G Ostrovski",
                "S Petersen",
                "C Beattie",
                "A Sadik",
                "I Antonoglou",
                "H King",
                "D Kumaran",
                "D Wierstra",
                "S Legg",
                "D Hassabis"
            ],
            "title": "Human-level control through deep reinforcement",
            "year": 2015
        },
        {
            "authors": [
                "M Moscovitch",
                "L Nadel",
                "G Winocur",
                "A Gilboa",
                "RS Rosenbaum"
            ],
            "title": "The cognitive neuroscience of remote episodic, semantic and spatial memory",
            "venue": "Curr Opin Neurobiol",
            "year": 2006
        },
        {
            "authors": [
                "CM O’Carroll",
                "SJ Martin",
                "J Sandin",
                "B Frenguelli",
                "RG Morris"
            ],
            "title": "Dopaminergic modulation of the persistence of one-trial hippocampusdependent memory",
            "venue": "Learn Mem",
            "year": 2006
        },
        {
            "authors": [
                "RC O’Reilly",
                "JW Rudy"
            ],
            "title": "Conjunctive representations in learning and memory: principles of cortical and hippocampal function",
            "venue": "Psychol Rev 108:311–345",
            "year": 2001
        },
        {
            "authors": [
                "BA Richards",
                "F Xia",
                "A Santoro",
                "J Husse",
                "MA Woodin",
                "SA Josselyn",
                "PW Frankland"
            ],
            "title": "Patterns across multiple memories are identified over time",
            "venue": "Nat Neurosci 17:981–986",
            "year": 2014
        },
        {
            "authors": [
                "R Rojas"
            ],
            "title": "Neural networks: a systematic introduction",
            "year": 1996
        },
        {
            "authors": [
                "DE Rumelhart",
                "GE Hinton",
                "RJ Williams"
            ],
            "title": "Learning representations by back-propagating errors",
            "venue": "Nature",
            "year": 1988
        },
        {
            "authors": [
                "W Schultz",
                "P Dayan",
                "PR Montague"
            ],
            "title": "A neural substrate of prediction and reward. Science 275:1593–1599",
            "venue": "CrossRef Medline",
            "year": 1997
        },
        {
            "authors": [
                "WB Scoville",
                "B Milner"
            ],
            "title": "Loss of recent memory after bilateral hippocampal lesions",
            "venue": "J Neurol Neurosurg Psychiatry 20:11–21",
            "year": 1957
        },
        {
            "authors": [
                "MJ Sekeres",
                "K Bonasia",
                "M St-Laurent",
                "S Pishdadian",
                "G Winocur",
                "C Grady",
                "M Moscovitch"
            ],
            "title": "Recovering and preventing loss of detailed memory: differential rates of forgetting for detail types in episodic memory",
            "venue": "Learn Mem",
            "year": 2016
        },
        {
            "authors": [
                "LR Squire",
                "P Alvarez"
            ],
            "title": "Retrograde amnesia and memory consolidation: a neurobiological perspective",
            "venue": "Curr Opin Neurobiol",
            "year": 1995
        },
        {
            "authors": [
                "R.S. Sutton",
                "Barto",
                "A. G"
            ],
            "title": "Reinforcement learning: an introduction, Vol 1. Cambridge, MA: Massachusetts Institute of Technology",
            "year": 1998
        },
        {
            "authors": [
                "D Tse",
                "RF Langston",
                "M Kakeyama",
                "I Bethus",
                "PA Spooner",
                "ER Wood",
                "MP Witter",
                "RG Morris"
            ],
            "title": "Schemas and memory consolidation",
            "year": 2007
        },
        {
            "authors": [
                "D Tse",
                "T Takeuchi",
                "M Kakeyama",
                "Y Kajii",
                "H Okuno",
                "C Tohyama",
                "H Bito",
                "RG Morris"
            ],
            "title": "Schema-dependent gene activation and memory encoding in neocortex",
            "venue": "Science",
            "year": 2011
        },
        {
            "authors": [
                "E Tulving"
            ],
            "title": "Episodic and semantic memory: 1. Organization of memory. London: Academic",
            "year": 1972
        },
        {
            "authors": [
                "NB Turk-Browne",
                "BJ Scholl",
                "MK Johnson",
                "MM Chun"
            ],
            "title": "Implicit perceptual anticipation triggered by statistical learning",
            "venue": "J Neurosci",
            "year": 2010
        },
        {
            "authors": [
                "SH Wang",
                "RG Morris"
            ],
            "title": "Hippocampal-neocortical interactions in memory formation, consolidation, and reconsolidation",
            "venue": "Annu Rev Psychol",
            "year": 2010
        },
        {
            "authors": [
                "BJ Wiltgen",
                "AJ Silva"
            ],
            "title": "Memory for context becomes less specific with time. Learn Mem 14:313–317",
            "venue": "CrossRef Medline",
            "year": 2007
        },
        {
            "authors": [
                "G Winocur",
                "M Moscovitch"
            ],
            "title": "Memory transformation and systems consolidation",
            "venue": "J Int Neuropsychol Soc",
            "year": 2011
        },
        {
            "authors": [
                "G Winocur",
                "M Moscovitch",
                "M Sekeres"
            ],
            "title": "Memory consolidation or transformation: context manipulation and hippocampal representations of memory",
            "venue": "Nat Neurosci 10:555–557",
            "year": 2007
        },
        {
            "authors": [
                "G Winocur",
                "M Moscovitch",
                "B Bontempi"
            ],
            "title": "Memory formation and long-term retention in humans and animals: convergence towards a transformation account of hippocampal–neocortical interactions",
            "year": 2010
        },
        {
            "authors": [
                "K Wunderlich",
                "P Smittenaar",
                "RJ Dolan"
            ],
            "title": "Dopamine enhances modelbased over model-free choice behavior. Neuron",
            "year": 2012
        },
        {
            "authors": [
                "SM Zola-Morgan",
                "LR Squire"
            ],
            "title": "The primate hippocampal formation: evidence for a time-limited role in memory",
            "venue": "storage. Science",
            "year": 1990
        }
    ],
    "sections": [
        {
            "heading": "Systems/Circuits",
            "text": ""
        },
        {
            "heading": "Memory Transformation Enhances Reinforcement Learning",
            "text": "in Dynamic Environments\nAdam Santoro,1,2 Paul W. Frankland,1,2,3,4 and X Blake A. Richards5,6 1Institute of Medical Sciences, University of Toronto, Toronto, Ontario M5S 1AB, Canada, 2Program in Neurosciences and Mental Health, Hospital for Sick"
        },
        {
            "heading": "Children, Toronto, Ontario M5G 1X8, Canada, 3Department of Psychology, University of Toronto, Toronto, Ontario M5S 3G3, Canada, 4Department of Physiology, University of Toronto, Toronto, Ontario M5S 1A8, Canada, 5Department of Biological Sciences, University of Toronto Scarborough, Toronto, Ontario M1C 1A4, Canada, and 6Department of Cell and Systems Biology, University of Toronto, Toronto, Ontario M5S 3G5, Canada",
            "text": "Over the course of systems consolidation, there is a switch from a reliance on detailed episodic memories to generalized schematic memories. This switch is sometimes referred to as “memory transformation.” Here we demonstrate a previously unappreciated benefit of memory transformation, namely, its ability to enhance reinforcement learning in a dynamic environment. We developed a neural network that is trained to find rewards in a foraging task where reward locations are continuously changing. The network can use memories for specific locations (episodic memories) and statistical patterns of locations (schematic memories) to guide its search. We find that switching from an episodic to a schematic strategy over time leads to enhanced performance due to the tendency for the reward location to be highly correlated with itself in the short-term, but regress to a stable distribution in the long-term. We also show that the statistics of the environment determine the optimal utilization of both types of memory. Our work recasts the theoretical question of why memory transformation occurs, shifting the focus from the avoidance of memory interference toward the enhancement of reinforcement learning across multiple timescales.\nKey words: computational modeling; decision making; episodic memory; memory transformation; reinforcement learning; schema"
        },
        {
            "heading": "Introduction",
            "text": "Over short time periods, the natural world is highly correlated with itself, whereas over longer time periods, short-term correlations give way to larger statistical patterns. For example, if a bird discovers fruit on a tree, then returns to the same tree an hour later, it is likely to find more. In contrast, if the bird returns to the same tree a month later all of the fruit may be gone, making the memory for that individual tree less useful. Nonetheless, combining many such memories provides the bird with general knowledge of where food may typically be found. Hence, two types of memory (specific vs general) may be more or less useful depending on the amount of time that has passed.\nGiven these considerations, it makes sense that the brain relies on multiple systems to guide behavior (Klein et al., 2002; Doll et al., 2012, 2015), including those that capture general patterns (schematic memories) and those that capture specific experiences (episodic memories) (Tulving, 1972; Lengyel and Dayan, 2007). Moreover, humans and animals often rely on recent episodic memories to make decisions, but episodic memories give way to schematic memories over time as part of a memory reorganization process (Moscovitch et al., 2006; Tse et al., 2007; Winocur et al., 2010; Tse et al., 2011; Winocur and Moscovitch, 2011; Richards et al., 2014), sometimes referred to as “schematization” or “memory transformation” (Winocur et al., 2010; Received March 8, 2016; revised Aug. 15, 2016; accepted Sept. 28, 2016. Author contributions: A.S., P.W.F., and B.A.R. designed research; A.S. and B.A.R. performed research; A.S. and B.A.R. analyzed data; A.S., P.W.F., and B.A.R. wrote the paper. P.W.F. was supported by Canadian Institutes for Health Research Grant FDN143227. B.A.R. was supported by Natural Sciences and Engineering Research Council of Canada Grant RGPIN-2014 – 04947 and a Google Faculty Research Award. The authors declare no competing financial interests. Correspondence should be addressed to either of the following: Dr. Paul W. Frankland, Program in Neurosciences and Mental Health, Hospital for Sick Children, 555 University Avenue, Toronto, Ontario M5G 1X8, Canada, E-mail: paul.frankland@sickkids.ca; or Dr. Blake Aaron Richards, Department of Biological Sciences, University of Toronto Scarborough, Toronto, Ontario M1C 1A4, Canada. E-mail: blake.richards@utoronto.ca. DOI:10.1523/JNEUROSCI.0763-16.2016 Copyright © 2016 the authors 0270-6474/16/3612228-15$15.00/0\nSignificance Statement\nAs time passes, memories transform from a highly detailed state to a more gist-like state, in a process called “memory transformation.” Theories of memory transformation speak to its advantages in terms of reducing memory interference, increasing memory robustness, and building models of the environment. However, the role of memory transformation from the perspective of an agent that continuously acts and receives reward in its environment is not well explored. In this work, we demonstrate a view of memory transformation that defines it as a way of optimizing behavior across multiple timescales.\nWinocur and Moscovitch, 2011). The benefits of this episodic to schematic transformation are usually assumed to be reduced memory interference (McClelland et al., 1995; O’Reilly and Rudy, 2001), and the formation of a more stable memory (Squire and Alvarez, 1995). However, these perspectives do not consider the potential advantages of memory transformation for making decisions that exploit the temporal statistics of the environment.\nSome computational work has explored the idea that episodic to schematic transitions could be beneficial for guiding behavior (Lengyel and Dayan, 2007). These results suggest that there is a performance improvement in using episodic memories soon after a novel experience and schematic memories after more experience. However, this considers only the accumulation of data, and not the passage of time. In the real world, there are periods of data accumulation (e.g., foraging) and periods without data (e.g., rest, migration), and memory transformation occurs regardless of whether data are being accumulated or not (Winocur et al., 2007; Richards et al., 2014).\nTo test whether the benefits of memory transformation are a consequence of the accumulation of time itself, we developed a computational model of an agent with both episodic and schematic memories. The agent uses only its position in a 2D environment and rewards found at specific spatial locations to learn a navigational model, store episodic memories, and build schematic memories via replay. We trained the agent in an environment where the reward locations constantly changed, such that new reward locations were correlated in the short-term but were independent and sampled from a stable distribution in the longterm. As well, we varied the amount of time between foraging trials. We show that the best strategy in this environment is to rely on episodic memories after short delays but to shift to schematic memories after long delays, independent of data accumulation. We also find that the timing of this shift depends on the temporal statistics of the environment. When in environments that tend to be consistent for long periods of time, the optimal strategy is to shift to schematic memories slowly. Finally, we explored whether memory transformation was also beneficial when the long-term distribution of rewards was nonstationary. We found that the benefits of schematic memories are limited to cases where the long-term pattern of rewards is relatively stable. These results suggest that the temporal statistics of the world are one of the principal reasons that both episodic and schematic memories are used by the brain to guide behavior. Furthermore, the extent to which an animal relies on detailed or gist-like memories at different times may be tuned to optimize reinforcement learning in different contexts (Moscovitch et al., 2006; Winocur et al., 2010; Winocur and Moscovitch, 2011)."
        },
        {
            "heading": "Materials and Methods",
            "text": "Simulated foraging task. In this study, we use a simulated foraging task wherein a model agent must navigate a space to find a moving reward. Here, the reward moves within a bounded space of arbitrary units, with the boundaries set to [0, 1]. The reward location, l t , moves with incremental shifts within a “bout” or sudden shifts between “bouts” (see Fig. 1). Incremental shifts correspond to the addition of a white noise variable to l t , whereas sudden shifts correspond to a resampling of the reward location according to a predefined multivariate normal distribution in space. Specifically:\nl t l t 1 , if tb B , if tb B (1) Here, [ 1, 2], where 1, 2 N(0, ). Similarly, [ 1, 2], where 1, 2 N( , ), and thus represents a randomly selected new\nlocation for the reward at the start of a new bout with mean value . tb refers to the time within a bout of length B. In some simulations, B was held at a constant value (see Figs. 4, 6, 7), whereas in other simulations, it was sampled from an exponential distribution with rate parameter bout (see Fig. 5).\nImportantly, this formulation ensures that over short time spans the expected value of the reward location is correlated with its last location, whereas over longer time spans the reward location tends to be a random variable that is independent of any previous, specific locations, given knowledge of the underlying distribution (see Expected reward probability distribution). Although this is a very abstract environment, we would argue that this principle tends to hold true in the real world (i.e., the world tends to be correlated with itself over short periods of time and regress to a general distribution over longer periods of time). Additionally, by changing or bout, the degree of the correlation in reward locations over different time spans can be modulated.\nAt the start of each trial, the agent is initiated in a random location in the space. Whenever a reward is found, a trial ends and the agent “rests” for an intertrial delay. Importantly, although the agent is not interacting with the environment during the intertrial delay, the reward location continues to move, meaning that the reward will be at a new location when the agent begins a new trial. Hence, the length of the intertrial delay influences the probability that the reward location is correlated with the location where the agent last found it.\nWe tasked the agent to find 120 rewards in total and measured its performance on the final 100 rewards (the first 20 rewards were considered the “pretraining period” for the agent). We calculated the agent’s performance as the reward rate (s 1), or mean latency to reward (s). Reward rate is computed as the inverse mean latency to reward. Each simulation (i.e., the finding of 100 rewards) counts as one sample in the data presented, within which the mean or inverse mean is computed. In all the data presented in this paper, we use n 20 samples per condition.\nBasic agent architecture. The agent we use in this simulated foraging task has three major components: an episodic memory system, a schematic memory system, and a forward model for navigation (see Fig. 2A). In addition, the agent has a critic module that estimates the value of any given location in space based on the agent’s reward history, which then enables the calculation of a prediction error for rewards (for details, see Temporal difference learning). In most of the simulations in this paper, the agent makes decisions about where to move in the environment by using the outputs from its episodic and schematic memory systems as goal locations, and using the forward model to determine how to navigate to those goals. The one exception is the “habitual” agent (see Habitual agent; see Fig. 6), which uses an actor module coupled to the critic module to implement a typical actor-critic reinforcement learning strategy (Sutton and Barto, 1998; Foster et al., 2000).\nEpisodic system architecture and information flow. The episodic memory system in our agent is a neural network designed to have the following properties: (1) store specific reward locations, (2) emphasize more recent memories, and (3) store new memories continuously, with the strength of storage modulated by the relevance of the memory to finding rewards. We chose these properties because they are in line with the characteristics of episodic memory in mammals (Clayton et al., 2007; Conway, 2009; Kumaran et al., 2016), although we note that these characteristics are by no means a complete representation of true episodic memory, which has many more components to it (Hassabis and Maguire, 2007; Conway, 2009). We designed the episodic memory network as an abstraction of the medial temporal lobes (see Fig. 2A), which are central to episodic memory storage in mammals (Kumaran et al., 2016). Within this framework, our episodic network consists of a spatial encoder, a recurrent network (in analogy to the CA3 region of the hippocampus) and a network of place field units (in analogy to the CA1 region of the hippocampus). Although we make obvious reference to these subregions, we note that we are capturing the believed computations of these regions, and are not necessarily making statements or predictions of the neurophysiology, or algorithms implemented within them. To borrow from Marr’s level of analysis (Marr, 1982), we are assuming certain computational properties while being general, and/or agnostic about the algorithmic and implementation details. The number of units per region is as follows: Ns 2\n(spatial encoder), Ne 490 (autoencoder), and Nm 980 (place cells). The initial synaptic weights between the spatial encoder and autoencoder, autoencoder and itself (recurrent connections), and autoencoder and place cells are sampled from a Gaussian distribution, N(0, 0.1).\nThe spatial encoder acts as the input for the episodic network, responding to location-based information. Spatial inputs (i.e., Cartesian coordinates) elicit activation states equal to the coordinate values (one unit represents the x position, one unit represents the y position). Population activity for the autoencoder, e, is calculated as a function of the weighted sum of the spatial encoder activity, s, as follows:\ne sig(WSE-AEs) (2)\nThe autoencoder is a three-layer feedforward network through time, with the synaptic weight matrices WAE-AE being equal between each “layer” of time. Autoencoder activities in subsequent time layers are calculated as functions of the previous layer’s activity and the recurrent weight matrix. The superscript in autoencoder activity (e.g., e(0)) refers to the feedforward time layer, indexed as an element of the set (0, 1, 2). As such, activity is calculated as follows: e t 1 sig WAE-AE e\nt , where e(0) sig(WSE-AE s). The final time layer, e\n(2), then projects to the place cells.\nThe activity of the place cells is the ultimate memory readout for the episodic system and is denoted by m. These activities are calculated differently depending on whether the agent is encoding its location or retrieving a memory. When retrieving a memory, activity is calculated similar to the autoencoder as follows:\nm sig(WAE-PC e (2)) (3)\nHowever, when encoding a location, place cell activity is calculated using each unit’s place cell receptive field and the current location as follows:\nmi xt e xt si 2 2 m2 (4)\nwhere xt is the vector of the agent’s current position, si is the vector of the center of cell i’s place field, and m controls the breadth of the place fields. Here, si s1, s2 i, where s1 and s2 are each uniformly sampled from the interval [0, 1], and m 0.16.\nEpisodic memory storage. In line with the third property of episodic memories listed above, as the agent moves throughout space, it constantly encodes its location, but in a manner that is modulated by the relevance to its goal of finding rewards. To do this, it passes activity through the spatial encoder and into the autoencoder. The autoencoder activity state is then stored in its recurrent synapses using a backpropagation through time algorithm (Rojas, 1996) computed to three time steps. And so, the autoencoder, which is a three-layer feedforward network through time, learns to recapitulate this spatial encoder-driven activity state in its final temporal layer as it passes activity through its recurrent synapses. Thus, the supervised training vector for the final autoencoder layer is identical to the initial activity vector in the autoencoder given the spatial encoder input (see below). However, to ensure that the storage is goal-relevant, we modulate the learning rate by a prediction error term (see Temporal difference learning).\nMathematically, the autoencoder’s initial activity is dependent on spatial encoder input, e, and the synaptic weight matrix between the spatial encoder and the autoencoder, WSE-AE as follows:\ne(0) sig(WSE-AE s) (5)\nwhere the superscript in e(0) refers to the feedforward time layer, indexed as an element of the set (0, 1, 2). The activity states in subsequent layers are calculated similarly as follows:\ne(t 1) sig(WAE-AE e (t)), t {0, 1} (6)\nThe weight matrix is updated in accordance with the derivation in Rojas (1996), but with the additional modulation of the prediction error term, t as follows:\nt Q(t)(d(t) WAE-AE (t 1)) (7)\nWAE-AE t 1 t (t)e(t 1) (t 1)e(t)), t 1, 2 (8)\nWAE-AE ( WAE-AE\n(0) WAE-AE (1) )\n2 (9)\nwhere is the error vector computed for a particular layer, Q is the derivative matrix for all the units in that layer, d is the difference between the final layer activity state and the training data (i.e., d e(2) e(0)), WAE-AE is the autoencoder weight matrix, and is the learning rate. At each time step, one training epoch occurs. However, when a reward is found, 50 training epochs occur.\nThe weights between the autoencoder and place cells are learned using the perceptron learning rule, treating the system as a two-layer feedforward network. Here, the input is e(2) and the training data are place cell activity mt m1 xt , m2 xt , …, mNm xt , where mi xt is place cell i’s activity given the agent’s current position. With this training, the network learns to map the activity patterns stored in the autoencoder to place field activity patterns. This is what then allows the system to recall place field patterns it has previously encountered. Indeed, with these learning rules, the episodic network exhibits the desired properties, showing an ability to recall specific place field patterns, but with a clear bias toward the most recent reward location (see Fig. 2B).\nSchematic system architecture and information flow. The schematic system is a neural network designed to store a general statistical model of reward locations, in line with the current understanding of schematized memories in the mammalian brain (Ghosh and Gilboa, 2014; Richards et al., 2014). To achieve this, we built the schematic system as a Restricted Boltzmann Machine (RBM), which is a two-layer neural network architecture that can store a generative model of the probability distribution of a dataset (Hinton, 2010), and which has been used to model schematic memory in previous papers (Káli and Dayan, 2004). The lower layer of the network is a direct projection of the place cells from the episodic network. This layer functions as the visible layer, v, in the RBM and contains 980 units. The second layer functions as the hidden layer in the RBM, h, which models the probability distribution of its inputs and consists of 300 units. Thus, the second layer receives information from the place cell projection and determines the statistical regularities contained within (see Fig. 2D). These two layers are connected bidirectionally, and symmetrically, with a weight matrix WCTX. Again, despite the obvious analogy to the neocortex, we emphasize that we are agnostic as to the brain’s actual implementation of the schematic store. Indeed, there is some evidence to suggest that the traditional “episodic” regions of the medial temporal lobes may be capable of learning probabilities or statistics across memories (Turk-Browne et al., 2010; Kumaran and McClelland, 2012).\nSchematic memory storage and recall. Learning in the schematic network is conducted offline, via episodic replay events that occur during “rest” at the end of a trial. This design was motivated by the current understanding of the neurobiology of memory transformation, wherein schematic memories are built via replay in the hippocampus during rest (Frankland and Bontempi, 2005; Winocur et al., 2010). Details of the replay events are provided in the next section.\nTraining of the schematic network is accomplished with the contrastive divergence algorithm. According to the contrastive divergence, algorithm computed to one step (Hinton, 2010) as follows:\nwi, j vihj data vihj reconstruction (10)\nwhere wi, j is the weight in WCTX between unit i in the visible layer and unit j in the hidden layer. The first term, vihj data, is computed in a single step by clamping the visible layer to some training data, and sampling the hidden layer. Place cell activity states, as they are replayed during offline states after a reward is found, constitute the input at the visible layer, and hence the training data (for details on how these training data are generated, see Episodic replay). The probability of activation of a hidden unit is stochastic and is given by the following:\nP hj 1 v) sig bj iviwi, j (11)\nwhere bi is the bias to unit i. That is, the value of hidden unit hj is set to 1 given an input in the visible layer with a probability defined by the sigmoid of the sum of all its inputs. So, as the weights of the inputs to a particular hidden unit increase, the probability of its activation increases.\nDuring the reconstruction step, the activity of a visible unit is given by the following:\nvi P vi 1 h) sig ci j hjwi, j (12)\nwhere cj is the bias to unit j the difference here being that the activity state of the reconstructed visible unit is set to its probability value, rather than being set to 1 with some probability. This update allows us to reconstruct a visible layer with values contained within the interval (0, 1), which better map onto actual place cell values. The result is a more accurate determination of the hidden layer’s prediction of a particular place cell activity state. Recall in the schematic system involves a single reconstruction step as described above, taking a cue activity vector over v, passing it to the hidden layer, then passing it back to determine the probabilities in the visible layer, which then constitute the recalled memory.\nThe products vihj for both the data and the reconstruction are determined by computing the outer products of the vectors of activation. The overall weight update to the weigh matrix is given by the following:\nWCTX (vdata hsampled vreconstructed hreconstructed) (13)\nwith being the learning rate. We use 200 epochs of training for the schematic network in each “rest” period for the agent.\nReplay. Replay is initiated by the episodic network using random activity in the spatial encoder. This is then propagated through to the autoencoderplace cell system. Here, the activity elicits a recall event by first triggering pattern completion in the autoencoder recurrent network by running it forward for three time steps. The completed pattern is then used to activate the place cells. The resultant place cell activity pattern, mR, is then passed to the schematic system visible layer, v, and schematic training occurs using mR as the training data. Because episodic storage is modulated by a prediction error term, the result is that the schematic system tends to receive any recently discovered reward location for training, in line with the in vivo recording literature (Kudrimoti et al., 1999; Euston et al., 2007). This is potentially more biologically plausible than the assumption of iid sampling from the episodic memory store and thus is an important difference from previous models of consolidation (McClelland et al., 1995; Mnih et al., 2015). The ultimate result of replay is that the schematic system learns a generative model of relevant reward locations. We note that this is consistent with a recent proposal regarding the potential utility of memory replay for learning goal-relevant statistics (Kumaran et al., 2016).\nMoving through space. As mentioned above, to navigate through space the agent uses a forward model and an action selector. The forward model is a three-layer neural network: the first layer contains 988 units, consisting of the 980 place cells, and 8 action units. The last layer contains 980 place cell units. The action units at N, NE, E, SE, S, SW, W, NW correspond to each of the eight principal cardinal directions. The network functions to predict place cell activity should a movement in a direction be taken; that is, given current place cell activity, m xt , and a potential action choice at (e.g., N, the network outputs a vector in its final layer that is a prediction of place cell activity should a movement be taken at the current position in the north direction; see Fig. 3A). So, at each time step, the network cycles through all eight potential actions, setting the appropriate action unit, ai, to 1, and initiates a prediction for the outcome of that action using current place cell activity, m xt .\nThe probability that an action is chosen by the action selector is as follows:\nP at ai 1\nmO m (xt 1 ai) (14)\nwhere m (xt 1 ai) is the predicted place cell activity pattern and where mO is the combined memory output from the episodic and schematic memory systems (see below). To introduce some randomness in choices\nas the task progresses, the probability that some action choice ai is taken is calculated as follows:\nP at ai R mO m (xt 1 ai) 1 j mO m (xt 1 aj) 1\n1 R\n8 (15)\nwhere R is a random policy unit that decreases as time within a trial increases as follows:\nt 1 R\nt R tL 4000\n(16)\nwhere tL is the time since the start of the current trial. R is bound to the interval (0, 1) and resets to 1 at the start of every trial. Therefore, the agent shifts toward random actions as the trial proceeds without finding a reward. This design helps to ensure that the agent explores the space sufficiently if it is having trouble finding a reward.\nUltimately, when the agent is not behaving randomly, it chooses the action that it predicts will bring it closer to the location recalled from memory, mO (see Fig. 3B). To compute mO, a recall event is initiated at each time step. This is accomplished by allowing the current location, xt to act as a cue for the episodic and schematic networks. Activity passes through the episodic and/or schematic system, as described previously, to produce an output (mE or mS for the episodic and schematic outputs, respectively, or mO; for more information on combining outputs using a policy unit, see below).\nThe forward model is trained in an online manner to predict place cell\nactivity (i.e., m (xt 1 at)) given current place cell activity (i.e., m(xt)) and an action. After a movement is chosen by the agent, the true value of m(xt 1 at) is computed as the agent moves in the space and its place cells are activated, and this activity is used as a training vector for the forward model given its previous place cell activity m(xt) and action choice at as inputs. Thus, the forward model is only trained using place cell activity from positions it actually traverses and movements it actually makes. Training proceeds using a back-propagation algorithm (Rumelhart et al., 1988) with a learning rate of 0.05.\nEpisodic and schematic policy unit. As described above, memory recall events produce goals for the forward model, mO. These goals are convex combinations of the outputs from the episodic and schematic memory systems, mE and mS, respectively. Specifically:\nmO mE (1 )mS (17)\nwith being a policy unit that sets the balance between episodic and schematic control. is computed using an exponential function as follows:\nt 1 te t, if Rt 01, if Rt 1 (18) where is the exponential decay constant. Importantly, this means that, in the absence of a reward, over time the memories being used to guide navigation gradually switch from what the episodic system is recalling to what the schematic system is recalling. This is ultimately how we implement the process of memory transformation in this network. We emphasize again that we remain agnostic as to the actual mechanisms in the brain. Indeed, we think it extremely unlikely that memory transformation is implemented by the exponential decay of a single policy unit. But we note that this captures the general computational principle of memory transformation, and it leads to a switch in the agent’s foraging behavior from focusing on specific locations to focusing on statistical patterns of locations, as we previously observed in water-maze search behavior in mice (Richards et al., 2014).\nTemporal difference learning. As described above, the agent modulates the strength of memory storage using an internal prediction error, t. The agent calculates t by estimating the value of each position in space with its critic function, C m(xt ), and setting as follows:\nt Rt C m xt , if Rt 1 C m(xt 1 ) C m xt , otherwise (19) Where is a temporal discounting factor and C m(xt 1 ) is the new critic value after a move has been made at time t. C m(xt ) is calculated as a\nweighted linear sum of the place cell units: C m(xt ) WPC-Critic m(xt). The weights, WPC-Critic, are updated at each time step via a temporal difference learning algorithm, as in previous models of navigation (Foster et al., 2000) as follows:\nWPC-Critic tm(xt) (20)\nHabitual network. The habitual, or model-free, network (see Fig. 6) is an actor-critic network (Sutton and Barto, 1998) that consists of three components, two of which were described above: place cell units that receive location coordinates as their input and output f(xt), and a critic that outputs C m(xt ). The third, novel component is the “actor.” The actor consists of 8 units corresponding to the principle cardinal directions and receives connections from the place cell units (see Fig. 6A). As the agent moves through space, place cell-to-actor weights, WPC-Actor, are modulated according to the critic’s computed temporal difference error (for this computation, see the previous section) and the action selected as follows:\nWPC-Actor t at m(xt) (21)\nwhere is the learning rate, at is the selected action, and t is the temporal difference error computed by the critic. Hence, the model-free network learns a value function across space via the critic and uses this predicted value to influence actor choices given place cell activity. Ultimately, the model-free network learns to select appropriate actions given m(xt) and is driven to reward locations based on this mapping. For a comprehensive explanation of an actor-critic network using place cells, see Foster et al. (2000).\nExpected reward probability distribution and Kullback–Leibler divergence difference score. To estimate whether the episodic or schematic systems provided better predictions for the reward location, we compared the distribution of recalled locations with the expected probability distribution for the reward. Specifically, if a new bout starts at time ts with the reward at location xs, and tb is the length of the new bout (with tb Exp bout ), then according to Equation 1, the expectation with respect to tb of the reward probability distribution function at time t ts is given by the following:\nR x, t tb P t ts tb PDFNorm(x xs; 0, t ts )\nP t ts tb PDFNorm(x; , )\n1 CDFExp(t ts; bout )PDFNorm(x xs; 0, t ts )\nCDFExp t ts; bout PDFNorm(x; , ) (22)\nwhere CDFExp is the cumulative distribution function for the exponential distribution and PDFNorm is the probability density function for the normal distribution. Essentially, this equation shows that the expectation of R x, t is comprised of a Brownian motion term multiplied by the probability that a new bout has not occurred, plus a Normal distribution with mean multiplied by the probability that a new bout has occurred.\nComparison between the expected reward probability distribution and the output of the memory systems was accomplished with a Kullback–Leibler divergence difference score. Formally, this difference score, S, was defined as follows:\nS DKL Z mS) R x, t tb DKL Z mE) R x, t tb\nDKL Z mS) R(x, t tb ] DKL Z mE) R(x, t tb ] (23)\nwhere DKL is the Kullback–Leibler divergence and Z(.) is a distribution defined as the normalized inverse Euclidean distance in place cell activity between each point in space and the memory trace. According to this formula, S is closer to 1 when the episodic memory is a better match to the expected distribution reward, and S is closer to 1 when the schematic memory is a better fit to the expected reward distribution.\nSymbols and parameter values. Table 1 provides a list of all of the symbols used in the equations above and provides the values that were used for the parameters in the simulations.\nSoftware. All simulations were performed using custom code written in the Python programming language (RRID: SCR_008394) with the NumPy (RRID: SCR_008633) and SciPy (RRID: SCR_008058) libraries. The software is freely available as a github repository (https://github.com/adamsantoro/episodic-semantic-network.git)."
        },
        {
            "heading": "Results",
            "text": "Simulating foraging and the passage of time In natural environments, resources such as food have predictable, but often changing, locations. Importantly, one can distinguish two categories of change. First, there are small incremental changes where the resource, or reward, stays in approximately the same location, but drifts over time. In such situations, memories for recently found reward locations hold high predictive value for future reward locations because reward location deviations are small. Second, there are sudden changes, where reward location deviations may be quite large. In such cases, memories for recently found rewards may not hold high predictive value, but a statistical model of where rewards occur in general could be advantageous.\nTo simulate both types of change in an environment, we devised a foraging task wherein a reward is located in a bounded 2D space, with the reward location changing in an incremental or sudden fashion (Fig. 1). Time is divided into distinct epochs (or bouts), of length B, with each bout corresponding to a period of relative stability in the environment where the reward location shifts in an incremental manner. The time in a given bout, tb, is set to zero at the start of the bout and increments upwards until tb B, at which point a new bout begins. When a new bout begins, the reward location is randomly sampled from a 2D normal distribution with mean , and so, potentially large and sudden shifts in the reward location can occur. (Formally, the location of the reward at any time, l(t), is given by Eq. 1). When a reward is found, a “rest” period is enforced before the start of a subsequent foraging trial, which is called the intertrial delay. During this delay, the reward location continues to move. So, large intertrial delays most probably entail large deviations from the last found reward location, whereas small intertrial delays probably entail only small deviations from the last found reward location. Therefore, we hypothesize that, when the delay is small, specific memories will be more predictive, but when the delay is large, a generalized model will be more predictive."
        },
        {
            "heading": "A neural network model of a foraging agent with two memory systems",
            "text": "To perform this task, we developed an agent that moves through the 2D space searching for rewards. Ultimately, the agent’s search behavior is governed by a model-based system that consists of three interacting components: an episodic memory store, a schematic memory store, and a navigation system (Fig. 2A). The episodic memory store performs computations thought to occur in the medial temporal lobes. The first stage is a spatial encoder, which receives the current agent position as Cartesian coordinates. The encoder acts as an interface to the mnemonic system, consisting of an autoencoder (analogous to CA3) and a place cell cognitive map (analogous to CA1). The autoencoder functions to encode activity states induced by the spatial encoder. Place cell activities are calculated differently depending on whether or not the agent is engaged in memory storage or memory recall. When the agent is engaged in memory storage, we assume that the place cells receive direct spatial information from the spatial encoder.\nThe schematic memory in the agent is a two-layer generative model (i.e., it stores information about probability distributions and can sample from them). This is in line with the current understanding of schematic memory (Winocur et al., 2010; Ghosh and Gilboa, 2014) and a previous computational model of schematic memory in the neocortex (Káli and Dayan, 2000, 2004). The input layer to the schematic memory system is a direct copy of the place cells in the episodic system, and it is trained with place cell activity generated via offline replay in the episodic system during the intertrial delay (the learning algorithm is detailed in Eqs 10 –13). Recall in the schematic system involves cuing it with a given place cell activity pattern at its input layer, then activating the upper layer, before reactivating the input layer via the upper layer, providing a new set of place cell activities modified to the expectations of the schematic memory system.\nBecause the episodic system stores recently discovered unexpected reward locations, replay provides the schematic system with data that reflect the underlying distribution from which reward locations are sampled between bouts. Thus, recall in the episodic and schematic systems provides very different patterns of place cell activity: episodic recall leads to place cell activity patterns that reflect the most recently discovered reward location, whereas schematic recall leads to place cell activity patterns that match the overall pattern of where new platform locations appear (Fig. 2B–D). Put another way, the episodic system recalls specific, recent reward locations, and the schematic system recalls the probability distribution that governs the location of rewards in the environment."
        },
        {
            "heading": "Navigation by the foraging agent",
            "text": "The final major component of the agent is the navigation system, which is composed of a forward model and an action selector. The navigation system uses place cell activity to predict which action would bring the agent closer to goals recalled by the memory systems (Fig. 3A). The goals, mO, are a combination of the place cell activities generated by recall in the episodic and schematic memory systems. The extent to which the goal reflects episodic or schematic recall is determined by a policy unit, , such that 1 ensures purely episodic goals, 0 ensures purely schematic goals, and 0 1 ensures goals that are\nsome mixture of episodic and schematic recall (Fig. 2B–D; see Eq. 17).\nActions at any point in time, at, correspond to movements in any one of the eight principle cardinal directions (i.e., at N, NE, E, SE, S, SW, W, NW ). At every time step, the forward model receives both the current location as encoded by the place cell activities, m(xt), and a potential action, ai. It then predicts subsequent place cell activities should this action be\ntaken, m (xt 1 ai). The action selector compares this prediction to the goal location mO and chooses at ai to bring it closer to mO (Fig. 3B). (However, the action selector becomes increasingly random as trials proceed to encourage exploration, see Eqs. 14–16.)\nTo make the situation faced by the agent more ecologically realistic, the forward model is trained online during the search for a reward. In other words, although the forward model provides the agent with an understanding of the consequences of movement in the environment, it does not possess this understanding a priori. Therefore, performance in this task depends critically on the ability of the forward navigation model to accurately and rapidly learn to predict future place cell activity given the agent’s current state and a potential action. So, as an initial test of the system, we examined the forward model’s ability to guide search behavior. The forward model quickly learned to predict future place cell activity, exhibiting a vastly reduced error in its predictions in as quickly as 10 time steps, regardless of whether it was using only episodic recall, schematic recall, or a combination for its goals. As the agent completed the task and found rewards, it was reinitiated in random locations in space where the forward model had no experience. This resulted in spikes in the error rate for the forward model’s predictions (Fig. 3C, dashed lines). To show that this increase in learning error did not compromise the agent’s ability to navigate, we performed a control experiment (Fig. 3D,E). First, the agent explored the space until it found a reward in the due north, or due east location in the space. Next, it was transported to a location directly south (for the north condition) or west (for the east condition), and the navigation system\ncalculated what its next action would be. If the navigation system were to function properly, an accurately encoded memory should direct the agent to choose the action “north” for the north condition (Fig. 3D) and “east” for the east condition (Fig. 3E). Indeed, the most probable actions selected by the action selector were the correct choices (Fig. 3D,E). Most importantly, the distribution of action probabilities did not change appreciably even in instances where the agent was in a new location with higher levels of error in the forward model’s predictions (Fig. 3D,E, bottom). Together, these results indicate that the navigation system functions adequately within just a few time steps in the environment and continues to perform well even as the forward model’s error momentarily increases throughout the task. Moreover, it exhibits consistent performance whether it uses just its episodic system, schematic system, or a combination for its goals. Shifting from episodic to schematic memory over time improves agent performance To explore how memory transformation (i.e., a shift from episodic to schematic memory over time) might improve reinforcement learning in our foraging task, we implemented a time-dependent decay in the variable (Fig. 4A). Specifically, decayed exponentially over time with decay constant , but increased back to 1 whenever a reward was found (Fig. 4B; see Eq. 18). We note that this decay occurred with the passage of time, independently of data accumulation. Hence, decayed during the intertrial delays as well as during foraging. This design ensured that after short intertrial delays the goals used by the navigation system were primarily episodic (as a reward was found recently in this case), and after long intertrial delays the goals used by the navigation system were primarily schematic.\nWe predicted that this system of decaying would lead to improved performance in foraging, given that recent, specific locations would be highly predictive of the reward locations with small intertrial delays, but the overall probability distribution governing the interbout sampling would be more predictive with large intertrial delays. As expected, we observed that, when we clamped 1 (purely episodic goals), the agent performed better at short intertrial delays than long intertrial delays (Fig. 4C, blue circles). In contrast, when we clamped 0 (purely schematic goals), we found that the performance of the agent was largely flat across intertrial delays, such that it was worse than the purely episodic system at short intertrial delays but better than the purely episodic system at longer intertrial delays (Fig. 4C, white circles).\nGiven these results, we predicted that a system with exponential decay of would combine the best performance of both systems. Indeed, we found that the “transformation” model exhibited better overall performance than either the purely episodic of purely schematic systems. Indeed, the transformation from episodic to schematic memory did not merely achieve the best of the reward rates from either individual memory system. Instead, it had a higher rate of reward discovery than either system in isolation at every intertrial delay (Fig. 4C, orange circles). This result was somewhat unexpected, although we believe that it can be explained by the fact that a combined goal (0 a 1) is a better predictor when some amount of time has passed because the expected location of the reward itself would be determined by some combination of the most recent specific location and the probability distribution governing interbout samples. Our data suggest that, if an agent is presented with an environment where short-term, temporal correlations give way to long-term stochastic patterns, then reinforcement learning can be enhanced by memory transformation.\nMemory transformation can be optimized to the temporal dynamics of the environment In our simulated foraging task, there is regularity to the rate at which new bouts occur. As such, we reasoned that a combined system could optimize its performance by tuning the speed at which it switches between its episodic and schematic systems. In other words, if new bouts occur frequently, meaning that the environment regresses to the long-term distribution rapidly, then it may be best to switch to the schematic system more quickly. In contrast, if new bouts occur infrequently, meaning that the reward locations remain correlated with the previous location for extended periods of time, then it may be best to have the episodic system drive behavior for longer.\nTo explore this idea, we measured the degree of matching between the memory output for the episodic and schematic systems to an analytically computed expected probability distribution of reward locations under conditions where the bout length was sampled from an exponential distribution (Fig. 5A, B, inset). Because this introduced a new random variable (i.e., bout length, whose sampling was controlled by different values of the rate parameter bout), we reasoned that different values of bout would result in different degrees of matching to the episodic and schematic systems as a function of time. The reward distribution (i.e., Gaussian Brownian motion with variable length bouts sampled from an exponential distribution, see Eq. 22) was compared with the distribution of recalled place cell activities output by both the episodic and schematic systems using a Kullback–Leibler Divergence difference score (Fig. 5B; see Eqs. 22,23). The manner in which we designed this difference score ensured that a score 0 corresponds to a better match between the episodic memory output and the reward distribution, whereas a score 0 corresponds to a better match between the schematic memory output and the reward distribution. As expected, as the bout values increased\n(i.e., as the mean bout lengths increased), the time it took for the schematic system to be more predictive increased. Conversely, as bout decreased, the episodic system ceased to be more predictive more quickly (Fig. 5B).\nTo test whether we could tune the rate at which memory transformation occurred to the environmental statistics, we ran the agent in the reward-finding task under different bout values while modulating the rate at which the agent decayed toward schematic recall ( ; see Eq. 18). To visualize the effects of modulating , we computed the percentage increase in rewardfinding latency above the best-performing (Fig. 5C, larger circles) for each value of bout. In line with our predictions, as the value of bout increased, the optimal increased (Fig. 5C). However, one unexpected result was that the disadvantage of switching to the schematic system too quickly in conditions with a high bout was drastic (Fig. 5C, yellow circles, low values). We believe that this may be because the rapid transformation to the schematic system prevents the agent from exploiting the precision of the episodic system under conditions where the reward tends to remain in a similar location. Alternatively, the effects of switching to the episodic system too slowly under the lower bout conditions (Fig. 5C, blue circles, high values) were not as pronounced, which may be due to the fact that the episodic system will still direct the agent to the general area in which rewards occur, unless it is recalling a recent outlier.\nThese results suggest that the speed at which memory transformation occurs can be optimized to the environment: environments with rapid regression to a pattern demand rapid transformation, whereas environments that regress to a pattern slowly demand extended use of episodic memory. However, we also found that switching to the schematic system too quickly is particularly disadvantageous. This implies that a potentially good\naction-reward associations in the environment (Dolan and Dayan, 2013). As such, it may be that a goal-directed system, such as our memory transformation agent, would be superior to a habitual agent in a variable environment, though inferior in a constant environment.\nTo explore this, we built a habitual agent for our foraging task. The agent was based on a previous model of reinforcement learning for navigation (Foster et al., 2000). Like our memorytransformation agent, the habitual agent possessed a set of place cells and a critic system to estimate the value of different locations in the environment. However, the habitual system did not navigate using an explicit goal recalled from memory. Instead, it used an “actor” module that decided which action to select based purely on the current place cell activity (Fig. 6A). In other words, the habitual agent made decisions using location-to-action associations that it formed during learning (see Eqs. 19 –21). We then compared the performance of our memory transformation agent to the habitual agent, both at a variety of intertrial delays and at different levels of variance in the incremental ( ) and interbout ( ) movement of the reward (Fig. 6B–E, top rows).\nWhen there was no variance in the environment (i.e., when the reward stayed in one place), we found that the memory transformation agent (Fig. 6B, blue circles) was actually better at foraging than the habitual agent (Fig. 6B, white circles), when both agents were given equivalent amounts of training (20 pretraining trials followed by 100 regular trials). This held regardless of the intertrial interval. However, we found that, with overtraining (400 additional trials), the habitual agent came to outperform our memory transformation agent (Fig. 6B, gray circles). We believe that this is because the memory transformation agent is using its forward model, which is accurate but not perfect, whereas in a constant environment the habitual agent learns a\nvery accurate map of the value of each action at different points in space. Hence, our data suggest that, in a nonchanging environment, a habitual system is better than a goal-directed system as long as sufficient data can be accumulated.\nHowever, the situation was very different when the reward location was variable. In these cases, the memory transformation agent generally outperformed the habitual agent. In particular, if we introduced some within-bout variance, we found that the memory transformation agent was markedly better than the habitual agent at lower intertrial delays, even if the habitual agent received five times the training (Fig. 6C). We saw similar results at low intertrial delays if we introduced between-bout variance (Fig. 6D) or both within- and between-bout variance (Fig. 6E). However, interestingly, we found that the memory transformation agent did not outperform the habitual agent at higher intertrial delays (Fig. 6C–E). Indeed, with both within- and between-bout variance present, we observed a slight advantage for the overtrained habitual agent at the longest intertrial delay we tested (Fig. 6E). Nonetheless, given the general performance of the two systems, it would appear that in a variable environment it is generally better to rely on a goal-directed agent with memory transformation capabilities, except after very long periods of time.\nThe benefits of schematic memory depend on a stable long-term distribution of reward locations When considering the benefits of memory transformation in our foraging task, we wondered how much it depended on the presence of a stable long-term distribution of reward locations. In other words, if the distribution of between-bout reward locations was nonstationary, would schematic memories actually provide any benefit, or would they become “out-of-date” too quickly to be useful? Further, we wanted to explore whether the schematic system’s performance in a nonstationary environment would depend on the amount of training.\nTo explore these issues, we trained the agent with either episodic only ( 1) or schematic only ( 0) memories in a set of tasks where the mean of between-bout reward locations ( , see Fig. 1) was itself sampled from another distribution after\nevery 20 trials (Fig. 7A). Specifically, we tested the agent in conditions where was resampled every 20 trials from either a normal distribution with low variance (N(0.7, 0.05); Fig. 7B), a normal distribution with high variance (N(0.7, 0.11); Fig. 7C), or a uniform distribution (U(0, 1); Fig. 7D). As well, we examined the performance of the agent during the first 20 trials (pretraining; Fig. 7, left column), the second set of 20 trials (early training, Fig. 7, middle column), and the last set of 20 trials (late training,\nFig. 7, right column). In agreement with other studies of episodic versus schematic control (Lengyel and Dayan, 2007), we found that, during the first trials of pretraining, the episodic system outperformed the schematic system, regardless of the type of resampling or the intertrial delay (Fig. 7, left column). However, differences emerged when we examined later training. In particular, we found that, if the resampling of was done with low variance, then as training proceeded, the schematic memory system came to outperform the episodic system at high intertrial delays (Fig. 7A, middle, left), as we observed with a stationary distribution (Fig. 4B). In contrast, when resampling was done with high variance, the schematic system only ever tied with the episodic system in its performance at higher intertrial delays (Fig. 7B). Furthermore, when resampling was done with a uniform distribution, the episodic system always outperformed the schematic system (Fig. 7C). In total, our results demonstrated that the benefits of the schematic memory system (and with it, memory transformation) depended on the presence of a stable long-term distribution, or at the very least, a relatively low degree of nonstationarity coupled with sufficient training. Alternatively, a schematic memory system that actually attempts to develop a model of higherorder changes in the long-term pattern of reward locations may be able to further enhance reinforcement learning in more thoroughly nonstationary environments like these."
        },
        {
            "heading": "Discussion",
            "text": "Evidence of a transformation from specific memories to general or statistical memories during consolidation has become stronger in recent years (Wiltgen and Silva, 2007; Winocur et al., 2007; Durrant et al., 2011; Richards et al., 2014; Sekeres et al., 2016). However, the question of why this transformation may be beneficial has typically been framed in terms of reducing mnemonic interference or increasing mnemonic stability (McClelland et al., 1995; Squire and Alvarez, 1995; O’Reilly and Rudy, 2001). Here, we explored whether shifting from episodic to schematic systems over time is an advantageous strategy in environments where short-term consistency gives way to long-term patterns. We simulated a foraging task in which a reward shifted its location both gradually and suddenly throughout the environment, and built an agent that could use episodic or schematic memories to guide its searches. We observed a performance distinction between episodic and schematic memory-based control. With short delays between foraging trials, the episodic system more accurately predicted subsequent reward locations. With long delays between foraging trials, the schematic system more accurately predicted subsequent reward locations, even in the absence of further data accumulation. As such, when the agent was given the ability to switch between episodic and schematic control (transformation), it could take advantage of each system’s strengths and could efficiently find rewards regardless of the delay between foraging trials. We also found that the optimal timing of the shift was sensitive to the temporal dynamics of the environment: if the reward location regressed to a general statistical distribution very slowly, it was better to prolong the use of episodic memories. Further, we showed that our agent using episodic and schematic memories could generally outperform a habitual agent in variable environments, although our transformation agent performed worse in a constant environment if the habitual agent was given sufficient training. Finally, we showed that the benefits of the schematic memory system depended on the presence of either a stable long-term distribution for reward locations, or a nonstationary distribution with relatively low variance paired with sufficient training. Together, these results demonstrate that episodic\nand schematic memories have unique and complementary advantages for guiding behavior, and combining them in a manner that matches the statistics of the environment can produce sophisticated reinforcement learning. This may help to explain the evolution of memory transformation in the mammalian brain.\nInfluenced by the ideas of Marr (1970, 1971), McClelland et al. (1995) considered the potentially complementary nature of episodic and schematic memory systems. They demonstrated that episodic systems may be good for rapidly encoding data for later replay, to allow schematic systems to slowly identify statistical patterns across events. Indeed, there is neuropsychological evidence supporting the idea that there are distinct learning systems in the brain with these complementary capabilities (Tulving, 1972; McClelland et al., 1995; Tse et al., 2007, 2011; Richards et al., 2014). In our model, we made the same fundamental distinction between these two forms of memory, but we embedded it within the larger context of reinforcement learning. As a natural extension of McClelland et al. (1995), our episodic system was designed for storing specific reward locations on-line, whereas our schematic system was designed for learning the general pattern of reward locations across time via episodic replay. Although our model shares these essential features with McClelland et al. (1995), it builds on their framework in three important ways. First, our model operates in a fully online, autonomous fashion with learning and consolidation occurring continuously in the environment as experiences occur, consistent with the situations faced by animals. This allowed us to explore how complementary episodic and schematic systems perform in tasks with realistic temporal dynamics (i.e., the passage of time in the absence of further data accumulation). Second, memory encoding in our model is controlled by a prediction error signal rather than being externally determined. This adds an additional layer of autonomy and ensures that only data relevant to unexpected rewards (i.e., changes) get stored in memory. It is also in line with neurophysiological evidence showing that learning is modulated by dopaminergic signaling that encodes a temporal difference prediction error (Schultz et al., 1997; O’Carroll et al., 2006; Bethus et al., 2010). Third, our model did not require interleaved learning for the schematic system, eliminating the need for independent and identically distributed memory replay events. Instead, recent events were replayed with higher probability in our model. This resembles the organization of memory replay observed in vivo (Kudrimoti et al., 1999; Euston et al., 2007). In considering these differences, our model implements episodic and schematic systems in a more realistic scenario. Moreover, it reframes complementary learning systems as a solution to reinforcement learning in changing environments. We believe that this new perspective fits well with the most recent articulation of the complementary learning systems theory from McClelland and colleagues, which emphasizes the relevance of the theory to the design of intelligent agents (Kumaran et al., 2016).\nNeuropsychological studies in humans and experimental animals have also explored the idea of complementary learning systems, especially from the perspective of systems consolidation (Zola-Morgan and Squire, 1990; Frankland and Bontempi, 2005; Moscovitch et al., 2006; Wang and Morris, 2010; Winocur et al., 2010; Winocur and Moscovitch, 2011). These studies have uncovered two major components of systems consolidation. First, as memories age, the cortex plays an increasingly important role in their expression. Second, aged memories tend to be less specific and less contextually dense, and instead are more gist-like (or schematic) in nature. Such findings are typically interpreted as reflecting a consolidation process that renders memories less\nvulnerable to disruption over time (Frankland and Bontempi, 2005). Our model expands on this proposed function of systems consolidation. It suggests that, in addition to protecting memories from interference, the consolidation process functions to optimize reward seeking behavior. By this account, systems consolidation need not be a unidirectional process. Instead, it suggests that the brain shifts back and forth between episodic and schematic control depending on which provides the best predictions. In our model, if a new reward is encountered, the network switches back to reliance on its episodic memory system. Consistent with this, there is evidence in the neuropsychology literature that remote memories recontextualize (i.e., become more episodic) following reminders (Hupbach et al., 2007). However, in the absence of reminders, there is a tendency to shift from episodic to schematic retrieval over time (Winocur et al., 2010). We would suggest that, as in our simulated foraging task, the real world tends to be relatively consistent over short time periods but regresses to general distributions over long time periods. Thus, schematic systems will usually be best for control after long periods of time have passed since an experience, which could provide a normative account for instances of temporally graded retrograde amnesia following damage to the episodic system (Scoville and Milner, 1957). However, we note that one component of our model that is likely different from the reality in the brain is that our episodic memory system only ever forgot as a result of overwriting. In contrast, in the regular brain, there is evidence that episodic memories are typically highly transient (Conway, 2009). Thus, switching between episodic and schematic memories in the real brain will also depend on the dynamics of forgetting. Future research should explore how forgetting would affect the importance of memory transformation for optimizing decisions.\nOur results can also be understood as being part of a broader examination of how the brain uses different memory systems to make decisions (Klein et al., 2002; Daw et al., 2005; Doll et al., 2012; Wunderlich et al., 2012). Researchers have observed a switch between goal-directed (or model-based) behavior, guided by memories of past events and action outcomes, to habitual (or model-free) behavior, guided by stimulus–response associations (Daw et al., 2005). This switch may be desirable because, in stable environments, habitual systems are both competent and computationally efficient (Watkins and Dayan, 1992; Sutton and Barto, 1998). However, in changing environments, habitual systems deal poorly with altered contingencies (Foster et al., 2000; Dolan and Dayan, 2013). In these cases, goal-directed systems typically offer a better solution because memories of recent events can be used to update action-outcome predictions (Dayan and Niv, 2008; Dolan and Dayan, 2013). In the simulated environment we used here, a habitual system did indeed struggle with the moving reward location (Fig. 6), although it performed better than our model with sufficient training when the reward location was stationary. It should be noted, however, that with enough time, and the right parameter settings, a habitual system can learn a stable, long-term distribution of reward locations. Nonetheless, what our work demonstrates is that a goal-directed system that uses a combination of both specific, recent memories and a generative model based on multiple memories can easily take advantage of both short-term correlations and long-term statistical patterns without the large amounts of training that a habitual system requires. Given our results, and previous research into switching between memory systems (Daw et al., 2005), we hypothesize that the optimal strategy for guiding behavior may be to rely on episodic, goal-directed control when experience is limited, switch to schematic, goal-directed control when enough time has passed to\nrender episodic memories nonpredictive, and then switch to habitual control when accumulated experience and/or environmental stability are relatively high.\nThis proposal is broadly in agreement with the work of Lengyel and Dayan (2007), which suggested that episodic systems should guide behavior early in training, and schematic systems should guide behavior late in training. However, our model examines the benefits of an episodic to schematic switch even in the absence of the accumulation of new data. By emphasizing the passage of time in addition to data accumulation, our model makes some explicit, novel predictions about the relationship between the structure of the environment and the optimal balance between episodic and schematic control. In highly stochastic environments (i.e., situations with a rapid regression to the underlying distribution), we predict that the brain will rapidly shift to schematic control. In contrast, in environments where changes always occur gradually, such that the most recent experiences accurately predict new events, we predict that the brain will rely on episodic control for longer periods of time. This seems to contrast with Lengyel and Dayan’s (2007) prediction that the episodic system should generally be engaged in rapidly changing environments. Perhaps these different predictions result from our focus on the general passage of time as opposed to more training data. However, it is important to note that there are other significant differences between our work and theirs. First, we used a one-step forward model for both episodic and schematic control, whereas Lengyel and Dayan (2007) did not use a forward model for episodic control (episodic control for them was explicit recapitulation of previous actions). This meant that for us, unlike for Lengyel and Dayan (2007), the difference between episodic and schematic systems boiled down to whether a specific or general model of reward locations was used, not whether a forward model was used. Second, we did not alter the action-state transitions that our agent faced, meaning that the agent’s one-step forward model was always fairly accurate following the initial pretraining phase (Fig. 3). In other words, whereas uncertainty about the accuracy of the forward model was a key feature of Lengyel and Dayan (2007), it did not factor into our study. Although there are many situations animals face in which the accuracy of their forward models may be uncertain, we would argue that for foraging tasks such as the one we studied here that is not the case: barring totally new environments, or major motor changes, animals are likely to have good internal models of how their movements alter their position in space. Thus, whereas for Lengyel and Dayan (2007) one of the central advantages of episodic control was that it did not depend on a forward model, in our work, this issue was not central to the episodic versus schematic division, nor clearly a major issue for the specific task we were studying. More research that explores how the passage of time would impact a system where uncertainty is embedded within model-based control would help clarify these discrepancies. Once these issues have been explored, a more comprehensive theory of the utility of memory transformation for animal survival can be fleshed out."
        }
    ],
    "title": "Memory Transformation Enhances Reinforcement Learning in Dynamic Environments",
    "year": 2016
}